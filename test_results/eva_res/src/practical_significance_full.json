{
  "analysis_type": "full",
  "alpha_level": 0.05,
  "model_groups_defined": [
    "gpt_family",
    "lightrag_family",
    "moses_family",
    "spark_family",
    "llasmol_family",
    "flagship_models",
    "reasoning_comparison"
  ],
  "friedman_tests": {
    "Doubao-Seed-1.6-combined": {
      "test_type": "Friedman Test",
      "statistic": 195.22658413422397,
      "p_value": 1.303398454329736e-34,
      "degrees_of_freedom": 13,
      "significant": true,
      "models_tested": [
        "gpt-4.1",
        "gpt-4.1-nano",
        "gpt-4o",
        "gpt-4o-mini",
        "lightrag-4.1",
        "lightrag-4.1-nano",
        "llasmol-top1",
        "llasmol-top5",
        "MOSES",
        "MOSES-nano",
        "o1",
        "o3",
        "spark-chem13b-nothink",
        "spark-chem13b-think"
      ],
      "n_questions": 27,
      "interpretation": "Very strong evidence of significant differences among 14 models"
    },
    "fxx_gemini2.5-pro": {
      "test_type": "Friedman Test",
      "statistic": 201.60664435556302,
      "p_value": 6.392565048280807e-36,
      "degrees_of_freedom": 13,
      "significant": true,
      "models_tested": [
        "gpt-4.1",
        "gpt-4.1-nano",
        "gpt-4o",
        "gpt-4o-mini",
        "lightrag-4.1",
        "lightrag-4.1-nano",
        "llasmol-top1",
        "llasmol-top5",
        "MOSES",
        "MOSES-nano",
        "o1",
        "o3",
        "spark-chem13b-nothink",
        "spark-chem13b-think"
      ],
      "n_questions": 27,
      "interpretation": "Very strong evidence of significant differences among 14 models"
    }
  },
  "cascading_significance": {
    "Doubao-Seed-1.6-combined": {
      "cascading_analysis": [
        {
          "model": "MOSES",
          "rank": 1,
          "mean_score": 8.800483801001208,
          "significant_difference_found": true,
          "first_significant_model": "o3",
          "first_significant_rank": 2,
          "first_significant_mean": 7.901300160060954,
          "rank_gap": 1,
          "mean_difference": 0.8991836409402545,
          "p_value": 0.03394443768284035,
          "effect_size": 0.4618674623275953,
          "models_tested": 2,
          "non_significant_models": []
        },
        {
          "model": "o3",
          "rank": 2,
          "mean_score": 7.901300160060954,
          "significant_difference_found": true,
          "first_significant_model": "lightrag-4.1-nano",
          "first_significant_rank": 5,
          "first_significant_mean": 6.7981868113294475,
          "rank_gap": 3,
          "mean_difference": 1.1031133487315063,
          "p_value": 0.006450757384300232,
          "effect_size": 0.6186862174972834,
          "models_tested": 4,
          "non_significant_models": [
            "gpt-4.1",
            "o1"
          ]
        },
        {
          "model": "gpt-4.1",
          "rank": 3,
          "mean_score": 7.465862025119161,
          "significant_difference_found": true,
          "first_significant_model": "lightrag-4.1",
          "first_significant_rank": 6,
          "first_significant_mean": 6.63214203806513,
          "rank_gap": 3,
          "mean_difference": 0.8337199870540317,
          "p_value": 0.029904991388320923,
          "effect_size": 0.43283661182281485,
          "models_tested": 4,
          "non_significant_models": [
            "o1",
            "lightrag-4.1-nano"
          ]
        },
        {
          "model": "o1",
          "rank": 4,
          "mean_score": 7.284069670596636,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o",
          "first_significant_rank": 9,
          "first_significant_mean": 6.04233865071822,
          "rank_gap": 5,
          "mean_difference": 1.241731019878416,
          "p_value": 0.01506437361240387,
          "effect_size": 0.5306867966073117,
          "models_tested": 6,
          "non_significant_models": [
            "lightrag-4.1-nano",
            "lightrag-4.1",
            "gpt-4.1-nano",
            "MOSES-nano"
          ]
        },
        {
          "model": "lightrag-4.1-nano",
          "rank": 5,
          "mean_score": 6.7981868113294475,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o-mini",
          "first_significant_rank": 10,
          "first_significant_mean": 5.584983602669819,
          "rank_gap": 5,
          "mean_difference": 1.2132032086596283,
          "p_value": 0.0046126991510391235,
          "effect_size": 0.5781313009509365,
          "models_tested": 6,
          "non_significant_models": [
            "lightrag-4.1",
            "gpt-4.1-nano",
            "MOSES-nano",
            "gpt-4o"
          ]
        },
        {
          "model": "lightrag-4.1",
          "rank": 6,
          "mean_score": 6.63214203806513,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o-mini",
          "first_significant_rank": 10,
          "first_significant_mean": 5.584983602669819,
          "rank_gap": 4,
          "mean_difference": 1.0471584353953105,
          "p_value": 0.028018027544021606,
          "effect_size": 0.4608694697163074,
          "models_tested": 5,
          "non_significant_models": [
            "gpt-4.1-nano",
            "MOSES-nano",
            "gpt-4o"
          ]
        },
        {
          "model": "gpt-4.1-nano",
          "rank": 7,
          "mean_score": 6.541608338510047,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o-mini",
          "first_significant_rank": 10,
          "first_significant_mean": 5.584983602669819,
          "rank_gap": 3,
          "mean_difference": 0.9566247358402276,
          "p_value": 0.028018027544021606,
          "effect_size": 0.44683204222035705,
          "models_tested": 4,
          "non_significant_models": [
            "MOSES-nano",
            "gpt-4o"
          ]
        },
        {
          "model": "MOSES-nano",
          "rank": 8,
          "mean_score": 6.334349038233593,
          "significant_difference_found": true,
          "first_significant_model": "spark-chem13b-nothink",
          "first_significant_rank": 12,
          "first_significant_mean": 4.960236102519155,
          "rank_gap": 4,
          "mean_difference": 1.3741129357144377,
          "p_value": 0.01738584041595459,
          "effect_size": 0.5180433958052021,
          "models_tested": 5,
          "non_significant_models": [
            "gpt-4o",
            "gpt-4o-mini",
            "spark-chem13b-think"
          ]
        },
        {
          "model": "gpt-4o",
          "rank": 9,
          "mean_score": 6.04233865071822,
          "significant_difference_found": true,
          "first_significant_model": "spark-chem13b-nothink",
          "first_significant_rank": 12,
          "first_significant_mean": 4.960236102519155,
          "rank_gap": 3,
          "mean_difference": 1.0821025481990647,
          "p_value": 0.03189583122730255,
          "effect_size": 0.46487459256890357,
          "models_tested": 4,
          "non_significant_models": [
            "gpt-4o-mini",
            "spark-chem13b-think"
          ]
        },
        {
          "model": "gpt-4o-mini",
          "rank": 10,
          "mean_score": 5.584983602669819,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top5",
          "first_significant_rank": 13,
          "first_significant_mean": 1.1157517165618918,
          "rank_gap": 3,
          "mean_difference": 4.469231886107927,
          "p_value": 1.4901161193847656e-08,
          "effect_size": 2.3511255112893776,
          "models_tested": 4,
          "non_significant_models": [
            "spark-chem13b-think",
            "spark-chem13b-nothink"
          ]
        },
        {
          "model": "spark-chem13b-think",
          "rank": 11,
          "mean_score": 5.268674942333672,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top5",
          "first_significant_rank": 13,
          "first_significant_mean": 1.1157517165618918,
          "rank_gap": 2,
          "mean_difference": 4.15292322577178,
          "p_value": 1.872638514690607e-05,
          "effect_size": 1.5486813756000142,
          "models_tested": 3,
          "non_significant_models": [
            "spark-chem13b-nothink"
          ]
        },
        {
          "model": "spark-chem13b-nothink",
          "rank": 12,
          "mean_score": 4.960236102519155,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top5",
          "first_significant_rank": 13,
          "first_significant_mean": 1.1157517165618918,
          "rank_gap": 1,
          "mean_difference": 3.844484385957263,
          "p_value": 1.4901161193847656e-07,
          "effect_size": 1.621686592806144,
          "models_tested": 2,
          "non_significant_models": []
        },
        {
          "model": "llasmol-top5",
          "rank": 13,
          "mean_score": 1.1157517165618918,
          "significant_difference_found": false,
          "first_significant_model": null,
          "first_significant_rank": null,
          "first_significant_mean": null,
          "rank_gap": null,
          "mean_difference": null,
          "p_value": null,
          "effect_size": null,
          "models_tested": 1,
          "non_significant_models": [
            "llasmol-top1"
          ]
        }
      ],
      "total_models": 14,
      "models_with_significance": 12,
      "models_without_significance": 1
    },
    "fxx_gemini2.5-pro": {
      "cascading_analysis": [
        {
          "model": "o3",
          "rank": 1,
          "mean_score": 8.950441327796339,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4.1",
          "first_significant_rank": 4,
          "first_significant_mean": 8.188242001007163,
          "rank_gap": 3,
          "mean_difference": 0.762199326789176,
          "p_value": 0.04618034278166727,
          "effect_size": 0.4284655873800702,
          "models_tested": 4,
          "non_significant_models": [
            "MOSES",
            "lightrag-4.1"
          ]
        },
        {
          "model": "MOSES",
          "rank": 2,
          "mean_score": 8.900321608856698,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4.1",
          "first_significant_rank": 4,
          "first_significant_mean": 8.188242001007163,
          "rank_gap": 2,
          "mean_difference": 0.7120796078495353,
          "p_value": 0.03845841807877782,
          "effect_size": 0.4235526438885287,
          "models_tested": 3,
          "non_significant_models": [
            "lightrag-4.1"
          ]
        },
        {
          "model": "lightrag-4.1",
          "rank": 3,
          "mean_score": 8.351381327432428,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4.1-nano",
          "first_significant_rank": 7,
          "first_significant_mean": 6.784779520670091,
          "rank_gap": 4,
          "mean_difference": 1.5666018067623364,
          "p_value": 0.0029667913913726807,
          "effect_size": 0.6322657616322886,
          "models_tested": 5,
          "non_significant_models": [
            "gpt-4.1",
            "o1",
            "lightrag-4.1-nano"
          ]
        },
        {
          "model": "gpt-4.1",
          "rank": 4,
          "mean_score": 8.188242001007163,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4.1-nano",
          "first_significant_rank": 7,
          "first_significant_mean": 6.784779520670091,
          "rank_gap": 3,
          "mean_difference": 1.4034624803370717,
          "p_value": 0.0029667913913726807,
          "effect_size": 0.6451521592124673,
          "models_tested": 4,
          "non_significant_models": [
            "o1",
            "lightrag-4.1-nano"
          ]
        },
        {
          "model": "o1",
          "rank": 5,
          "mean_score": 7.725203759464886,
          "significant_difference_found": true,
          "first_significant_model": "MOSES-nano",
          "first_significant_rank": 9,
          "first_significant_mean": 6.503518806593166,
          "rank_gap": 4,
          "mean_difference": 1.22168495287172,
          "p_value": 0.03620612621307373,
          "effect_size": 0.4507958688561532,
          "models_tested": 5,
          "non_significant_models": [
            "lightrag-4.1-nano",
            "gpt-4.1-nano",
            "gpt-4o"
          ]
        },
        {
          "model": "lightrag-4.1-nano",
          "rank": 6,
          "mean_score": 7.6815939635514745,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o",
          "first_significant_rank": 8,
          "first_significant_mean": 6.602131821480401,
          "rank_gap": 2,
          "mean_difference": 1.0794621420710735,
          "p_value": 0.028018027544021606,
          "effect_size": 0.49023396666510516,
          "models_tested": 3,
          "non_significant_models": [
            "gpt-4.1-nano"
          ]
        },
        {
          "model": "gpt-4.1-nano",
          "rank": 7,
          "mean_score": 6.784779520670091,
          "significant_difference_found": true,
          "first_significant_model": "spark-chem13b-think",
          "first_significant_rank": 10,
          "first_significant_mean": 5.585508713560854,
          "rank_gap": 3,
          "mean_difference": 1.1992708071092375,
          "p_value": 0.024539664387702942,
          "effect_size": 0.4370677138179681,
          "models_tested": 4,
          "non_significant_models": [
            "gpt-4o",
            "MOSES-nano"
          ]
        },
        {
          "model": "gpt-4o",
          "rank": 8,
          "mean_score": 6.602131821480401,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o-mini",
          "first_significant_rank": 12,
          "first_significant_mean": 5.169528864763046,
          "rank_gap": 4,
          "mean_difference": 1.4326029567173553,
          "p_value": 0.03620612621307373,
          "effect_size": 0.4602635531988734,
          "models_tested": 5,
          "non_significant_models": [
            "MOSES-nano",
            "spark-chem13b-think",
            "spark-chem13b-nothink"
          ]
        },
        {
          "model": "MOSES-nano",
          "rank": 9,
          "mean_score": 6.503518806593166,
          "significant_difference_found": true,
          "first_significant_model": "gpt-4o-mini",
          "first_significant_rank": 12,
          "first_significant_mean": 5.169528864763046,
          "rank_gap": 3,
          "mean_difference": 1.3339899418301204,
          "p_value": 0.008888542652130127,
          "effect_size": 0.5663573365496232,
          "models_tested": 4,
          "non_significant_models": [
            "spark-chem13b-think",
            "spark-chem13b-nothink"
          ]
        },
        {
          "model": "spark-chem13b-think",
          "rank": 10,
          "mean_score": 5.585508713560854,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top1",
          "first_significant_rank": 13,
          "first_significant_mean": 1.5373326520610602,
          "rank_gap": 3,
          "mean_difference": 4.0481760614997935,
          "p_value": 2.086162567138672e-07,
          "effect_size": 1.3141109392862418,
          "models_tested": 4,
          "non_significant_models": [
            "spark-chem13b-nothink",
            "gpt-4o-mini"
          ]
        },
        {
          "model": "spark-chem13b-nothink",
          "rank": 11,
          "mean_score": 5.333280056671611,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top1",
          "first_significant_rank": 13,
          "first_significant_mean": 1.5373326520610602,
          "rank_gap": 2,
          "mean_difference": 3.7959474046105504,
          "p_value": 2.518296241760254e-06,
          "effect_size": 1.2950422517412217,
          "models_tested": 3,
          "non_significant_models": [
            "gpt-4o-mini"
          ]
        },
        {
          "model": "gpt-4o-mini",
          "rank": 12,
          "mean_score": 5.169528864763046,
          "significant_difference_found": true,
          "first_significant_model": "llasmol-top1",
          "first_significant_rank": 13,
          "first_significant_mean": 1.5373326520610602,
          "rank_gap": 1,
          "mean_difference": 3.6321962127019853,
          "p_value": 1.043081283569336e-07,
          "effect_size": 1.4530325042753833,
          "models_tested": 2,
          "non_significant_models": []
        },
        {
          "model": "llasmol-top1",
          "rank": 13,
          "mean_score": 1.5373326520610602,
          "significant_difference_found": false,
          "first_significant_model": null,
          "first_significant_rank": null,
          "first_significant_mean": null,
          "rank_gap": null,
          "mean_difference": null,
          "p_value": null,
          "effect_size": null,
          "models_tested": 1,
          "non_significant_models": [
            "llasmol-top5"
          ]
        }
      ],
      "total_models": 14,
      "models_with_significance": 12,
      "models_without_significance": 1
    }
  },
  "practical_comparisons": {
    "Doubao-Seed-1.6-combined": {
      "group_comparisons": {
        "gpt_family": {
          "group_name": "GPT Family Models",
          "comparisons": [
            {
              "comparison": "gpt-4.1 vs gpt-4.1-nano",
              "description": "GPT-4.1 vs Nano variant",
              "model1_mean": 7.465862025119161,
              "model2_mean": 6.541608338510047,
              "mean_difference": 0.9242536866091147,
              "wilcoxon_statistic": 102.0,
              "p_value": 0.03620612621307373,
              "effect_size": 0.4642585664090882,
              "significant": true,
              "practical_significance": false,
              "p_value_corrected": 0.10861837863922119,
              "significant_corrected": false
            },
            {
              "comparison": "gpt-4o vs gpt-4o-mini",
              "description": "GPT-4o vs Mini variant",
              "model1_mean": 6.04233865071822,
              "model2_mean": 5.584983602669819,
              "mean_difference": 0.45735504804840055,
              "wilcoxon_statistic": 156.0,
              "p_value": 0.44104358553886414,
              "effect_size": 0.1913188434151522,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            },
            {
              "comparison": "gpt-4.1 vs gpt-4o",
              "description": "GPT-4.1 vs GPT-4o flagship",
              "model1_mean": 7.465862025119161,
              "model2_mean": 6.04233865071822,
              "mean_difference": 1.4235233744009417,
              "wilcoxon_statistic": 59.0,
              "p_value": 0.0011315494775772095,
              "effect_size": 0.7480333919370497,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 0.0033946484327316284,
              "significant_corrected": true
            }
          ]
        },
        "lightrag_family": {
          "group_name": "LightRAG Family Models",
          "comparisons": [
            {
              "comparison": "lightrag-4.1 vs lightrag-4.1-nano",
              "description": "LightRAG full vs nano",
              "model1_mean": 6.63214203806513,
              "model2_mean": 6.7981868113294475,
              "mean_difference": -0.16604477326431777,
              "wilcoxon_statistic": 157.0,
              "p_value": 0.4553176909685135,
              "effect_size": -0.08678347704749122,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.4553176909685135,
              "significant_corrected": false
            }
          ]
        },
        "moses_family": {
          "group_name": "MOSES Family Models",
          "comparisons": [
            {
              "comparison": "MOSES vs MOSES-nano",
              "description": "MOSES full vs nano variant",
              "model1_mean": 8.800483801001208,
              "model2_mean": 6.334349038233593,
              "mean_difference": 2.4661347627676156,
              "wilcoxon_statistic": 25.0,
              "p_value": 1.3470649719238281e-05,
              "effect_size": 1.0485476863902699,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 1.3470649719238281e-05,
              "significant_corrected": true
            }
          ]
        },
        "spark_family": {
          "group_name": "Spark-Chem Models",
          "comparisons": [
            {
              "comparison": "spark-chem13b-think vs spark-chem13b-nothink",
              "description": "Chain-of-thought vs direct reasoning",
              "model1_mean": 5.268674942333672,
              "model2_mean": 4.960236102519155,
              "mean_difference": 0.30843883981451725,
              "wilcoxon_statistic": 156.0,
              "p_value": 0.44104358553886414,
              "effect_size": 0.0991575393178959,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.44104358553886414,
              "significant_corrected": false
            }
          ]
        },
        "llasmol_family": {
          "group_name": "LlasMol Models",
          "comparisons": [
            {
              "comparison": "llasmol-top1 vs llasmol-top5",
              "description": "Top-1 vs Top-5 selection",
              "model1_mean": 1.0069326839786863,
              "model2_mean": 1.1157517165618918,
              "mean_difference": -0.10881903258320547,
              "wilcoxon_statistic": 156.0,
              "p_value": 0.8611621969468249,
              "effect_size": -0.07469079057321677,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.8611621969468249,
              "significant_corrected": false
            }
          ]
        },
        "flagship_models": {
          "group_name": "Flagship/Best Models",
          "comparisons": [
            {
              "comparison": "MOSES vs o3",
              "description": "Top 2 performers",
              "model1_mean": 8.800483801001208,
              "model2_mean": 7.901300160060954,
              "mean_difference": 0.8991836409402545,
              "wilcoxon_statistic": 92.0,
              "p_value": 0.03394443768284035,
              "effect_size": 0.4618674623275953,
              "significant": true,
              "practical_significance": false,
              "p_value_corrected": 0.1357777507313614,
              "significant_corrected": false
            },
            {
              "comparison": "o3 vs gpt-4.1",
              "description": "Rank 2 vs 3",
              "model1_mean": 7.901300160060954,
              "model2_mean": 7.465862025119161,
              "mean_difference": 0.4354381349417924,
              "wilcoxon_statistic": 154.0,
              "p_value": 0.41325296461582184,
              "effect_size": 0.25859830846880083,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            },
            {
              "comparison": "gpt-4.1 vs o1",
              "description": "Rank 3 vs 4",
              "model1_mean": 7.465862025119161,
              "model2_mean": 7.284069670596636,
              "mean_difference": 0.18179235452252573,
              "wilcoxon_statistic": 170.0,
              "p_value": 0.8889058167448884,
              "effect_size": 0.08224070516673977,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            },
            {
              "comparison": "o1 vs lightrag-4.1-nano",
              "description": "Rank 4 vs 5",
              "model1_mean": 7.284069670596636,
              "model2_mean": 6.7981868113294475,
              "mean_difference": 0.4858828592671882,
              "wilcoxon_statistic": 134.0,
              "p_value": 0.19368016719818115,
              "effect_size": 0.22265151226763152,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.7747206687927246,
              "significant_corrected": false
            }
          ]
        },
        "reasoning_comparison": {
          "group_name": "Reasoning Approaches",
          "comparisons": [
            {
              "comparison": "o1 vs o3",
              "description": "OpenAI reasoning models",
              "model1_mean": 7.284069670596636,
              "model2_mean": 7.901300160060954,
              "mean_difference": -0.6172304894643181,
              "wilcoxon_statistic": 138.0,
              "p_value": 0.2290707677602768,
              "effect_size": -0.31454792652694175,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.4581415355205536,
              "significant_corrected": false
            },
            {
              "comparison": "spark-chem13b-think vs spark-chem13b-nothink",
              "description": "Chain-of-thought impact",
              "model1_mean": 5.268674942333672,
              "model2_mean": 4.960236102519155,
              "mean_difference": 0.30843883981451725,
              "wilcoxon_statistic": 156.0,
              "p_value": 0.44104358553886414,
              "effect_size": 0.0991575393178959,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.8820871710777283,
              "significant_corrected": false
            }
          ]
        }
      },
      "adjacent_ranking_tests": [
        {
          "rank_comparison": "#1 vs #2",
          "models": "MOSES vs o3",
          "model1_mean": 8.800483801001208,
          "model2_mean": 7.901300160060954,
          "mean_difference": 0.8991836409402545,
          "p_value": 0.03394443768284035,
          "effect_size": 0.4618674623275953,
          "significant": true,
          "practical_significance": true,
          "p_value_corrected": 0.3054999391455631,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#2 vs #3",
          "models": "o3 vs gpt-4.1",
          "model1_mean": 7.901300160060954,
          "model2_mean": 7.465862025119161,
          "mean_difference": 0.4354381349417924,
          "p_value": 0.41325296461582184,
          "effect_size": 0.25859830846880083,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#3 vs #4",
          "models": "gpt-4.1 vs o1",
          "model1_mean": 7.465862025119161,
          "model2_mean": 7.284069670596636,
          "mean_difference": 0.18179235452252573,
          "p_value": 0.8889058167448884,
          "effect_size": 0.08224070516673977,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#4 vs #5",
          "models": "o1 vs lightrag-4.1-nano",
          "model1_mean": 7.284069670596636,
          "model2_mean": 6.7981868113294475,
          "mean_difference": 0.4858828592671882,
          "p_value": 0.19368016719818115,
          "effect_size": 0.22265151226763152,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#5 vs #6",
          "models": "lightrag-4.1-nano vs lightrag-4.1",
          "model1_mean": 6.7981868113294475,
          "model2_mean": 6.63214203806513,
          "mean_difference": 0.16604477326431777,
          "p_value": 0.4553176909685135,
          "effect_size": 0.08678347704749122,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#6 vs #7",
          "models": "lightrag-4.1 vs gpt-4.1-nano",
          "model1_mean": 6.63214203806513,
          "model2_mean": 6.541608338510047,
          "mean_difference": 0.09053369955508295,
          "p_value": 0.9153143465518951,
          "effect_size": 0.03764056111054178,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#7 vs #8",
          "models": "gpt-4.1-nano vs MOSES-nano",
          "model1_mean": 6.541608338510047,
          "model2_mean": 6.334349038233593,
          "mean_difference": 0.207259300276454,
          "p_value": 0.7495975196361542,
          "effect_size": 0.07362425310058159,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#8 vs #9",
          "models": "MOSES-nano vs gpt-4o",
          "model1_mean": 6.334349038233593,
          "model2_mean": 6.04233865071822,
          "mean_difference": 0.292010387515373,
          "p_value": 0.6109333634376526,
          "effect_size": 0.10584817093164968,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#9 vs #10",
          "models": "gpt-4o vs gpt-4o-mini",
          "model1_mean": 6.04233865071822,
          "model2_mean": 5.584983602669819,
          "mean_difference": 0.45735504804840055,
          "p_value": 0.44104358553886414,
          "effect_size": 0.1913188434151522,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        }
      ],
      "summary_stats": {
        "total_group_comparisons": 13,
        "significant_group_comparisons": 2,
        "group_significance_rate": 0.15384615384615385,
        "total_adjacent_comparisons": 9,
        "significant_adjacent_comparisons": 0,
        "adjacent_significance_rate": 0.0
      }
    },
    "fxx_gemini2.5-pro": {
      "group_comparisons": {
        "gpt_family": {
          "group_name": "GPT Family Models",
          "comparisons": [
            {
              "comparison": "gpt-4.1 vs gpt-4.1-nano",
              "description": "GPT-4.1 vs Nano variant",
              "model1_mean": 8.188242001007163,
              "model2_mean": 6.784779520670091,
              "mean_difference": 1.4034624803370717,
              "wilcoxon_statistic": 69.0,
              "p_value": 0.0029667913913726807,
              "effect_size": 0.6451521592124673,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 0.008900374174118042,
              "significant_corrected": true
            },
            {
              "comparison": "gpt-4o vs gpt-4o-mini",
              "description": "GPT-4o vs Mini variant",
              "model1_mean": 6.602131821480401,
              "model2_mean": 5.169528864763046,
              "mean_difference": 1.4326029567173553,
              "wilcoxon_statistic": 102.0,
              "p_value": 0.03620612621307373,
              "effect_size": 0.4602635531988734,
              "significant": true,
              "practical_significance": false,
              "p_value_corrected": 0.10861837863922119,
              "significant_corrected": false
            },
            {
              "comparison": "gpt-4.1 vs gpt-4o",
              "description": "GPT-4.1 vs GPT-4o flagship",
              "model1_mean": 8.188242001007163,
              "model2_mean": 6.602131821480401,
              "mean_difference": 1.586110179526762,
              "wilcoxon_statistic": 72.0,
              "p_value": 0.003877997398376465,
              "effect_size": 0.6711007478065912,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 0.011633992195129395,
              "significant_corrected": true
            }
          ]
        },
        "lightrag_family": {
          "group_name": "LightRAG Family Models",
          "comparisons": [
            {
              "comparison": "lightrag-4.1 vs lightrag-4.1-nano",
              "description": "LightRAG full vs nano",
              "model1_mean": 8.351381327432428,
              "model2_mean": 7.6815939635514745,
              "mean_difference": 0.6697873638809533,
              "wilcoxon_statistic": 88.0,
              "p_value": 0.07648946089689826,
              "effect_size": 0.3457828978912326,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.07648946089689826,
              "significant_corrected": false
            }
          ]
        },
        "moses_family": {
          "group_name": "MOSES Family Models",
          "comparisons": [
            {
              "comparison": "MOSES vs MOSES-nano",
              "description": "MOSES full vs nano variant",
              "model1_mean": 8.900321608856698,
              "model2_mean": 6.503518806593166,
              "mean_difference": 2.396802802263532,
              "wilcoxon_statistic": 13.0,
              "p_value": 1.3113021850585938e-06,
              "effect_size": 1.069693584919936,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 1.3113021850585938e-06,
              "significant_corrected": true
            }
          ]
        },
        "spark_family": {
          "group_name": "Spark-Chem Models",
          "comparisons": [
            {
              "comparison": "spark-chem13b-think vs spark-chem13b-nothink",
              "description": "Chain-of-thought vs direct reasoning",
              "model1_mean": 5.585508713560854,
              "model2_mean": 5.333280056671611,
              "mean_difference": 0.2522286568892431,
              "wilcoxon_statistic": 182.0,
              "p_value": 0.8779177963733673,
              "effect_size": 0.06319318483665899,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.8779177963733673,
              "significant_corrected": false
            }
          ]
        },
        "llasmol_family": {
          "group_name": "LlasMol Models",
          "comparisons": [
            {
              "comparison": "llasmol-top1 vs llasmol-top5",
              "description": "Top-1 vs Top-5 selection",
              "model1_mean": 1.5373326520610602,
              "model2_mean": 1.4381421129660015,
              "mean_difference": 0.0991905390950587,
              "wilcoxon_statistic": 171.0,
              "p_value": 0.9090071978307225,
              "effect_size": 0.05060679939533176,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 0.9090071978307225,
              "significant_corrected": false
            }
          ]
        },
        "flagship_models": {
          "group_name": "Flagship/Best Models",
          "comparisons": [
            {
              "comparison": "MOSES vs o3",
              "description": "Top 2 performers",
              "model1_mean": 8.900321608856698,
              "model2_mean": 8.950441327796339,
              "mean_difference": -0.050119718939640734,
              "wilcoxon_statistic": 168.0,
              "p_value": 0.8489292299042391,
              "effect_size": -0.036362681399426756,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            },
            {
              "comparison": "o3 vs gpt-4.1",
              "description": "Rank 2 vs 3",
              "model1_mean": 8.950441327796339,
              "model2_mean": 8.188242001007163,
              "mean_difference": 0.762199326789176,
              "wilcoxon_statistic": 97.0,
              "p_value": 0.04618034278166727,
              "effect_size": 0.4284655873800702,
              "significant": true,
              "practical_significance": false,
              "p_value_corrected": 0.1847213711266691,
              "significant_corrected": false
            },
            {
              "comparison": "gpt-4.1 vs o1",
              "description": "Rank 3 vs 4",
              "model1_mean": 8.188242001007163,
              "model2_mean": 7.725203759464886,
              "mean_difference": 0.4630382415422769,
              "wilcoxon_statistic": 142.0,
              "p_value": 0.2686719596385956,
              "effect_size": 0.22152158540710468,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            },
            {
              "comparison": "o1 vs lightrag-4.1-nano",
              "description": "Rank 4 vs 5",
              "model1_mean": 7.725203759464886,
              "model2_mean": 7.6815939635514745,
              "mean_difference": 0.04360979591341163,
              "wilcoxon_statistic": 143.0,
              "p_value": 0.8414805811217939,
              "effect_size": 0.020227791460187775,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            }
          ]
        },
        "reasoning_comparison": {
          "group_name": "Reasoning Approaches",
          "comparisons": [
            {
              "comparison": "o1 vs o3",
              "description": "OpenAI reasoning models",
              "model1_mean": 7.725203759464886,
              "model2_mean": 8.950441327796339,
              "mean_difference": -1.225237568331453,
              "wilcoxon_statistic": 66.0,
              "p_value": 0.009417424955860197,
              "effect_size": -0.5595043337818942,
              "significant": true,
              "practical_significance": true,
              "p_value_corrected": 0.018834849911720394,
              "significant_corrected": true
            },
            {
              "comparison": "spark-chem13b-think vs spark-chem13b-nothink",
              "description": "Chain-of-thought impact",
              "model1_mean": 5.585508713560854,
              "model2_mean": 5.333280056671611,
              "mean_difference": 0.2522286568892431,
              "wilcoxon_statistic": 182.0,
              "p_value": 0.8779177963733673,
              "effect_size": 0.06319318483665899,
              "significant": false,
              "practical_significance": false,
              "p_value_corrected": 1.0,
              "significant_corrected": false
            }
          ]
        }
      },
      "adjacent_ranking_tests": [
        {
          "rank_comparison": "#1 vs #2",
          "models": "o3 vs MOSES",
          "model1_mean": 8.950441327796339,
          "model2_mean": 8.900321608856698,
          "mean_difference": 0.050119718939640734,
          "p_value": 0.8489292299042391,
          "effect_size": 0.036362681399426756,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#2 vs #3",
          "models": "MOSES vs lightrag-4.1",
          "model1_mean": 8.900321608856698,
          "model2_mean": 8.351381327432428,
          "mean_difference": 0.5489402814242705,
          "p_value": 0.21087195144032855,
          "effect_size": 0.28905422620527566,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#3 vs #4",
          "models": "lightrag-4.1 vs gpt-4.1",
          "model1_mean": 8.351381327432428,
          "model2_mean": 8.188242001007163,
          "mean_difference": 0.16313932642526474,
          "p_value": 0.5303222090005875,
          "effect_size": 0.08054729128040863,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#4 vs #5",
          "models": "gpt-4.1 vs o1",
          "model1_mean": 8.188242001007163,
          "model2_mean": 7.725203759464886,
          "mean_difference": 0.4630382415422769,
          "p_value": 0.2686719596385956,
          "effect_size": 0.22152158540710468,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#5 vs #6",
          "models": "o1 vs lightrag-4.1-nano",
          "model1_mean": 7.725203759464886,
          "model2_mean": 7.6815939635514745,
          "mean_difference": 0.04360979591341163,
          "p_value": 0.8414805811217939,
          "effect_size": 0.020227791460187775,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#6 vs #7",
          "models": "lightrag-4.1-nano vs gpt-4.1-nano",
          "model1_mean": 7.6815939635514745,
          "model2_mean": 6.784779520670091,
          "mean_difference": 0.8968144428813831,
          "p_value": 0.11668527126312256,
          "effect_size": 0.3166536115836113,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#7 vs #8",
          "models": "gpt-4.1-nano vs gpt-4o",
          "model1_mean": 6.784779520670091,
          "model2_mean": 6.602131821480401,
          "mean_difference": 0.18264769918969037,
          "p_value": 0.7495975196361542,
          "effect_size": 0.06787790173619228,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#8 vs #9",
          "models": "gpt-4o vs MOSES-nano",
          "model1_mean": 6.602131821480401,
          "model2_mean": 6.503518806593166,
          "mean_difference": 0.09861301488723484,
          "p_value": 0.9717257022857666,
          "effect_size": 0.035780107745604486,
          "significant": false,
          "practical_significance": false,
          "p_value_corrected": 1.0,
          "significant_corrected": false
        },
        {
          "rank_comparison": "#9 vs #10",
          "models": "MOSES-nano vs spark-chem13b-think",
          "model1_mean": 6.503518806593166,
          "model2_mean": 5.585508713560854,
          "mean_difference": 0.9180100930323123,
          "p_value": 0.0692269504070282,
          "effect_size": 0.3393527506820762,
          "significant": false,
          "practical_significance": true,
          "p_value_corrected": 0.6230425536632538,
          "significant_corrected": false
        }
      ],
      "summary_stats": {
        "total_group_comparisons": 13,
        "significant_group_comparisons": 4,
        "group_significance_rate": 0.3076923076923077,
        "total_adjacent_comparisons": 9,
        "significant_adjacent_comparisons": 0,
        "adjacent_significance_rate": 0.0
      }
    }
  }
}