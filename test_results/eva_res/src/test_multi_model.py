#!/usr/bin/env python3
"""
Test script for multi-model matching functionality in TrueSkill ELO analysis.
Tests both 1vs1 pairwise mode and multi-model ranking mode.
"""

import os
import sys
sys.path.append('.')

from trueskill_elo_analysis import TrueSkillELOAnalyzer

def test_both_modes():
    """Test both 1vs1 and multi-model matching modes."""
    print("=== Testing TrueSkill Multi-Model Matching ===\n")
    
    # Test 1: 1vs1 Mode (existing functionality)
    print("1. Testing 1vs1 Mode (pairwise matching)")
    print("-" * 50)
    
    analyzer_1vs1 = TrueSkillELOAnalyzer(
        use_trueskill_default=True,
        match_mode="1vs1"
    )
    
    # Run analysis with double aggregation
    results_1vs1 = analyzer_1vs1.run_full_trueskill_analysis(aggregation_level="double")
    
    print(f"‚úì 1vs1 mode completed successfully")
    print(f"  - Total matches: {results_1vs1.get('total_matches', 'N/A')}")
    print(f"  - Aggregation level: {results_1vs1.get('aggregation_level', 'double')}")
    print(f"  - Match generation mode: 1vs1 pairwise")
    print()
    
    # Test 2: Multi-model Mode (new functionality)
    print("2. Testing Multi-Model Mode (simultaneous ranking)")
    print("-" * 50)
    
    analyzer_multi = TrueSkillELOAnalyzer(
        use_trueskill_default=True,
        match_mode="multi"
    )
    
    # Run analysis with double aggregation
    results_multi = analyzer_multi.run_full_trueskill_analysis(aggregation_level="double")
    
    print(f"‚úì Multi-model mode completed successfully")
    print(f"  - Total matches: {results_multi.get('total_matches', 'N/A')}")
    print(f"  - Aggregation level: {results_multi.get('aggregation_level', 'double')}")
    print(f"  - Match generation mode: multi-model ranking")
    print()
    
    # Compare results
    print("3. Comparing Results")
    print("-" * 50)
    
    # Compare top models in both modes
    overall_1vs1 = results_1vs1.get('overall_ratings', {})
    overall_multi = results_multi.get('overall_ratings', {})
    
    if not overall_1vs1 or not overall_multi:
        print("  Results structure might be different, checking available keys:")
        print(f"  1vs1 keys: {list(results_1vs1.keys()) if results_1vs1 else 'None'}")
        print(f"  Multi keys: {list(results_multi.keys()) if results_multi else 'None'}")
        return results_1vs1, results_multi
    
    for judge in overall_1vs1.keys():
        print(f"\nJudge: {judge}")
        
        # Get top 5 models from each mode
        # Check if the values are DimensionRating objects or dicts
        judge_1vs1 = overall_1vs1[judge]
        judge_multi = overall_multi[judge]
        
        if judge_1vs1 and isinstance(list(judge_1vs1.values())[0], dict):
            print("  Note: Results appear to be in dict format, not DimensionRating objects")
            # For now, just show the first few models
            models_1vs1 = list(judge_1vs1.items())[:5]
            models_multi = list(judge_multi.items())[:5]
            
            print("  1vs1 Mode Top 5:")
            for i, (model, data) in enumerate(models_1vs1):
                print(f"    {i+1}. {model}: Data keys={list(data.keys()) if isinstance(data, dict) else type(data)}")
            
            print("  Multi-model Mode Top 5:")
            for i, (model, data) in enumerate(models_multi):
                print(f"    {i+1}. {model}: Data keys={list(data.keys()) if isinstance(data, dict) else type(data)}")
        else:
            # Original logic for DimensionRating objects
            models_1vs1 = sorted(judge_1vs1.items(), 
                               key=lambda x: x[1].rating, reverse=True)[:5]
            models_multi = sorted(judge_multi.items(), 
                                key=lambda x: x[1].rating, reverse=True)[:5]
            
            print("  1vs1 Mode Top 5:")
            for i, (model, rating) in enumerate(models_1vs1):
                print(f"    {i+1}. {model}: ELO={rating.rating:.2f} (Œº={rating.mu:.2f}, œÉ={rating.sigma:.2f})")
            
            print("  Multi-model Mode Top 5:")
            for i, (model, rating) in enumerate(models_multi):
                print(f"    {i+1}. {model}: ELO={rating.rating:.2f} (Œº={rating.mu:.2f}, œÉ={rating.sigma:.2f})")
        
        break  # Only show first judge for brevity
    
    print("\n=== Test Summary ===")
    print(f"‚úì 1vs1 mode: {results_1vs1.get('total_matches', 'N/A')} matches processed")
    print(f"‚úì Multi-model mode: {results_multi.get('total_matches', 'N/A')} matches processed")
    print("‚úì Both modes completed without errors")
    print("‚úì Multi-model matching functionality successfully implemented!")
    
    return results_1vs1, results_multi

if __name__ == "__main__":
    # Change to the correct directory
    os.chdir(r'C:\d\CursorProj\Chem-Ontology-Constructor\test_results\eva_res\src')
    
    try:
        results_1vs1, results_multi = test_both_modes()
        print("\nüéâ All tests passed! Multi-model matching is working correctly.")
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()