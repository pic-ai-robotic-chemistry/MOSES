{
  "model_average_scores": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "overall_average": 7.391666666666667,
        "overall_std": 1.4307870166411378,
        "dimension_averages": {
          "correctness": 8.465185185185184,
          "completeness": 6.622222222222222,
          "logic": 8.936296296296296,
          "clarity": 9.174814814814814,
          "theoretical_depth": 3.2,
          "rigor_and_information_density": 7.647407407407408
        },
        "total_evaluations": 270
      },
      "gpt-4.1-nano": {
        "overall_average": 6.712037037037037,
        "overall_std": 1.5678085712532883,
        "dimension_averages": {
          "correctness": 7.857777777777778,
          "completeness": 5.348148148148148,
          "logic": 8.416296296296297,
          "clarity": 8.853333333333333,
          "theoretical_depth": 3.1155555555555554,
          "rigor_and_information_density": 6.899259259259259
        },
        "total_evaluations": 270
      },
      "gpt-4o": {
        "overall_average": 6.191111111111111,
        "overall_std": 1.7421610588274183,
        "dimension_averages": {
          "correctness": 6.96,
          "completeness": 4.88,
          "logic": 8.162962962962963,
          "clarity": 8.742222222222223,
          "theoretical_depth": 2.745185185185185,
          "rigor_and_information_density": 6.198518518518519
        },
        "total_evaluations": 270
      },
      "gpt-4o-mini": {
        "overall_average": 5.41462962962963,
        "overall_std": 1.9274573587373578,
        "dimension_averages": {
          "correctness": 5.84,
          "completeness": 4.065185185185185,
          "logic": 7.545185185185185,
          "clarity": 8.176296296296297,
          "theoretical_depth": 2.4340740740740743,
          "rigor_and_information_density": 5.351111111111111
        },
        "total_evaluations": 270
      },
      "lightrag-4.1": {
        "overall_average": 6.802407407407407,
        "overall_std": 1.4641006922748339,
        "dimension_averages": {
          "correctness": 7.6251851851851855,
          "completeness": 5.522962962962963,
          "logic": 8.357037037037037,
          "clarity": 8.554074074074075,
          "theoretical_depth": 4.025185185185185,
          "rigor_and_information_density": 7.1866666666666665
        },
        "total_evaluations": 270
      },
      "lightrag-4.1-nano": {
        "overall_average": 7.072962962962963,
        "overall_std": 1.3210233579985653,
        "dimension_averages": {
          "correctness": 8.016296296296296,
          "completeness": 6.0162962962962965,
          "logic": 8.457777777777778,
          "clarity": 8.685925925925925,
          "theoretical_depth": 4.025185185185185,
          "rigor_and_information_density": 7.3496296296296295
        },
        "total_evaluations": 270
      },
      "llasmol-top1": {
        "overall_average": 0.7964814814814815,
        "overall_std": 1.5290971622581901,
        "dimension_averages": {
          "correctness": 1.1733333333333333,
          "completeness": 0.14074074074074075,
          "logic": 1.2977777777777777,
          "clarity": 1.682962962962963,
          "theoretical_depth": 0.16,
          "rigor_and_information_density": 0.6029629629629629
        },
        "total_evaluations": 270
      },
      "llasmol-top5": {
        "overall_average": 0.799074074074074,
        "overall_std": 1.5705139316040855,
        "dimension_averages": {
          "correctness": 1.2148148148148148,
          "completeness": 0.10518518518518519,
          "logic": 1.2592592592592593,
          "clarity": 1.7822222222222222,
          "theoretical_depth": 0.09925925925925926,
          "rigor_and_information_density": 0.6118518518518519
        },
        "total_evaluations": 270
      },
      "MOSES": {
        "overall_average": 8.80462962962963,
        "overall_std": 1.1420025992375367,
        "dimension_averages": {
          "correctness": 9.564444444444444,
          "completeness": 8.722962962962963,
          "logic": 9.437037037037037,
          "clarity": 9.334814814814814,
          "theoretical_depth": 6.508148148148148,
          "rigor_and_information_density": 8.582222222222223
        },
        "total_evaluations": 270
      },
      "MOSES-nano": {
        "overall_average": 6.654629629629629,
        "overall_std": 1.8397275466652583,
        "dimension_averages": {
          "correctness": 7.546666666666667,
          "completeness": 5.312592592592592,
          "logic": 8.127407407407407,
          "clarity": 8.505185185185185,
          "theoretical_depth": 4.001481481481481,
          "rigor_and_information_density": 6.884444444444444
        },
        "total_evaluations": 270
      },
      "o1": {
        "overall_average": 7.183333333333334,
        "overall_std": 1.6675282779073997,
        "dimension_averages": {
          "correctness": 8.287407407407407,
          "completeness": 5.505185185185185,
          "logic": 9.093333333333334,
          "clarity": 9.278518518518519,
          "theoretical_depth": 3.6014814814814815,
          "rigor_and_information_density": 7.908148148148148
        },
        "total_evaluations": 270
      },
      "o3": {
        "overall_average": 8.380185185185185,
        "overall_std": 1.3932811301336125,
        "dimension_averages": {
          "correctness": 9.285925925925927,
          "completeness": 7.871111111111111,
          "logic": 9.694814814814816,
          "clarity": 9.648888888888889,
          "theoretical_depth": 4.545185185185185,
          "rigor_and_information_density": 8.83851851851852
        },
        "total_evaluations": 270
      },
      "spark-chem13b-nothink": {
        "overall_average": 5.579814814814815,
        "overall_std": 2.2706978528225994,
        "dimension_averages": {
          "correctness": 6.342222222222222,
          "completeness": 4.337777777777778,
          "logic": 7.402962962962963,
          "clarity": 7.764444444444444,
          "theoretical_depth": 2.5896296296296297,
          "rigor_and_information_density": 5.521481481481482
        },
        "total_evaluations": 270
      },
      "spark-chem13b-think": {
        "overall_average": 5.625740740740741,
        "overall_std": 2.2137989030380303,
        "dimension_averages": {
          "correctness": 6.3348148148148145,
          "completeness": 4.373333333333333,
          "logic": 7.3496296296296295,
          "clarity": 7.6903703703703705,
          "theoretical_depth": 2.788148148148148,
          "rigor_and_information_density": 5.761481481481481
        },
        "total_evaluations": 270
      },
      "darwin": {
        "overall_average": 0.3246296296296296,
        "overall_std": 0.7972856203752275,
        "dimension_averages": {
          "correctness": 0.21037037037037037,
          "completeness": 0.035555555555555556,
          "logic": 0.6311111111111111,
          "clarity": 1.048888888888889,
          "theoretical_depth": 0.04148148148148148,
          "rigor_and_information_density": 0.3837037037037037
        },
        "total_evaluations": 270
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "overall_average": 7.979814814814815,
        "overall_std": 1.4620127831925298,
        "dimension_averages": {
          "correctness": 9.177777777777777,
          "completeness": 6.5851851851851855,
          "logic": 9.733333333333334,
          "clarity": 9.682962962962963,
          "theoretical_depth": 4.271111111111111,
          "rigor_and_information_density": 8.625185185185185
        },
        "total_evaluations": 270
      },
      "gpt-4.1-nano": {
        "overall_average": 6.729629629629629,
        "overall_std": 1.8547016142574606,
        "dimension_averages": {
          "correctness": 8.52,
          "completeness": 5.017777777777778,
          "logic": 8.696296296296296,
          "clarity": 8.626666666666667,
          "theoretical_depth": 3.185185185185185,
          "rigor_and_information_density": 6.253333333333333
        },
        "total_evaluations": 270
      },
      "gpt-4o": {
        "overall_average": 6.208148148148148,
        "overall_std": 1.958972375015693,
        "dimension_averages": {
          "correctness": 7.841481481481481,
          "completeness": 4.742222222222222,
          "logic": 8.32148148148148,
          "clarity": 8.768888888888888,
          "theoretical_depth": 2.3466666666666667,
          "rigor_and_information_density": 5.060740740740741
        },
        "total_evaluations": 270
      },
      "gpt-4o-mini": {
        "overall_average": 5.104444444444445,
        "overall_std": 2.4078825530111336,
        "dimension_averages": {
          "correctness": 6.321481481481482,
          "completeness": 4.074074074074074,
          "logic": 6.702222222222222,
          "clarity": 7.653333333333333,
          "theoretical_depth": 1.854814814814815,
          "rigor_and_information_density": 3.834074074074074
        },
        "total_evaluations": 270
      },
      "lightrag-4.1": {
        "overall_average": 7.827407407407407,
        "overall_std": 1.6788064280890913,
        "dimension_averages": {
          "correctness": 9.16,
          "completeness": 5.463703703703704,
          "logic": 9.386666666666667,
          "clarity": 9.36888888888889,
          "theoretical_depth": 6.232592592592592,
          "rigor_and_information_density": 8.383703703703704
        },
        "total_evaluations": 270
      },
      "lightrag-4.1-nano": {
        "overall_average": 7.948611111111111,
        "overall_std": 1.6423276665112696,
        "dimension_averages": {
          "correctness": 9.102222222222222,
          "completeness": 5.955555555555556,
          "logic": 9.354074074074074,
          "clarity": 9.36888888888889,
          "theoretical_depth": 6.1866666666666665,
          "rigor_and_information_density": 8.563703703703704
        },
        "total_evaluations": 270
      },
      "llasmol-top1": {
        "overall_average": 0.7243518518518518,
        "overall_std": 1.4404124920020365,
        "dimension_averages": {
          "correctness": 1.2344444444444445,
          "completeness": 0.010370370370370372,
          "logic": 0.9925925925925926,
          "clarity": 1.891851851851852,
          "theoretical_depth": 0.05925925925925926,
          "rigor_and_information_density": 0.36148148148148146
        },
        "total_evaluations": 270
      },
      "llasmol-top5": {
        "overall_average": 0.9355555555555556,
        "overall_std": 1.7457741507359832,
        "dimension_averages": {
          "correctness": 1.8325925925925926,
          "completeness": 0.01925925925925926,
          "logic": 1.2844444444444445,
          "clarity": 1.9674074074074075,
          "theoretical_depth": 0.031111111111111114,
          "rigor_and_information_density": 0.49777777777777776
        },
        "total_evaluations": 270
      },
      "MOSES": {
        "overall_average": 9.316574074074074,
        "overall_std": 1.0871186568385887,
        "dimension_averages": {
          "correctness": 9.666666666666666,
          "completeness": 8.59851851851852,
          "logic": 9.807407407407407,
          "clarity": 9.888518518518518,
          "theoretical_depth": 8.684074074074074,
          "rigor_and_information_density": 9.622222222222222
        },
        "total_evaluations": 270
      },
      "MOSES-nano": {
        "overall_average": 6.780185185185185,
        "overall_std": 2.3429650222934373,
        "dimension_averages": {
          "correctness": 7.899259259259259,
          "completeness": 5.488888888888889,
          "logic": 7.859259259259259,
          "clarity": 8.367407407407407,
          "theoretical_depth": 4.745185185185186,
          "rigor_and_information_density": 6.493333333333333
        },
        "total_evaluations": 270
      },
      "o1": {
        "overall_average": 8.03648148148148,
        "overall_std": 1.922067270569007,
        "dimension_averages": {
          "correctness": 9.114074074074074,
          "completeness": 5.42962962962963,
          "logic": 9.917037037037037,
          "clarity": 9.921481481481482,
          "theoretical_depth": 6.026666666666666,
          "rigor_and_information_density": 9.33925925925926
        },
        "total_evaluations": 270
      },
      "o3": {
        "overall_average": 9.14037037037037,
        "overall_std": 1.4790481754961615,
        "dimension_averages": {
          "correctness": 9.379259259259259,
          "completeness": 8.16,
          "logic": 9.934814814814814,
          "clarity": 9.98962962962963,
          "theoretical_depth": 8.256296296296297,
          "rigor_and_information_density": 9.863703703703704
        },
        "total_evaluations": 270
      },
      "spark-chem13b-nothink": {
        "overall_average": 5.371666666666667,
        "overall_std": 2.6382521743222322,
        "dimension_averages": {
          "correctness": 6.2340740740740745,
          "completeness": 4.602962962962963,
          "logic": 6.108148148148148,
          "clarity": 7.84,
          "theoretical_depth": 2.6785185185185187,
          "rigor_and_information_density": 4.672592592592593
        },
        "total_evaluations": 270
      },
      "spark-chem13b-think": {
        "overall_average": 5.577962962962963,
        "overall_std": 2.6347231887147156,
        "dimension_averages": {
          "correctness": 6.408148148148148,
          "completeness": 4.902222222222222,
          "logic": 6.437037037037037,
          "clarity": 7.4725925925925925,
          "theoretical_depth": 3.0148148148148146,
          "rigor_and_information_density": 5.078518518518519
        },
        "total_evaluations": 270
      },
      "darwin": {
        "overall_average": 0.47074074074074074,
        "overall_std": 1.169491122855625,
        "dimension_averages": {
          "correctness": 0.7496296296296296,
          "completeness": 0.06666666666666667,
          "logic": 0.6385185185185185,
          "clarity": 1.2207407407407407,
          "theoretical_depth": 0.03851851851851852,
          "rigor_and_information_density": 0.23555555555555555
        },
        "total_evaluations": 270
      }
    }
  },
  "best_answer_rounds": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "best_round": "4",
        "best_score": 7.407407407407407,
        "best_std": 0.6716692646977389,
        "all_round_stats": {
          "1": {
            "mean": 7.345679012345679,
            "std": 0.7291927481429626,
            "count": 27
          },
          "2": {
            "mean": 7.31358024691358,
            "std": 0.8189663726886522,
            "count": 27
          },
          "3": {
            "mean": 7.325925925925926,
            "std": 0.7477792382607243,
            "count": 27
          },
          "4": {
            "mean": 7.407407407407407,
            "std": 0.6716692646977389,
            "count": 27
          },
          "5": {
            "mean": 7.312345679012346,
            "std": 0.8503969986995942,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.345679012345679,
          "2": 7.31358024691358,
          "3": 7.325925925925926,
          "4": 7.407407407407407,
          "5": 7.312345679012346
        }
      },
      "gpt-4.1-nano": {
        "best_round": "2",
        "best_score": 6.825925925925926,
        "best_std": 0.8813031014895822,
        "all_round_stats": {
          "1": {
            "mean": 6.703703703703704,
            "std": 1.2810363504031144,
            "count": 27
          },
          "2": {
            "mean": 6.825925925925926,
            "std": 0.8813031014895822,
            "count": 27
          },
          "3": {
            "mean": 6.7481481481481485,
            "std": 1.2157208593987388,
            "count": 27
          },
          "4": {
            "mean": 6.690123456790124,
            "std": 1.066899606568956,
            "count": 27
          },
          "5": {
            "mean": 6.774074074074074,
            "std": 0.8687024085634129,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.703703703703704,
          "2": 6.825925925925926,
          "3": 6.7481481481481485,
          "4": 6.690123456790124,
          "5": 6.774074074074074
        }
      },
      "gpt-4o": {
        "best_round": "1",
        "best_score": 6.3654320987654325,
        "best_std": 1.2357731059370858,
        "all_round_stats": {
          "1": {
            "mean": 6.3654320987654325,
            "std": 1.2357731059370858,
            "count": 27
          },
          "2": {
            "mean": 6.276543209876543,
            "std": 1.1068093364483016,
            "count": 27
          },
          "3": {
            "mean": 6.266666666666667,
            "std": 1.224570393882136,
            "count": 27
          },
          "4": {
            "mean": 6.260493827160494,
            "std": 1.160350559946854,
            "count": 27
          },
          "5": {
            "mean": 6.238271604938271,
            "std": 1.11714317057592,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.3654320987654325,
          "2": 6.276543209876543,
          "3": 6.266666666666667,
          "4": 6.260493827160494,
          "5": 6.238271604938271
        }
      },
      "gpt-4o-mini": {
        "best_round": "2",
        "best_score": 5.618518518518519,
        "best_std": 1.360031841423799,
        "all_round_stats": {
          "1": {
            "mean": 5.5962962962962965,
            "std": 1.4246107080587558,
            "count": 27
          },
          "2": {
            "mean": 5.618518518518519,
            "std": 1.360031841423799,
            "count": 27
          },
          "3": {
            "mean": 5.549382716049383,
            "std": 1.4383148488092445,
            "count": 27
          },
          "4": {
            "mean": 5.529629629629629,
            "std": 1.3684582794802562,
            "count": 27
          },
          "5": {
            "mean": 5.549382716049383,
            "std": 1.3821626300580883,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.5962962962962965,
          "2": 5.618518518518519,
          "3": 5.549382716049383,
          "4": 5.529629629629629,
          "5": 5.549382716049383
        }
      },
      "lightrag-4.1": {
        "best_round": "1",
        "best_score": 7.366666666666666,
        "best_std": 0.6552875013318272,
        "all_round_stats": {
          "1": {
            "mean": 7.366666666666666,
            "std": 0.6552875013318272,
            "count": 27
          },
          "2": {
            "mean": 6.782716049382716,
            "std": 0.940647638796556,
            "count": 27
          },
          "3": {
            "mean": 6.632098765432099,
            "std": 1.0993580676059929,
            "count": 27
          },
          "4": {
            "mean": 6.896296296296296,
            "std": 0.9206247104818867,
            "count": 27
          },
          "5": {
            "mean": 6.714814814814814,
            "std": 1.0531731999883918,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.366666666666666,
          "2": 6.782716049382716,
          "3": 6.632098765432099,
          "4": 6.896296296296296,
          "5": 6.714814814814814
        }
      },
      "lightrag-4.1-nano": {
        "best_round": "4",
        "best_score": 7.2592592592592595,
        "best_std": 0.8070910799878854,
        "all_round_stats": {
          "1": {
            "mean": 6.5320987654320986,
            "std": 0.8072577555448484,
            "count": 27
          },
          "2": {
            "mean": 7.212345679012346,
            "std": 0.7889811751325356,
            "count": 27
          },
          "3": {
            "mean": 7.230864197530864,
            "std": 0.854596717302634,
            "count": 27
          },
          "4": {
            "mean": 7.2592592592592595,
            "std": 0.8070910799878854,
            "count": 27
          },
          "5": {
            "mean": 7.224691358024692,
            "std": 0.8045592620835925,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.5320987654320986,
          "2": 7.212345679012346,
          "3": 7.230864197530864,
          "4": 7.2592592592592595,
          "5": 7.224691358024692
        }
      },
      "llasmol-top1": {
        "best_round": "2",
        "best_score": 0.8765432098765432,
        "best_std": 1.5702037217386633,
        "all_round_stats": {
          "1": {
            "mean": 0.8185185185185185,
            "std": 1.4803075833456172,
            "count": 27
          },
          "2": {
            "mean": 0.8765432098765432,
            "std": 1.5702037217386633,
            "count": 27
          },
          "3": {
            "mean": 0.8049382716049382,
            "std": 1.4754603425842316,
            "count": 27
          },
          "4": {
            "mean": 0.8530864197530864,
            "std": 1.533479905459967,
            "count": 27
          },
          "5": {
            "mean": 0.8617283950617284,
            "std": 1.5348293622604197,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.8185185185185185,
          "2": 0.8765432098765432,
          "3": 0.8049382716049382,
          "4": 0.8530864197530864,
          "5": 0.8617283950617284
        }
      },
      "llasmol-top5": {
        "best_round": "5",
        "best_score": 1.0913580246913581,
        "best_std": 1.6976502763836108,
        "all_round_stats": {
          "1": {
            "mean": 0.7469135802469136,
            "std": 1.3513291734360744,
            "count": 27
          },
          "2": {
            "mean": 0.7098765432098766,
            "std": 1.4092131166377322,
            "count": 27
          },
          "3": {
            "mean": 0.9148148148148147,
            "std": 1.7280821179179553,
            "count": 27
          },
          "4": {
            "mean": 0.7641975308641975,
            "std": 1.4131835287663719,
            "count": 27
          },
          "5": {
            "mean": 1.0913580246913581,
            "std": 1.6976502763836108,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.7469135802469136,
          "2": 0.7098765432098766,
          "3": 0.9148148148148147,
          "4": 0.7641975308641975,
          "5": 1.0913580246913581
        }
      },
      "MOSES": {
        "best_round": "4",
        "best_score": 8.71111111111111,
        "best_std": 0.5727426892018684,
        "all_round_stats": {
          "1": {
            "mean": 8.709876543209877,
            "std": 0.7510570792615955,
            "count": 27
          },
          "2": {
            "mean": 8.67283950617284,
            "std": 0.623337929301378,
            "count": 27
          },
          "3": {
            "mean": 8.669135802469135,
            "std": 0.8868905600262945,
            "count": 27
          },
          "4": {
            "mean": 8.71111111111111,
            "std": 0.5727426892018684,
            "count": 27
          },
          "5": {
            "mean": 8.69506172839506,
            "std": 0.634796131464019,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.709876543209877,
          "2": 8.67283950617284,
          "3": 8.669135802469135,
          "4": 8.71111111111111,
          "5": 8.69506172839506
        }
      },
      "MOSES-nano": {
        "best_round": "5",
        "best_score": 6.891358024691358,
        "best_std": 1.3533621505266058,
        "all_round_stats": {
          "1": {
            "mean": 6.766666666666667,
            "std": 1.464158694869065,
            "count": 27
          },
          "2": {
            "mean": 6.692592592592592,
            "std": 1.2511305428801727,
            "count": 27
          },
          "3": {
            "mean": 6.617283950617284,
            "std": 1.0682828449356436,
            "count": 27
          },
          "4": {
            "mean": 6.6802469135802465,
            "std": 1.2499661280627608,
            "count": 27
          },
          "5": {
            "mean": 6.891358024691358,
            "std": 1.3533621505266058,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.766666666666667,
          "2": 6.692592592592592,
          "3": 6.617283950617284,
          "4": 6.6802469135802465,
          "5": 6.891358024691358
        }
      },
      "o1": {
        "best_round": "4",
        "best_score": 7.327160493827161,
        "best_std": 0.8948588695028686,
        "all_round_stats": {
          "1": {
            "mean": 7.271604938271605,
            "std": 0.9758495620934219,
            "count": 27
          },
          "2": {
            "mean": 7.261728395061728,
            "std": 0.8912539760860568,
            "count": 27
          },
          "3": {
            "mean": 7.267901234567901,
            "std": 0.918307691087926,
            "count": 27
          },
          "4": {
            "mean": 7.327160493827161,
            "std": 0.8948588695028686,
            "count": 27
          },
          "5": {
            "mean": 7.266666666666667,
            "std": 1.0158571808900165,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.271604938271605,
          "2": 7.261728395061728,
          "3": 7.267901234567901,
          "4": 7.327160493827161,
          "5": 7.266666666666667
        }
      },
      "o3": {
        "best_round": "1",
        "best_score": 8.367901234567901,
        "best_std": 0.8784933431579184,
        "all_round_stats": {
          "1": {
            "mean": 8.367901234567901,
            "std": 0.8784933431579184,
            "count": 27
          },
          "2": {
            "mean": 8.285185185185185,
            "std": 0.8789724640696048,
            "count": 27
          },
          "3": {
            "mean": 8.308641975308642,
            "std": 0.7387094656091551,
            "count": 27
          },
          "4": {
            "mean": 8.27037037037037,
            "std": 0.8314508652413211,
            "count": 27
          },
          "5": {
            "mean": 8.33827160493827,
            "std": 0.886590690002217,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.367901234567901,
          "2": 8.285185185185185,
          "3": 8.308641975308642,
          "4": 8.27037037037037,
          "5": 8.33827160493827
        }
      },
      "spark-chem13b-nothink": {
        "best_round": "3",
        "best_score": 5.833333333333333,
        "best_std": 1.8411720533167262,
        "all_round_stats": {
          "1": {
            "mean": 5.374074074074074,
            "std": 1.7715784003227257,
            "count": 27
          },
          "2": {
            "mean": 5.745679012345679,
            "std": 1.7765121919699258,
            "count": 27
          },
          "3": {
            "mean": 5.833333333333333,
            "std": 1.8411720533167262,
            "count": 27
          },
          "4": {
            "mean": 5.590123456790123,
            "std": 1.4274720592357324,
            "count": 27
          },
          "5": {
            "mean": 5.7555555555555555,
            "std": 1.808999345626922,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.374074074074074,
          "2": 5.745679012345679,
          "3": 5.833333333333333,
          "4": 5.590123456790123,
          "5": 5.7555555555555555
        }
      },
      "spark-chem13b-think": {
        "best_round": "2",
        "best_score": 5.92962962962963,
        "best_std": 1.6607158721398456,
        "all_round_stats": {
          "1": {
            "mean": 5.559259259259259,
            "std": 1.8740543674290169,
            "count": 27
          },
          "2": {
            "mean": 5.92962962962963,
            "std": 1.6607158721398456,
            "count": 27
          },
          "3": {
            "mean": 5.676543209876543,
            "std": 1.7692485926129218,
            "count": 27
          },
          "4": {
            "mean": 5.534567901234568,
            "std": 1.513994023898419,
            "count": 27
          },
          "5": {
            "mean": 5.881481481481481,
            "std": 1.4558556485845942,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.559259259259259,
          "2": 5.92962962962963,
          "3": 5.676543209876543,
          "4": 5.534567901234568,
          "5": 5.881481481481481
        }
      },
      "darwin": {
        "best_round": "3",
        "best_score": 0.46172839506172836,
        "best_std": 0.8318296024403333,
        "all_round_stats": {
          "1": {
            "mean": 0.4049382716049383,
            "std": 0.7596503752544641,
            "count": 27
          },
          "2": {
            "mean": 0.25555555555555554,
            "std": 0.7527158803832934,
            "count": 27
          },
          "3": {
            "mean": 0.46172839506172836,
            "std": 0.8318296024403333,
            "count": 27
          },
          "4": {
            "mean": 0.45925925925925926,
            "std": 0.8387685995152439,
            "count": 27
          },
          "5": {
            "mean": 0.37777777777777777,
            "std": 0.40425091641059197,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.4049382716049383,
          "2": 0.25555555555555554,
          "3": 0.46172839506172836,
          "4": 0.45925925925925926,
          "5": 0.37777777777777777
        }
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "best_round": "1",
        "best_score": 8.137037037037036,
        "best_std": 0.6978803887752091,
        "all_round_stats": {
          "1": {
            "mean": 8.137037037037036,
            "std": 0.6978803887752091,
            "count": 27
          },
          "2": {
            "mean": 7.892592592592592,
            "std": 0.9589583617661089,
            "count": 27
          },
          "3": {
            "mean": 7.95925925925926,
            "std": 0.9099325293543518,
            "count": 27
          },
          "4": {
            "mean": 8.02716049382716,
            "std": 0.7881080361662557,
            "count": 27
          },
          "5": {
            "mean": 8.046913580246914,
            "std": 0.9697236922200717,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.137037037037036,
          "2": 7.892592592592592,
          "3": 7.95925925925926,
          "4": 8.02716049382716,
          "5": 8.046913580246914
        }
      },
      "gpt-4.1-nano": {
        "best_round": "4",
        "best_score": 6.785185185185186,
        "best_std": 1.5647105445797598,
        "all_round_stats": {
          "1": {
            "mean": 6.674074074074074,
            "std": 1.78635218561574,
            "count": 27
          },
          "2": {
            "mean": 6.777777777777778,
            "std": 1.5233790600778785,
            "count": 27
          },
          "3": {
            "mean": 6.637037037037037,
            "std": 1.5364609499470694,
            "count": 27
          },
          "4": {
            "mean": 6.785185185185186,
            "std": 1.5647105445797598,
            "count": 27
          },
          "5": {
            "mean": 6.708641975308642,
            "std": 1.4433340022676426,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.674074074074074,
          "2": 6.777777777777778,
          "3": 6.637037037037037,
          "4": 6.785185185185186,
          "5": 6.708641975308642
        }
      },
      "gpt-4o": {
        "best_round": "1",
        "best_score": 6.2555555555555555,
        "best_std": 1.493318452306808,
        "all_round_stats": {
          "1": {
            "mean": 6.2555555555555555,
            "std": 1.493318452306808,
            "count": 27
          },
          "2": {
            "mean": 6.167901234567902,
            "std": 1.39842651141164,
            "count": 27
          },
          "3": {
            "mean": 6.149382716049383,
            "std": 1.3240177901292203,
            "count": 27
          },
          "4": {
            "mean": 6.176543209876543,
            "std": 1.5607858752163188,
            "count": 27
          },
          "5": {
            "mean": 6.151851851851852,
            "std": 1.727909000980525,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.2555555555555555,
          "2": 6.167901234567902,
          "3": 6.149382716049383,
          "4": 6.176543209876543,
          "5": 6.151851851851852
        }
      },
      "gpt-4o-mini": {
        "best_round": "2",
        "best_score": 5.246913580246914,
        "best_std": 2.1603113747407168,
        "all_round_stats": {
          "1": {
            "mean": 5.104938271604938,
            "std": 2.1859243522640095,
            "count": 27
          },
          "2": {
            "mean": 5.246913580246914,
            "std": 2.1603113747407168,
            "count": 27
          },
          "3": {
            "mean": 4.974074074074074,
            "std": 2.2711933181301607,
            "count": 27
          },
          "4": {
            "mean": 4.897530864197531,
            "std": 2.1074942776721883,
            "count": 27
          },
          "5": {
            "mean": 5.14320987654321,
            "std": 2.0786627607818193,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.104938271604938,
          "2": 5.246913580246914,
          "3": 4.974074074074074,
          "4": 4.897530864197531,
          "5": 5.14320987654321
        }
      },
      "lightrag-4.1": {
        "best_round": "1",
        "best_score": 8.430864197530864,
        "best_std": 1.175749476408865,
        "all_round_stats": {
          "1": {
            "mean": 8.430864197530864,
            "std": 1.175749476408865,
            "count": 27
          },
          "2": {
            "mean": 7.820987654320987,
            "std": 1.5787896518532756,
            "count": 27
          },
          "3": {
            "mean": 7.713580246913581,
            "std": 1.481084704701521,
            "count": 27
          },
          "4": {
            "mean": 8.093827160493827,
            "std": 1.4899961864529019,
            "count": 27
          },
          "5": {
            "mean": 7.937037037037037,
            "std": 1.0641931791301438,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.430864197530864,
          "2": 7.820987654320987,
          "3": 7.713580246913581,
          "4": 8.093827160493827,
          "5": 7.937037037037037
        }
      },
      "lightrag-4.1-nano": {
        "best_round": "2",
        "best_score": 8.419753086419753,
        "best_std": 0.8575586127464778,
        "all_round_stats": {
          "1": {
            "mean": 7.241975308641975,
            "std": 1.800227729957732,
            "count": 27
          },
          "2": {
            "mean": 8.419753086419753,
            "std": 0.8575586127464778,
            "count": 27
          },
          "3": {
            "mean": 8.387037037037038,
            "std": 1.2399883971518546,
            "count": 27
          },
          "4": {
            "mean": 8.162962962962963,
            "std": 1.3124431882969725,
            "count": 27
          },
          "5": {
            "mean": 8.230864197530865,
            "std": 1.041076280936566,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.241975308641975,
          "2": 8.419753086419753,
          "3": 8.387037037037038,
          "4": 8.162962962962963,
          "5": 8.230864197530865
        }
      },
      "llasmol-top1": {
        "best_round": "4",
        "best_score": 0.7716049382716049,
        "best_std": 1.2965339627467813,
        "all_round_stats": {
          "1": {
            "mean": 0.7444444444444445,
            "std": 1.3513842222844383,
            "count": 27
          },
          "2": {
            "mean": 0.7533950617283951,
            "std": 1.3251396057304794,
            "count": 27
          },
          "3": {
            "mean": 0.7592592592592593,
            "std": 1.319781558373719,
            "count": 27
          },
          "4": {
            "mean": 0.7716049382716049,
            "std": 1.2965339627467813,
            "count": 27
          },
          "5": {
            "mean": 0.762962962962963,
            "std": 1.3511101270801635,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.7444444444444445,
          "2": 0.7533950617283951,
          "3": 0.7592592592592593,
          "4": 0.7716049382716049,
          "5": 0.762962962962963
        }
      },
      "llasmol-top5": {
        "best_round": "5",
        "best_score": 1.3,
        "best_std": 2.017890071802808,
        "all_round_stats": {
          "1": {
            "mean": 0.8296296296296296,
            "std": 1.3883315120908217,
            "count": 27
          },
          "2": {
            "mean": 0.7308641975308642,
            "std": 1.560131649220556,
            "count": 27
          },
          "3": {
            "mean": 0.9444444444444444,
            "std": 1.7925534478078646,
            "count": 27
          },
          "4": {
            "mean": 0.8888888888888888,
            "std": 1.543915546160221,
            "count": 27
          },
          "5": {
            "mean": 1.3,
            "std": 2.017890071802808,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.8296296296296296,
          "2": 0.7308641975308642,
          "3": 0.9444444444444444,
          "4": 0.8888888888888888,
          "5": 1.3
        }
      },
      "MOSES": {
        "best_round": "1",
        "best_score": 9.461728395061728,
        "best_std": 0.6695261561887104,
        "all_round_stats": {
          "1": {
            "mean": 9.461728395061728,
            "std": 0.6695261561887104,
            "count": 27
          },
          "2": {
            "mean": 9.255555555555556,
            "std": 0.8963363227153096,
            "count": 27
          },
          "3": {
            "mean": 9.380246913580248,
            "std": 0.8697803661883696,
            "count": 27
          },
          "4": {
            "mean": 9.41358024691358,
            "std": 0.7244956547715402,
            "count": 27
          },
          "5": {
            "mean": 9.378395061728396,
            "std": 0.7014303886061294,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 9.461728395061728,
          "2": 9.255555555555556,
          "3": 9.380246913580248,
          "4": 9.41358024691358,
          "5": 9.378395061728396
        }
      },
      "MOSES-nano": {
        "best_round": "2",
        "best_score": 7.15679012345679,
        "best_std": 1.930010734986637,
        "all_round_stats": {
          "1": {
            "mean": 6.623456790123457,
            "std": 2.3374837595270153,
            "count": 27
          },
          "2": {
            "mean": 7.15679012345679,
            "std": 1.930010734986637,
            "count": 27
          },
          "3": {
            "mean": 6.830864197530864,
            "std": 2.166211508916244,
            "count": 27
          },
          "4": {
            "mean": 6.611111111111111,
            "std": 2.3206063162918595,
            "count": 27
          },
          "5": {
            "mean": 6.822222222222222,
            "std": 1.9804385252102001,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.623456790123457,
          "2": 7.15679012345679,
          "3": 6.830864197530864,
          "4": 6.611111111111111,
          "5": 6.822222222222222
        }
      },
      "o1": {
        "best_round": "4",
        "best_score": 8.358024691358025,
        "best_std": 0.8528280050540642,
        "all_round_stats": {
          "1": {
            "mean": 8.308641975308642,
            "std": 0.9064626746664584,
            "count": 27
          },
          "2": {
            "mean": 8.228395061728396,
            "std": 0.9387072263101058,
            "count": 27
          },
          "3": {
            "mean": 8.235802469135802,
            "std": 1.0280645630531355,
            "count": 27
          },
          "4": {
            "mean": 8.358024691358025,
            "std": 0.8528280050540642,
            "count": 27
          },
          "5": {
            "mean": 8.325925925925926,
            "std": 1.0272498066401206,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.308641975308642,
          "2": 8.228395061728396,
          "3": 8.235802469135802,
          "4": 8.358024691358025,
          "5": 8.325925925925926
        }
      },
      "o3": {
        "best_round": "1",
        "best_score": 9.330864197530865,
        "best_std": 0.8035671500310685,
        "all_round_stats": {
          "1": {
            "mean": 9.330864197530865,
            "std": 0.8035671500310685,
            "count": 27
          },
          "2": {
            "mean": 9.107407407407408,
            "std": 0.9387814129554023,
            "count": 27
          },
          "3": {
            "mean": 9.317283950617284,
            "std": 0.7121875755492358,
            "count": 27
          },
          "4": {
            "mean": 9.264197530864198,
            "std": 0.7983918626329148,
            "count": 27
          },
          "5": {
            "mean": 9.3,
            "std": 0.767056756945709,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 9.330864197530865,
          "2": 9.107407407407408,
          "3": 9.317283950617284,
          "4": 9.264197530864198,
          "5": 9.3
        }
      },
      "spark-chem13b-nothink": {
        "best_round": "3",
        "best_score": 5.649382716049383,
        "best_std": 2.3759725021137945,
        "all_round_stats": {
          "1": {
            "mean": 5.3049382716049385,
            "std": 2.4511065651005364,
            "count": 27
          },
          "2": {
            "mean": 5.2444444444444445,
            "std": 2.6092209925635252,
            "count": 27
          },
          "3": {
            "mean": 5.649382716049383,
            "std": 2.3759725021137945,
            "count": 27
          },
          "4": {
            "mean": 5.119753086419753,
            "std": 2.5542533533498752,
            "count": 27
          },
          "5": {
            "mean": 5.461728395061728,
            "std": 2.381834811378116,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.3049382716049385,
          "2": 5.2444444444444445,
          "3": 5.649382716049383,
          "4": 5.119753086419753,
          "5": 5.461728395061728
        }
      },
      "spark-chem13b-think": {
        "best_round": "2",
        "best_score": 5.774074074074074,
        "best_std": 2.3700108790787366,
        "all_round_stats": {
          "1": {
            "mean": 5.692283950617284,
            "std": 2.6901300745466514,
            "count": 27
          },
          "2": {
            "mean": 5.774074074074074,
            "std": 2.3700108790787366,
            "count": 27
          },
          "3": {
            "mean": 5.556481481481481,
            "std": 2.2699640577246645,
            "count": 27
          },
          "4": {
            "mean": 5.564197530864198,
            "std": 2.421748431145224,
            "count": 27
          },
          "5": {
            "mean": 5.174074074074074,
            "std": 2.55293250742443,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.692283950617284,
          "2": 5.774074074074074,
          "3": 5.556481481481481,
          "4": 5.564197530864198,
          "5": 5.174074074074074
        }
      },
      "darwin": {
        "best_round": "4",
        "best_score": 0.7876543209876543,
        "best_std": 1.5803047457959847,
        "all_round_stats": {
          "1": {
            "mean": 0.43209876543209874,
            "std": 0.7327201096605588,
            "count": 27
          },
          "2": {
            "mean": 0.3197530864197531,
            "std": 1.1441512085257248,
            "count": 27
          },
          "3": {
            "mean": 0.5864197530864197,
            "std": 1.1810924704985264,
            "count": 27
          },
          "4": {
            "mean": 0.7876543209876543,
            "std": 1.5803047457959847,
            "count": 27
          },
          "5": {
            "mean": 0.33209876543209876,
            "std": 0.49072434526178266,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.43209876543209874,
          "2": 0.3197530864197531,
          "3": 0.5864197530864197,
          "4": 0.7876543209876543,
          "5": 0.33209876543209876
        }
      }
    }
  },
  "answer_round_volatility": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "round_scores": [
          7.345679012345679,
          7.31358024691358,
          7.325925925925926,
          7.407407407407407,
          7.312345679012346
        ],
        "mean": 7.340987654320988,
        "std": 0.039471435345069125,
        "coefficient_of_variation": 0.005376856249286265
      },
      "gpt-4.1-nano": {
        "round_scores": [
          6.703703703703703,
          6.825925925925926,
          6.7481481481481485,
          6.690123456790124,
          6.774074074074074
        ],
        "mean": 6.748395061728395,
        "std": 0.05491676649429286,
        "coefficient_of_variation": 0.0081377521606193
      },
      "gpt-4o": {
        "round_scores": [
          6.3654320987654325,
          6.276543209876543,
          6.266666666666667,
          6.260493827160494,
          6.238271604938271
        ],
        "mean": 6.281481481481482,
        "std": 0.04898761698525084,
        "coefficient_of_variation": 0.0077987361945859235
      },
      "gpt-4o-mini": {
        "round_scores": [
          5.5962962962962965,
          5.618518518518519,
          5.549382716049383,
          5.529629629629629,
          5.549382716049383
        ],
        "mean": 5.568641975308642,
        "std": 0.037135671131892936,
        "coefficient_of_variation": 0.006668712281477692
      },
      "lightrag-4.1": {
        "round_scores": [
          7.366666666666666,
          6.782716049382716,
          6.632098765432099,
          6.896296296296296,
          6.714814814814814
        ],
        "mean": 6.8785185185185185,
        "std": 0.2895273624086247,
        "coefficient_of_variation": 0.04209152910312765
      },
      "lightrag-4.1-nano": {
        "round_scores": [
          6.5320987654320986,
          7.212345679012346,
          7.230864197530864,
          7.2592592592592595,
          7.224691358024692
        ],
        "mean": 7.091851851851852,
        "std": 0.31338406422731424,
        "coefficient_of_variation": 0.04418931342248529
      },
      "llasmol-top1": {
        "round_scores": [
          0.8185185185185185,
          0.8765432098765432,
          0.8049382716049382,
          0.8530864197530864,
          0.8617283950617284
        ],
        "mean": 0.8429629629629629,
        "std": 0.030106755135195395,
        "coefficient_of_variation": 0.03571539493191018
      },
      "llasmol-top5": {
        "round_scores": [
          0.7469135802469136,
          0.7098765432098766,
          0.9148148148148147,
          0.7641975308641975,
          1.0913580246913581
        ],
        "mean": 0.8454320987654321,
        "std": 0.1580931564648456,
        "coefficient_of_variation": 0.1869968702344114
      },
      "MOSES": {
        "round_scores": [
          8.709876543209877,
          8.67283950617284,
          8.669135802469135,
          8.71111111111111,
          8.695061728395062
        ],
        "mean": 8.691604938271604,
        "std": 0.019895321242604572,
        "coefficient_of_variation": 0.0022890273296937167
      },
      "MOSES-nano": {
        "round_scores": [
          6.766666666666667,
          6.692592592592592,
          6.617283950617284,
          6.6802469135802465,
          6.891358024691358
        ],
        "mean": 6.729629629629629,
        "std": 0.10483291733917253,
        "coefficient_of_variation": 0.015577813803839618
      },
      "o1": {
        "round_scores": [
          7.271604938271605,
          7.261728395061728,
          7.267901234567901,
          7.327160493827161,
          7.266666666666667
        ],
        "mean": 7.279012345679012,
        "std": 0.027146461021353218,
        "coefficient_of_variation": 0.0037294154388222704
      },
      "o3": {
        "round_scores": [
          8.367901234567901,
          8.285185185185185,
          8.308641975308642,
          8.27037037037037,
          8.33827160493827
        ],
        "mean": 8.314074074074075,
        "std": 0.03955051523812631,
        "coefficient_of_variation": 0.004757055913352683
      },
      "spark-chem13b-nothink": {
        "round_scores": [
          5.374074074074074,
          5.745679012345679,
          5.833333333333333,
          5.590123456790123,
          5.7555555555555555
        ],
        "mean": 5.659753086419753,
        "std": 0.18245017359164492,
        "coefficient_of_variation": 0.03223641929352421
      },
      "spark-chem13b-think": {
        "round_scores": [
          5.559259259259259,
          5.92962962962963,
          5.676543209876543,
          5.534567901234568,
          5.881481481481481
        ],
        "mean": 5.716296296296297,
        "std": 0.18170264511610354,
        "coefficient_of_variation": 0.03178677865838276
      },
      "darwin": {
        "round_scores": [
          0.4049382716049383,
          0.25555555555555554,
          0.4617283950617284,
          0.45925925925925926,
          0.37777777777777777
        ],
        "mean": 0.39185185185185184,
        "std": 0.08422069845268775,
        "coefficient_of_variation": 0.21492994879230332
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "round_scores": [
          8.137037037037038,
          7.892592592592592,
          7.95925925925926,
          8.02716049382716,
          8.046913580246914
        ],
        "mean": 8.012592592592593,
        "std": 0.09235937707872335,
        "coefficient_of_variation": 0.011526778132224878
      },
      "gpt-4.1-nano": {
        "round_scores": [
          6.674074074074074,
          6.777777777777778,
          6.637037037037037,
          6.785185185185186,
          6.708641975308642
        ],
        "mean": 6.716543209876543,
        "std": 0.06451488511075215,
        "coefficient_of_variation": 0.009605370366096103
      },
      "gpt-4o": {
        "round_scores": [
          6.2555555555555555,
          6.167901234567902,
          6.149382716049383,
          6.176543209876543,
          6.151851851851852
        ],
        "mean": 6.180246913580247,
        "std": 0.043578673073389584,
        "coefficient_of_variation": 0.007051283497691882
      },
      "gpt-4o-mini": {
        "round_scores": [
          5.104938271604938,
          5.246913580246914,
          4.974074074074074,
          4.897530864197531,
          5.14320987654321
        ],
        "mean": 5.073333333333333,
        "std": 0.13853755288335623,
        "coefficient_of_variation": 0.027307007795668112
      },
      "lightrag-4.1": {
        "round_scores": [
          8.430864197530864,
          7.820987654320987,
          7.713580246913581,
          8.093827160493827,
          7.937037037037037
        ],
        "mean": 7.999259259259259,
        "std": 0.2795043733310305,
        "coefficient_of_variation": 0.03494128197026495
      },
      "lightrag-4.1-nano": {
        "round_scores": [
          7.241975308641975,
          8.419753086419753,
          8.387037037037038,
          8.162962962962963,
          8.230864197530865
        ],
        "mean": 8.08851851851852,
        "std": 0.4850951758937153,
        "coefficient_of_variation": 0.05997330348976745
      },
      "llasmol-top1": {
        "round_scores": [
          0.7444444444444445,
          0.7533950617283951,
          0.7592592592592593,
          0.7716049382716049,
          0.762962962962963
        ],
        "mean": 0.7583333333333333,
        "std": 0.010194533682045506,
        "coefficient_of_variation": 0.013443341119180887
      },
      "llasmol-top5": {
        "round_scores": [
          0.8296296296296296,
          0.7308641975308642,
          0.9444444444444444,
          0.8888888888888888,
          1.3
        ],
        "mean": 0.9387654320987654,
        "std": 0.21687777693093666,
        "coefficient_of_variation": 0.23102445990802037
      },
      "MOSES": {
        "round_scores": [
          9.461728395061728,
          9.255555555555556,
          9.380246913580248,
          9.41358024691358,
          9.378395061728394
        ],
        "mean": 9.3779012345679,
        "std": 0.0762794140097872,
        "coefficient_of_variation": 0.008133953653575866
      },
      "MOSES-nano": {
        "round_scores": [
          6.6234567901234565,
          7.1567901234567906,
          6.830864197530865,
          6.611111111111111,
          6.822222222222222
        ],
        "mean": 6.808888888888889,
        "std": 0.2209055644482656,
        "coefficient_of_variation": 0.03244370235043065
      },
      "o1": {
        "round_scores": [
          8.308641975308642,
          8.228395061728396,
          8.235802469135802,
          8.358024691358025,
          8.325925925925926
        ],
        "mean": 8.291358024691359,
        "std": 0.056984369890589234,
        "coefficient_of_variation": 0.006872742646125264
      },
      "o3": {
        "round_scores": [
          9.330864197530865,
          9.107407407407408,
          9.317283950617284,
          9.264197530864198,
          9.3
        ],
        "mean": 9.263950617283951,
        "std": 0.09100448778065895,
        "coefficient_of_variation": 0.009823507436543318
      },
      "spark-chem13b-nothink": {
        "round_scores": [
          5.3049382716049385,
          5.2444444444444445,
          5.649382716049383,
          5.119753086419753,
          5.461728395061728
        ],
        "mean": 5.35604938271605,
        "std": 0.20501002759851192,
        "coefficient_of_variation": 0.038276351271158644
      },
      "spark-chem13b-think": {
        "round_scores": [
          5.692283950617284,
          5.774074074074074,
          5.556481481481481,
          5.564197530864198,
          5.174074074074074
        ],
        "mean": 5.552222222222222,
        "std": 0.23021354390298693,
        "coefficient_of_variation": 0.04146331589207289
      },
      "darwin": {
        "round_scores": [
          0.43209876543209874,
          0.3197530864197531,
          0.5864197530864198,
          0.7876543209876543,
          0.33209876543209876
        ],
        "mean": 0.49160493827160495,
        "std": 0.19694628003579043,
        "coefficient_of_variation": 0.4006190025841041
      }
    }
  },
  "question_volatility": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "question_scores": {
          "q_1": 6.533333333333333,
          "q_2": 7.62,
          "q_3": 7.726666666666667,
          "q_4": 7.766666666666667,
          "q_5": 5.3,
          "q_6": 7.586666666666667,
          "q_7": 6.126666666666667,
          "q_8": 7.793333333333333,
          "q_9": 7.1,
          "q_10": 7.093333333333333,
          "q_11": 7.593333333333334,
          "q_12": 9.426666666666668,
          "q_13": 7.753333333333333,
          "q_14": 7.226666666666667,
          "q_15": 7.386666666666667,
          "q_16": 7.72,
          "q_17": 7.333333333333333,
          "q_18": 7.153333333333333,
          "q_19": 6.86,
          "q_20": 7.726666666666667,
          "q_21": 7.466666666666667,
          "q_22": 7.446666666666666,
          "q_23": 7.653333333333333,
          "q_24": 7.6,
          "q_25": 7.133333333333334,
          "q_26": 6.58,
          "q_27": 7.5
        },
        "mean": 7.340987654320988,
        "std": 0.7093266427863179,
        "coefficient_of_variation": 0.09662550547524763
      },
      "gpt-4.1-nano": {
        "question_scores": {
          "q_1": 6.273333333333333,
          "q_2": 7.253333333333333,
          "q_3": 7.166666666666667,
          "q_4": 7.48,
          "q_5": 6.56,
          "q_6": 6.846666666666667,
          "q_7": 5.926666666666667,
          "q_8": 7.246666666666667,
          "q_9": 7.1866666666666665,
          "q_10": 6.993333333333333,
          "q_11": 7.1,
          "q_12": 8.8,
          "q_13": 7.386666666666667,
          "q_14": 6.166666666666667,
          "q_15": 6.14,
          "q_16": 7.166666666666667,
          "q_17": 7.153333333333333,
          "q_18": 6.713333333333333,
          "q_19": 7.273333333333333,
          "q_20": 7.926666666666667,
          "q_21": 3.2266666666666666,
          "q_22": 6.3533333333333335,
          "q_23": 7.0,
          "q_24": 6.56,
          "q_25": 5.906666666666667,
          "q_26": 5.173333333333334,
          "q_27": 7.226666666666667
        },
        "mean": 6.748395061728395,
        "std": 0.9988413674050715,
        "coefficient_of_variation": 0.14801169141233542
      },
      "gpt-4o": {
        "question_scores": {
          "q_1": 6.193333333333333,
          "q_2": 7.2,
          "q_3": 6.586666666666666,
          "q_4": 7.566666666666666,
          "q_5": 4.013333333333334,
          "q_6": 6.8533333333333335,
          "q_7": 5.0,
          "q_8": 6.66,
          "q_9": 6.746666666666667,
          "q_10": 5.966666666666667,
          "q_11": 6.66,
          "q_12": 9.02,
          "q_13": 7.446666666666667,
          "q_14": 7.253333333333333,
          "q_15": 5.18,
          "q_16": 6.033333333333333,
          "q_17": 6.666666666666667,
          "q_18": 6.3933333333333335,
          "q_19": 6.346666666666667,
          "q_20": 6.726666666666667,
          "q_21": 3.413333333333333,
          "q_22": 5.193333333333333,
          "q_23": 6.6466666666666665,
          "q_24": 6.0,
          "q_25": 5.333333333333333,
          "q_26": 5.513333333333334,
          "q_27": 6.986666666666666
        },
        "mean": 6.281481481481482,
        "std": 1.1327484664827794,
        "coefficient_of_variation": 0.18033141860280097
      },
      "gpt-4o-mini": {
        "question_scores": {
          "q_1": 6.446666666666666,
          "q_2": 6.026666666666666,
          "q_3": 6.926666666666667,
          "q_4": 4.526666666666666,
          "q_5": 5.266666666666667,
          "q_6": 5.573333333333333,
          "q_7": 5.133333333333334,
          "q_8": 6.346666666666667,
          "q_9": 6.36,
          "q_10": 6.26,
          "q_11": 6.133333333333334,
          "q_12": 8.806666666666667,
          "q_13": 7.373333333333333,
          "q_14": 6.926666666666667,
          "q_15": 4.286666666666666,
          "q_16": 5.22,
          "q_17": 6.253333333333333,
          "q_18": 4.053333333333334,
          "q_19": 4.453333333333333,
          "q_20": 6.733333333333333,
          "q_21": 2.6066666666666665,
          "q_22": 4.24,
          "q_23": 5.253333333333334,
          "q_24": 5.74,
          "q_25": 4.2,
          "q_26": 3.04,
          "q_27": 6.166666666666667
        },
        "mean": 5.568641975308642,
        "std": 1.3619603955420319,
        "coefficient_of_variation": 0.2445767570587163
      },
      "lightrag-4.1": {
        "question_scores": {
          "q_1": 6.793333333333333,
          "q_2": 7.886666666666667,
          "q_3": 6.62,
          "q_4": 7.92,
          "q_5": 7.746666666666667,
          "q_6": 6.226666666666667,
          "q_7": 6.26,
          "q_8": 6.8533333333333335,
          "q_9": 7.306666666666667,
          "q_10": 6.52,
          "q_11": 7.9,
          "q_12": 9.066666666666666,
          "q_13": 6.626666666666667,
          "q_14": 6.9399999999999995,
          "q_15": 7.38,
          "q_16": 5.46,
          "q_17": 7.413333333333333,
          "q_18": 6.733333333333333,
          "q_19": 7.673333333333333,
          "q_20": 7.066666666666666,
          "q_21": 6.053333333333333,
          "q_22": 6.233333333333333,
          "q_23": 6.1866666666666665,
          "q_24": 5.6066666666666665,
          "q_25": 5.513333333333334,
          "q_26": 6.44,
          "q_27": 7.293333333333334
        },
        "mean": 6.8785185185185185,
        "std": 0.8459604026483762,
        "coefficient_of_variation": 0.12298584358984578
      },
      "lightrag-4.1-nano": {
        "question_scores": {
          "q_1": 6.533333333333333,
          "q_2": 7.866666666666667,
          "q_3": 7.006666666666667,
          "q_4": 7.926666666666667,
          "q_5": 8.106666666666666,
          "q_6": 6.133333333333333,
          "q_7": 7.253333333333333,
          "q_8": 7.446666666666666,
          "q_9": 6.793333333333333,
          "q_10": 6.94,
          "q_11": 6.773333333333333,
          "q_12": 8.5,
          "q_13": 7.553333333333334,
          "q_14": 7.22,
          "q_15": 7.113333333333333,
          "q_16": 5.62,
          "q_17": 7.426666666666667,
          "q_18": 5.74,
          "q_19": 7.486666666666666,
          "q_20": 7.746666666666667,
          "q_21": 7.26,
          "q_22": 7.166666666666667,
          "q_23": 7.053333333333334,
          "q_24": 6.746666666666667,
          "q_25": 6.193333333333333,
          "q_26": 5.96,
          "q_27": 7.913333333333333
        },
        "mean": 7.091851851851851,
        "std": 0.7272647111705338,
        "coefficient_of_variation": 0.10254933779822652
      },
      "llasmol-top1": {
        "question_scores": {
          "q_1": 1.1866666666666668,
          "q_2": 4.1066666666666665,
          "q_3": 3.7666666666666666,
          "q_4": 4.546666666666667,
          "q_5": 0.0,
          "q_6": 0.0,
          "q_7": 0.0,
          "q_8": 0.0,
          "q_9": 1.7333333333333332,
          "q_10": 0.0,
          "q_11": 0.0,
          "q_12": 0.0,
          "q_13": 0.8866666666666667,
          "q_14": 0.0,
          "q_15": 0.0,
          "q_16": 0.0,
          "q_17": 0.0,
          "q_18": 3.8000000000000003,
          "q_19": 0.0,
          "q_20": 2.7333333333333334,
          "q_21": 0.0,
          "q_22": 0.0,
          "q_23": 0.0,
          "q_24": 0.0,
          "q_25": 0.0,
          "q_26": 0.0,
          "q_27": 0.0
        },
        "mean": 0.842962962962963,
        "std": 1.5146558474393899,
        "coefficient_of_variation": 1.7968237206003306
      },
      "llasmol-top5": {
        "question_scores": {
          "q_1": 4.38,
          "q_2": 0.9666666666666667,
          "q_3": 2.1266666666666665,
          "q_4": 4.713333333333334,
          "q_5": 0.0,
          "q_6": 0.0,
          "q_7": 0.0,
          "q_8": 0.0,
          "q_9": 2.4066666666666667,
          "q_10": 0.06,
          "q_11": 0.0,
          "q_12": 0.0,
          "q_13": 0.38,
          "q_14": 0.2,
          "q_15": 0.92,
          "q_16": 0.0,
          "q_17": 0.9866666666666667,
          "q_18": 1.4866666666666666,
          "q_19": 0.0,
          "q_20": 3.493333333333333,
          "q_21": 0.10666666666666666,
          "q_22": 0.0,
          "q_23": 0.0,
          "q_24": 0.0,
          "q_25": 0.0,
          "q_26": 0.06,
          "q_27": 0.54
        },
        "mean": 0.8454320987654321,
        "std": 1.3878422495112086,
        "coefficient_of_variation": 1.6415774271379657
      },
      "MOSES": {
        "question_scores": {
          "q_1": 8.973333333333333,
          "q_2": 8.786666666666667,
          "q_3": 8.633333333333333,
          "q_4": 8.133333333333333,
          "q_5": 8.566666666666666,
          "q_6": 8.52,
          "q_7": 9.193333333333333,
          "q_8": 8.64,
          "q_9": 8.313333333333333,
          "q_10": 9.253333333333334,
          "q_11": 8.753333333333334,
          "q_12": 9.033333333333333,
          "q_13": 8.726666666666667,
          "q_14": 9.04,
          "q_15": 9.146666666666667,
          "q_16": 7.673333333333333,
          "q_17": 8.526666666666667,
          "q_18": 8.266666666666667,
          "q_19": 8.58,
          "q_20": 8.793333333333333,
          "q_21": 9.013333333333334,
          "q_22": 8.693333333333333,
          "q_23": 8.86,
          "q_24": 8.953333333333333,
          "q_25": 8.42,
          "q_26": 8.513333333333334,
          "q_27": 8.666666666666666
        },
        "mean": 8.691604938271604,
        "std": 0.3500456262939105,
        "coefficient_of_variation": 0.040273991832343904
      },
      "MOSES-nano": {
        "question_scores": {
          "q_1": 7.04,
          "q_2": 6.66,
          "q_3": 6.626666666666667,
          "q_4": 7.446666666666666,
          "q_5": 7.42,
          "q_6": 6.966666666666667,
          "q_7": 6.933333333333334,
          "q_8": 6.386666666666667,
          "q_9": 6.233333333333333,
          "q_10": 8.08,
          "q_11": 6.72,
          "q_12": 6.78,
          "q_13": 6.7,
          "q_14": 6.566666666666666,
          "q_15": 5.86,
          "q_16": 6.473333333333334,
          "q_17": 6.3933333333333335,
          "q_18": 6.506666666666667,
          "q_19": 7.1466666666666665,
          "q_20": 6.14,
          "q_21": 7.24,
          "q_22": 6.72,
          "q_23": 5.6066666666666665,
          "q_24": 6.833333333333333,
          "q_25": 6.46,
          "q_26": 6.6466666666666665,
          "q_27": 7.113333333333333
        },
        "mean": 6.729629629629629,
        "std": 0.5069983170063718,
        "coefficient_of_variation": 0.07533821991839317
      },
      "o1": {
        "question_scores": {
          "q_1": 6.88,
          "q_2": 7.6866666666666665,
          "q_3": 7.733333333333333,
          "q_4": 7.433333333333334,
          "q_5": 5.12,
          "q_6": 7.6466666666666665,
          "q_7": 6.706666666666667,
          "q_8": 7.693333333333333,
          "q_9": 6.753333333333333,
          "q_10": 6.293333333333333,
          "q_11": 7.093333333333333,
          "q_12": 8.78,
          "q_13": 7.5,
          "q_14": 7.966666666666667,
          "q_15": 6.546666666666667,
          "q_16": 8.193333333333333,
          "q_17": 7.6,
          "q_18": 7.52,
          "q_19": 7.0600000000000005,
          "q_20": 8.46,
          "q_21": 4.666666666666666,
          "q_22": 7.180000000000001,
          "q_23": 7.713333333333333,
          "q_24": 7.84,
          "q_25": 6.953333333333333,
          "q_26": 7.466666666666667,
          "q_27": 8.046666666666667
        },
        "mean": 7.279012345679012,
        "std": 0.8977021067655433,
        "coefficient_of_variation": 0.12332746039350238
      },
      "o3": {
        "question_scores": {
          "q_1": 8.113333333333333,
          "q_2": 8.846666666666668,
          "q_3": 8.393333333333333,
          "q_4": 7.513333333333334,
          "q_5": 6.806666666666667,
          "q_6": 6.913333333333333,
          "q_7": 8.280000000000001,
          "q_8": 8.78,
          "q_9": 9.273333333333333,
          "q_10": 8.02,
          "q_11": 7.766666666666667,
          "q_12": 9.673333333333334,
          "q_13": 8.293333333333333,
          "q_14": 7.846666666666667,
          "q_15": 8.173333333333334,
          "q_16": 8.28,
          "q_17": 8.7,
          "q_18": 8.273333333333333,
          "q_19": 9.513333333333334,
          "q_20": 9.84,
          "q_21": 8.126666666666667,
          "q_22": 7.566666666666666,
          "q_23": 8.1,
          "q_24": 8.1,
          "q_25": 8.026666666666667,
          "q_26": 8.14,
          "q_27": 9.12
        },
        "mean": 8.314074074074075,
        "std": 0.7385764393041229,
        "coefficient_of_variation": 0.08883447906811884
      },
      "spark-chem13b-nothink": {
        "question_scores": {
          "q_1": 3.6799999999999997,
          "q_2": 7.633333333333334,
          "q_3": 2.6133333333333333,
          "q_4": 4.66,
          "q_5": 5.9799999999999995,
          "q_6": 7.3,
          "q_7": 6.62,
          "q_8": 7.24,
          "q_9": 6.826666666666666,
          "q_10": 7.1066666666666665,
          "q_11": 5.666666666666667,
          "q_12": 6.12,
          "q_13": 3.6,
          "q_14": 7.213333333333334,
          "q_15": 6.253333333333333,
          "q_16": 3.466666666666667,
          "q_17": 7.193333333333333,
          "q_18": 7.653333333333333,
          "q_19": 4.306666666666667,
          "q_20": 6.76,
          "q_21": 6.306666666666667,
          "q_22": 4.54,
          "q_23": 3.76,
          "q_24": 4.973333333333334,
          "q_25": 5.413333333333333,
          "q_26": 4.0600000000000005,
          "q_27": 5.866666666666666
        },
        "mean": 5.659753086419753,
        "std": 1.4880946131595485,
        "coefficient_of_variation": 0.262925712559819
      },
      "spark-chem13b-think": {
        "question_scores": {
          "q_1": 3.6133333333333333,
          "q_2": 7.533333333333333,
          "q_3": 3.0733333333333333,
          "q_4": 6.806666666666667,
          "q_5": 6.253333333333333,
          "q_6": 7.22,
          "q_7": 6.866666666666666,
          "q_8": 7.006666666666667,
          "q_9": 6.173333333333334,
          "q_10": 6.633333333333333,
          "q_11": 4.546666666666667,
          "q_12": 5.8999999999999995,
          "q_13": 3.02,
          "q_14": 7.58,
          "q_15": 6.26,
          "q_16": 3.1733333333333333,
          "q_17": 6.786666666666666,
          "q_18": 6.033333333333333,
          "q_19": 4.153333333333333,
          "q_20": 6.2,
          "q_21": 6.52,
          "q_22": 5.333333333333333,
          "q_23": 5.013333333333334,
          "q_24": 6.546666666666667,
          "q_25": 5.453333333333333,
          "q_26": 4.806666666666667,
          "q_27": 5.833333333333334
        },
        "mean": 5.716296296296297,
        "std": 1.361603294929743,
        "coefficient_of_variation": 0.23819676663925785
      },
      "darwin": {
        "question_scores": {
          "q_1": 0.0,
          "q_2": 0.2533333333333333,
          "q_3": 0.006666666666666666,
          "q_4": 0.42,
          "q_5": 0.14666666666666667,
          "q_6": 0.3133333333333333,
          "q_7": 0.7533333333333333,
          "q_8": 0.28,
          "q_9": 0.9266666666666666,
          "q_10": 0.25333333333333335,
          "q_11": 0.13999999999999999,
          "q_12": 1.46,
          "q_13": 0.44666666666666666,
          "q_14": 0.18,
          "q_15": 0.18,
          "q_16": 0.026666666666666665,
          "q_17": 0.0,
          "q_18": 0.4066666666666667,
          "q_19": 0.060000000000000005,
          "q_20": 0.2733333333333334,
          "q_21": 1.6,
          "q_22": 0.7266666666666667,
          "q_23": 0.1,
          "q_24": 0.25333333333333335,
          "q_25": 0.5133333333333333,
          "q_26": 0.08666666666666667,
          "q_27": 0.7733333333333333
        },
        "mean": 0.39185185185185184,
        "std": 0.41484559307875757,
        "coefficient_of_variation": 1.0586796798796272
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "question_scores": {
          "q_1": 6.266666666666667,
          "q_2": 8.0,
          "q_3": 8.373333333333333,
          "q_4": 8.313333333333333,
          "q_5": 6.286666666666666,
          "q_6": 8.353333333333333,
          "q_7": 8.366666666666667,
          "q_8": 8.8,
          "q_9": 7.046666666666667,
          "q_10": 8.466666666666667,
          "q_11": 7.96,
          "q_12": 9.486666666666666,
          "q_13": 8.313333333333333,
          "q_14": 6.88,
          "q_15": 7.82,
          "q_16": 8.773333333333333,
          "q_17": 8.28,
          "q_18": 8.3,
          "q_19": 7.6466666666666665,
          "q_20": 7.986666666666667,
          "q_21": 8.413333333333334,
          "q_22": 8.26,
          "q_23": 8.46,
          "q_24": 8.586666666666666,
          "q_25": 8.606666666666667,
          "q_26": 6.96,
          "q_27": 7.333333333333333
        },
        "mean": 8.012592592592593,
        "std": 0.7690072039793124,
        "coefficient_of_variation": 0.09597482900730994
      },
      "gpt-4.1-nano": {
        "question_scores": {
          "q_1": 5.966666666666667,
          "q_2": 7.746666666666667,
          "q_3": 7.6066666666666665,
          "q_4": 7.88,
          "q_5": 7.846666666666667,
          "q_6": 7.92,
          "q_7": 6.4,
          "q_8": 8.186666666666666,
          "q_9": 6.9,
          "q_10": 7.16,
          "q_11": 7.866666666666667,
          "q_12": 9.433333333333334,
          "q_13": 7.573333333333333,
          "q_14": 3.1133333333333333,
          "q_15": 5.293333333333333,
          "q_16": 6.62,
          "q_17": 7.1000000000000005,
          "q_18": 5.693333333333333,
          "q_19": 6.8133333333333335,
          "q_20": 7.833333333333333,
          "q_21": 3.453333333333333,
          "q_22": 5.140000000000001,
          "q_23": 7.14,
          "q_24": 6.8933333333333335,
          "q_25": 6.16,
          "q_26": 4.34,
          "q_27": 7.266666666666667
        },
        "mean": 6.716543209876543,
        "std": 1.4610058242047637,
        "coefficient_of_variation": 0.217523475774917
      },
      "gpt-4o": {
        "question_scores": {
          "q_1": 6.066666666666666,
          "q_2": 7.96,
          "q_3": 6.72,
          "q_4": 6.873333333333333,
          "q_5": 3.7266666666666666,
          "q_6": 7.706666666666666,
          "q_7": 4.7,
          "q_8": 6.08,
          "q_9": 6.72,
          "q_10": 6.173333333333334,
          "q_11": 7.086666666666667,
          "q_12": 9.1,
          "q_13": 8.453333333333333,
          "q_14": 6.8133333333333335,
          "q_15": 3.2866666666666666,
          "q_16": 5.593333333333334,
          "q_17": 6.273333333333333,
          "q_18": 6.793333333333333,
          "q_19": 5.033333333333333,
          "q_20": 6.833333333333333,
          "q_21": 3.62,
          "q_22": 4.34,
          "q_23": 6.113333333333333,
          "q_24": 5.546666666666667,
          "q_25": 6.286666666666667,
          "q_26": 5.8933333333333335,
          "q_27": 7.073333333333333
        },
        "mean": 6.1802469135802465,
        "std": 1.4113719838723473,
        "coefficient_of_variation": 0.2283682195238916
      },
      "gpt-4o-mini": {
        "question_scores": {
          "q_1": 6.073333333333333,
          "q_2": 6.62,
          "q_3": 7.613333333333333,
          "q_4": 5.42,
          "q_5": 6.36,
          "q_6": 6.866666666666666,
          "q_7": 4.526666666666666,
          "q_8": 5.4399999999999995,
          "q_9": 5.386666666666667,
          "q_10": 6.886666666666667,
          "q_11": 6.42,
          "q_12": 9.280000000000001,
          "q_13": 8.266666666666667,
          "q_14": 4.666666666666667,
          "q_15": 2.993333333333333,
          "q_16": 3.7600000000000002,
          "q_17": 6.633333333333334,
          "q_18": 2.14,
          "q_19": 2.16,
          "q_20": 5.8533333333333335,
          "q_21": 1.7466666666666668,
          "q_22": 2.506666666666667,
          "q_23": 3.9266666666666667,
          "q_24": 5.6466666666666665,
          "q_25": 2.8333333333333335,
          "q_26": 1.44,
          "q_27": 5.513333333333334
        },
        "mean": 5.073333333333333,
        "std": 2.0794139542578804,
        "coefficient_of_variation": 0.4098713444660737
      },
      "lightrag-4.1": {
        "question_scores": {
          "q_1": 8.693333333333333,
          "q_2": 8.706666666666667,
          "q_3": 8.493333333333334,
          "q_4": 8.746666666666666,
          "q_5": 9.406666666666666,
          "q_6": 7.96,
          "q_7": 7.78,
          "q_8": 8.2,
          "q_9": 8.553333333333333,
          "q_10": 9.246666666666668,
          "q_11": 8.933333333333334,
          "q_12": 9.06,
          "q_13": 8.26,
          "q_14": 7.873333333333333,
          "q_15": 8.66,
          "q_16": 4.746666666666666,
          "q_17": 8.366666666666667,
          "q_18": 7.4,
          "q_19": 8.593333333333334,
          "q_20": 8.846666666666666,
          "q_21": 6.546666666666667,
          "q_22": 7.753333333333333,
          "q_23": 7.533333333333333,
          "q_24": 7.08,
          "q_25": 7.02,
          "q_26": 6.293333333333333,
          "q_27": 7.226666666666667
        },
        "mean": 7.999259259259259,
        "std": 1.039227743083441,
        "coefficient_of_variation": 0.1299154970981244
      },
      "lightrag-4.1-nano": {
        "question_scores": {
          "q_1": 8.473333333333333,
          "q_2": 8.94,
          "q_3": 8.926666666666666,
          "q_4": 9.433333333333334,
          "q_5": 9.68,
          "q_6": 7.7,
          "q_7": 8.053333333333333,
          "q_8": 8.953333333333333,
          "q_9": 7.923333333333334,
          "q_10": 9.073333333333332,
          "q_11": 7.453333333333333,
          "q_12": 9.293333333333333,
          "q_13": 8.62,
          "q_14": 7.746666666666667,
          "q_15": 8.086666666666666,
          "q_16": 4.913333333333333,
          "q_17": 8.413333333333334,
          "q_18": 5.866666666666666,
          "q_19": 8.72,
          "q_20": 8.56,
          "q_21": 8.3,
          "q_22": 8.213333333333333,
          "q_23": 8.353333333333333,
          "q_24": 6.92,
          "q_25": 6.706666666666666,
          "q_26": 6.726666666666667,
          "q_27": 8.34
        },
        "mean": 8.088518518518518,
        "std": 1.0943351934711392,
        "coefficient_of_variation": 0.1352948863213552
      },
      "llasmol-top1": {
        "question_scores": {
          "q_1": 1.72,
          "q_2": 1.3,
          "q_3": 3.0533333333333332,
          "q_4": 5.28,
          "q_5": 0.06666666666666667,
          "q_6": 0.0,
          "q_7": 0.23333333333333334,
          "q_8": 0.06666666666666667,
          "q_9": 2.32,
          "q_10": 0.019999999999999997,
          "q_11": 0.0,
          "q_12": 0.03333333333333333,
          "q_13": 1.815,
          "q_14": 0.0,
          "q_15": 0.0,
          "q_16": 0.006666666666666666,
          "q_17": 0.0,
          "q_18": 2.1866666666666665,
          "q_19": 0.006666666666666666,
          "q_20": 2.3466666666666667,
          "q_21": 0.0,
          "q_22": 0.0,
          "q_23": 0.0,
          "q_24": 0.0,
          "q_25": 0.0,
          "q_26": 0.02,
          "q_27": 0.0
        },
        "mean": 0.7583333333333333,
        "std": 1.3207381871417057,
        "coefficient_of_variation": 1.7416327742527988
      },
      "llasmol-top5": {
        "question_scores": {
          "q_1": 3.493333333333333,
          "q_2": 0.5333333333333333,
          "q_3": 1.56,
          "q_4": 4.906666666666666,
          "q_5": 0.0,
          "q_6": 0.006666666666666666,
          "q_7": 0.06666666666666667,
          "q_8": 0.16,
          "q_9": 3.24,
          "q_10": 0.06666666666666667,
          "q_11": 0.22000000000000003,
          "q_12": 0.013333333333333332,
          "q_13": 1.1133333333333333,
          "q_14": 0.22666666666666666,
          "q_15": 1.5133333333333334,
          "q_16": 0.35333333333333333,
          "q_17": 0.9266666666666667,
          "q_18": 0.9466666666666667,
          "q_19": 0.0,
          "q_20": 4.24,
          "q_21": 0.47333333333333333,
          "q_22": 0.0,
          "q_23": 0.0,
          "q_24": 0.0,
          "q_25": 0.013333333333333332,
          "q_26": 0.26,
          "q_27": 1.0133333333333332
        },
        "mean": 0.9387654320987654,
        "std": 1.3942188782499334,
        "coefficient_of_variation": 1.485162140166289
      },
      "MOSES": {
        "question_scores": {
          "q_1": 9.92,
          "q_2": 9.636666666666667,
          "q_3": 9.853333333333333,
          "q_4": 9.753333333333334,
          "q_5": 9.566666666666666,
          "q_6": 9.766666666666666,
          "q_7": 9.846666666666668,
          "q_8": 9.393333333333333,
          "q_9": 8.92,
          "q_10": 9.693333333333333,
          "q_11": 9.786666666666667,
          "q_12": 9.593333333333334,
          "q_13": 9.406666666666666,
          "q_14": 9.606666666666667,
          "q_15": 9.7,
          "q_16": 8.566666666666666,
          "q_17": 8.866666666666667,
          "q_18": 8.113333333333333,
          "q_19": 9.386666666666667,
          "q_20": 9.553333333333333,
          "q_21": 9.573333333333334,
          "q_22": 9.106666666666667,
          "q_23": 9.546666666666667,
          "q_24": 9.473333333333334,
          "q_25": 8.946666666666667,
          "q_26": 8.780000000000001,
          "q_27": 8.846666666666668
        },
        "mean": 9.3779012345679,
        "std": 0.45313452291027,
        "coefficient_of_variation": 0.048319395947567664
      },
      "MOSES-nano": {
        "question_scores": {
          "q_1": 8.326666666666666,
          "q_2": 7.053333333333334,
          "q_3": 7.886666666666667,
          "q_4": 8.34,
          "q_5": 7.193333333333333,
          "q_6": 7.766666666666667,
          "q_7": 8.193333333333333,
          "q_8": 8.213333333333333,
          "q_9": 8.473333333333333,
          "q_10": 8.34,
          "q_11": 6.8933333333333335,
          "q_12": 5.78,
          "q_13": 6.793333333333334,
          "q_14": 6.293333333333334,
          "q_15": 3.96,
          "q_16": 4.866666666666666,
          "q_17": 6.306666666666667,
          "q_18": 7.1066666666666665,
          "q_19": 7.02,
          "q_20": 5.666666666666667,
          "q_21": 5.493333333333333,
          "q_22": 6.333333333333333,
          "q_23": 5.56,
          "q_24": 7.0,
          "q_25": 6.306666666666667,
          "q_26": 6.04,
          "q_27": 6.633333333333333
        },
        "mean": 6.808888888888889,
        "std": 1.1648029069185104,
        "coefficient_of_variation": 0.17107092301348878
      },
      "o1": {
        "question_scores": {
          "q_1": 6.133333333333334,
          "q_2": 8.066666666666666,
          "q_3": 8.713333333333333,
          "q_4": 8.48,
          "q_5": 6.413333333333333,
          "q_6": 8.533333333333333,
          "q_7": 8.566666666666666,
          "q_8": 8.693333333333333,
          "q_9": 7.866666666666666,
          "q_10": 8.026666666666667,
          "q_11": 8.553333333333333,
          "q_12": 9.546666666666667,
          "q_13": 8.08,
          "q_14": 8.453333333333333,
          "q_15": 7.466666666666667,
          "q_16": 9.526666666666667,
          "q_17": 8.62,
          "q_18": 8.64,
          "q_19": 8.286666666666667,
          "q_20": 9.206666666666667,
          "q_21": 6.1866666666666665,
          "q_22": 8.166666666666666,
          "q_23": 8.906666666666666,
          "q_24": 8.88,
          "q_25": 8.56,
          "q_26": 8.42,
          "q_27": 8.873333333333333
        },
        "mean": 8.291358024691357,
        "std": 0.8678252902569867,
        "coefficient_of_variation": 0.10466624257119704
      },
      "o3": {
        "question_scores": {
          "q_1": 8.333333333333334,
          "q_2": 9.746666666666666,
          "q_3": 9.74,
          "q_4": 8.62,
          "q_5": 6.753333333333333,
          "q_6": 7.88,
          "q_7": 9.766666666666667,
          "q_8": 9.826666666666666,
          "q_9": 9.64,
          "q_10": 9.573333333333334,
          "q_11": 8.646666666666667,
          "q_12": 9.546666666666667,
          "q_13": 9.266666666666666,
          "q_14": 8.4,
          "q_15": 9.16,
          "q_16": 9.846666666666668,
          "q_17": 9.833333333333334,
          "q_18": 9.393333333333333,
          "q_19": 9.466666666666667,
          "q_20": 9.96,
          "q_21": 9.706666666666667,
          "q_22": 9.486666666666666,
          "q_23": 9.686666666666667,
          "q_24": 9.5,
          "q_25": 9.173333333333334,
          "q_26": 9.433333333333334,
          "q_27": 9.74
        },
        "mean": 9.263950617283951,
        "std": 0.7320460946190755,
        "coefficient_of_variation": 0.07902094094211615
      },
      "spark-chem13b-nothink": {
        "question_scores": {
          "q_1": 6.193333333333333,
          "q_2": 8.52,
          "q_3": 1.4866666666666668,
          "q_4": 4.713333333333333,
          "q_5": 5.526666666666667,
          "q_6": 7.96,
          "q_7": 6.746666666666667,
          "q_8": 3.88,
          "q_9": 8.673333333333334,
          "q_10": 6.86,
          "q_11": 4.2,
          "q_12": 3.6133333333333333,
          "q_13": 5.073333333333333,
          "q_14": 8.76,
          "q_15": 5.633333333333333,
          "q_16": 1.8333333333333333,
          "q_17": 7.473333333333334,
          "q_18": 8.566666666666666,
          "q_19": 2.06,
          "q_20": 4.6066666666666665,
          "q_21": 7.326666666666666,
          "q_22": 4.766666666666667,
          "q_23": 3.9333333333333336,
          "q_24": 3.7733333333333334,
          "q_25": 4.746666666666667,
          "q_26": 2.5933333333333333,
          "q_27": 5.093333333333334
        },
        "mean": 5.35604938271605,
        "std": 2.16494152958628,
        "coefficient_of_variation": 0.4042049232354985
      },
      "spark-chem13b-think": {
        "question_scores": {
          "q_1": 6.053333333333333,
          "q_2": 8.373333333333333,
          "q_3": 1.7066666666666666,
          "q_4": 5.08,
          "q_5": 6.733333333333333,
          "q_6": 7.5,
          "q_7": 7.0200000000000005,
          "q_8": 4.966666666666667,
          "q_9": 8.2,
          "q_10": 5.6466666666666665,
          "q_11": 3.3066666666666666,
          "q_12": 3.1733333333333333,
          "q_13": 4.8,
          "q_14": 9.233333333333334,
          "q_15": 6.06,
          "q_16": 1.6266666666666667,
          "q_17": 7.32,
          "q_18": 8.326666666666666,
          "q_19": 1.8466666666666667,
          "q_20": 5.126666666666667,
          "q_21": 7.526666666666666,
          "q_22": 6.126666666666667,
          "q_23": 4.298333333333334,
          "q_24": 5.9,
          "q_25": 4.746666666666667,
          "q_26": 3.2466666666666666,
          "q_27": 5.965
        },
        "mean": 5.552222222222222,
        "std": 2.1049759423681587,
        "coefficient_of_variation": 0.37912314351237597
      },
      "darwin": {
        "question_scores": {
          "q_1": 0.0,
          "q_2": 0.12000000000000001,
          "q_3": 0.0,
          "q_4": 1.02,
          "q_5": 0.06,
          "q_6": 0.060000000000000005,
          "q_7": 1.0933333333333333,
          "q_8": 0.11333333333333333,
          "q_9": 1.3533333333333335,
          "q_10": 0.29333333333333333,
          "q_11": 0.27999999999999997,
          "q_12": 0.86,
          "q_13": 0.78,
          "q_14": 0.03333333333333333,
          "q_15": 0.08666666666666667,
          "q_16": 0.013333333333333332,
          "q_17": 0.0,
          "q_18": 0.3,
          "q_19": 0.0,
          "q_20": 0.48666666666666664,
          "q_21": 2.1466666666666665,
          "q_22": 1.1466666666666667,
          "q_23": 0.13333333333333333,
          "q_24": 0.44666666666666666,
          "q_25": 1.0,
          "q_26": 0.08666666666666667,
          "q_27": 1.3599999999999999
        },
        "mean": 0.49160493827160495,
        "std": 0.5691426828827276,
        "coefficient_of_variation": 1.1577236894400034
      }
    }
  },
  "judge_volatility": {
    "gpt-4.1": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 3.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.848528137423857
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.5210260492953509
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1958141855593517
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.3604858100166714
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.34569664858008986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.261891400439462
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3057759053779665
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.0,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5364258340035878
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 4.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.5533879157112113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5303300858899107
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3193385463423118
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.4,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.538748023761179
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.13468700594029476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.4,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.6,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.6148754619013457
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.6,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3689252771408074
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.31747651400212334
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.6,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.7614996105085896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16835875742536846
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.19284730395996752
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2232968782694361
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.690662437438023
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2618914004394621
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5303300858899107
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.4714045207910317
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.3,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.2555807642842943
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16778804977307898
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.41057813101154367
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.27549614851423926
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.14430750636460155
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 4.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.2636669353576957
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05050762722761047
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6248850624439257
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.41057813101154367
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 2.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.2636669353576957
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.026683274761756533
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.031426968052735337
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.4949747468305833
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0872971334798207
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.20203050891044205
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.4074852637346205
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.8142441722754185
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.29351602237932156
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          }
        }
      }
    },
    "gpt-4.1-nano": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.0,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.9,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6638145292771671
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.027729677693590127
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3894211258708523
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.9,
            "std": 4.101219330881976,
            "coefficient_of_variation": 0.6951219204884705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 7.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.37845151669139165
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.6137153195203997
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 7.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.37845151669139165
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4419417382415922
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.29116161578269595
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.32933740493620023
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.34569664858008986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.10878565864408424
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8999999999999995,
            "std": 3.252691193458118,
            "coefficient_of_variation": 0.5513035921115454
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16778804977307898
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2618914004394621
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.4545686450484949
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.21910350966343722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02481076425215959
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3122289683161379
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.14142135623730942
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.100000000000001,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.29681025383139026
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.8249579113843054
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.19110994086122907
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18856180831641264
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2685215624759041
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 3.6999999999999997,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.1084376569951286
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.7430613632807788
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.8249579113843054
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.6,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4285495643554834
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.19110994086122907
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 2.9000000000000004,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.7314897736412561
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.33941125496954283
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10878565864408422
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.2,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.5499719409228703
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.6,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4351426345763369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.6,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.5050762722761054
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5656854249492381
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.6,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.6148754619013457
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.11313708498984772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.6599663291074443
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.18608073189119673
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.24692617755720705
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.34896178811803646
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 5.4,
            "std": 4.242640687119286,
            "coefficient_of_variation": 0.7856742013183862
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 5.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.8041606531141129
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11223917161691231
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2987775131774144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2664460334905831
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.8485281374238571
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 2.9000000000000004,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3413618943659194
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.29351602237932156
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2232968782694361
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3193385463423118
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.3166149766506929
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.5,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.7228202652129152
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.11313708498984772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14886458551295745
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2327186874791169
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05050762722761047
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.29009508971755793
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 2.3000000000000003,
            "std": 2.9698484809835,
            "coefficient_of_variation": 1.2912384699928259
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5717033550018896
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.0,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5364258340035878
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2589405114204259
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13685937700384795
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4002491214263476
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.100000000000001,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.29681025383139026
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5603487699968867
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.1999999999999997,
            "std": 1.9798989873223327,
            "coefficient_of_variation": 0.618718433538229
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6553672606119222
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.4000000000000004,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 1.178511301977579
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.42426406871192857
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3057759053779665
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 4.3,
            "std": 5.232590180780452,
            "coefficient_of_variation": 1.2168814373908028
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.8999999999999995,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.7910008060730871
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.9,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.8369835369146889
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2142747821777417
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.8249579113843054
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.30304576336566313
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.3309861528958307
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.199999999999999,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1964185503295965
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.800000000000001,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3413618943659194
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.3604858100166714
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.32253993527807423
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        }
      }
    },
    "gpt-4o": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6734350297014738
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02481076425215959
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0872971334798207
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3074377309506728
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5303300858899107
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.2,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.7071067811865477
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5785419118799024
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6527139518645055
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8999999999999995,
            "std": 3.252691193458118,
            "coefficient_of_variation": 0.5513035921115454
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.6164520656498106
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.42426406871192857
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.027729677693590127
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.20865446002225993
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.3299831645537223
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.8,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.2570787221094177
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.39283710065919303
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.5,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5004140297627875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.28284271247461906
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.026683274761756533
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.8,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.7443229275647868
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3588303068707853
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.1,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5863812331790883
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.42426406871192857
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.4,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.33275613232308116
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6428243465332251
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6734350297014738
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.11313708498984772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.4536156709498607
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.2,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.26516504294495524
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.24692617755720705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.42426406871192857
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.5999999999999996,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.29182184620397195
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.6,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4285495643554834
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0800498242852695
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.026683274761756533
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.7071067811865476
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12297509238026912
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.36987123938988636
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.3309861528958307
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3636549160387959
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.14347094111031392
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3074377309506728
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.4,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21998877636914818
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.5,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.591398398810567
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.1000000000000005,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.33861451493440303
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.4949747468305833
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3588303068707853
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3636549160387959
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2987775131774144
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.3999999999999995,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3439978935502123
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5237828008789241
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18446263857040368
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.5999999999999996,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6527139518645055
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2474873734152917
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.1,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.39412509115315764
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.3,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.7614996105085896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 6.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.5474375080153917
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.6999999999999997,
            "std": 2.68700576850888,
            "coefficient_of_variation": 0.7262177752726704
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2664460334905831
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.5210260492953509
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3856946079199351
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.3604858100166714
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.14142135623730942
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20797258270192576
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.2538332035028631
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.0,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.565685424949238
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.1,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.899954085146515
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.6164520656498106
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.13468700594029476
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.555583899503716
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.5499719409228703
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5591076874498283
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.8,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.8838834764831843
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6734350297014738
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2743996464306005
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16778804977307898
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.3,
            "std": 2.6870057685088797,
            "coefficient_of_variation": 0.42650885214426665
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        }
      }
    },
    "gpt-4o-mini": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.0,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.9192388155425117
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.2,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.24145109601491868
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.7195121633126274
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.6869037302955033
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.23932844901698525
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20797258270192576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.8,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.30304576336566313
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.6386770926846236
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6377825869525724
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.1164843913471802
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.5761610809668164
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7954951288348658
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.9,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 1.0999438818457405
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6060915267313264
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.6599663291074443
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.7542472332656507
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6428243465332251
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.7071067811865476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.7,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4002491214263476
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2618914004394621
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.7071067811865476
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.4,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.8356716504931925
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3367175148507369
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4444671196029727
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5342584568965026
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.1000000000000005,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.7933393154775898
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.4000000000000004,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.4159451654038515
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.30304576336566313
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.41057813101154367
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.11313708498984772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16835875742536846
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.33275613232308116
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.20203050891044208
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4444671196029727
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.6,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3689252771408074
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4242640687119285
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.4000000000000004,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7487012977269325
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.5,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.848528137423857
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.7,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.6999999999999997,
            "std": 2.68700576850888,
            "coefficient_of_variation": 0.9951873216699557
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.3,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.7614996105085896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.6428243465332251
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.7071067811865476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10878565864408422
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6060915267313264
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.538748023761179
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10878565864408422
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.5173952057462543
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.20203050891044208
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.6,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.7614996105085896
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.1,
            "std": 2.969848480983499,
            "coefficient_of_variation": 0.4868604067186064
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.4,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.25712973861328997
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1958141855593517
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.4949747468305833
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.7,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5210260492953508
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05050762722761047
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.3,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.2555807642842943
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.9,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.5992430349038538
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.6137153195203997
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.49064552163964514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.7071067811865476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.1999999999999997,
            "std": 1.9798989873223327,
            "coefficient_of_variation": 0.618718433538229
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4444671196029727
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 4.0,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.9899494936611665
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.8702852691526738
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3856946079199351
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.5533879157112113
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.35954582094231224
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.7999999999999998,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.7856742013183862
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.9616652224137047
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2525381361380527
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02481076425215959
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.1966422450849266
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6363961030678927
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.565685424949238
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.49688584623919557
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3367175148507369
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.14430750636460155
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6599663291074443
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.35954582094231224
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.7999999999999998,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.4000000000000004,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.4159451654038515
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.08318903308077037
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.582323231565392
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.7999999999999998,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.1,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5863812331790883
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.4,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.32141217326661253
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.42242742772183356
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.6,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3689252771408074
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.9000000000000004,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.5439282932204212
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.35954582094231224
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.9,
            "std": 4.101219330881976,
            "coefficient_of_variation": 0.6951219204884705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.4419417382415922
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.0,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6222539674441618
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17367534976511698
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.7999999999999998,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.7856742013183862
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.7856742013183862
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.1966422450849266
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 3.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.7407785326716212
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.8485281374238569
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6553672606119222
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.8485281374238571
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.9,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.43292251909380464
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3193385463423118
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.9,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.375199516547964
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6428243465332251
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.49333031245573084
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16778804977307898
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.2,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.6655122646461624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.9,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.43292251909380464
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.1,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.899954085146515
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.5091168824543142
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.3166149766506929
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.6000000000000005,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.22329687826943606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.7243532880447561
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6377825869525724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.49333031245573084
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21998877636914818
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1616244071283537
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.49913419848462165
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.300000000000001,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.29059182788488264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3771236166328253
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1616244071283537
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.1,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.5930573003500076
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2327186874791169
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3477574333704332
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17367534976511698
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.8485281374238571
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.7000000000000002,
            "std": 0.7071067811865477,
            "coefficient_of_variation": 0.4159451654038515
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.6599663291074443
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.7285342594043218
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.8623253429104238
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3193385463423118
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.8485281374238569
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.32253993527807423
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.37216146378239345
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.14347094111031392
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.9000000000000004,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.5439282932204212
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2525381361380527
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.7000000000000002,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.3,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.822217187426218
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.4074852637346205
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.4419417382415922
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.49688584623919557
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.26516504294495524
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          }
        }
      }
    },
    "lightrag-4.1": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.2636669353576957
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.14142135623730942
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.4,
            "std": 4.242640687119286,
            "coefficient_of_variation": 0.6629126073623883
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.7,
            "std": 2.9698484809835004,
            "coefficient_of_variation": 0.4432609673109702
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.2693740118805895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.42178299228671257
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4388938641847536
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.29116161578269595
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.8,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.48765984909417076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.800000000000001,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3413618943659194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.13468700594029476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.19506393963766824
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02481076425215959
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21998877636914818
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0872971334798207
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.0,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.5185449728701348
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.261891400439462
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.31747651400212334
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.33275613232308116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.5,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.4133855028475202
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.5795957222840553
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.3,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.2555807642842943
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05892556509887902
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2618914004394621
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.4,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.25712973861328997
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4455741360901532
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4554247065269289
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2664460334905831
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.21910350966343722
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.6060915267313265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4204418698947039
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15713484026367727
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.539972451087909
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.300000000000001,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6958828640248562
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.20865446002225993
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.5,
            "std": 2.969848480983499,
            "coefficient_of_variation": 0.4568997663051537
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.555583899503716
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.20203050891044225
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3588303068707853
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.3999999999999995,
            "std": 3.9597979746446654,
            "coefficient_of_variation": 0.618718433538229
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3477574333704332
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5303300858899107
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.42242742772183356
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.6,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.642824346533225
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2589405114204259
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.10878565864408424
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.08318903308077037
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.4,
            "std": 5.091168824543142,
            "coefficient_of_variation": 0.7954951288348658
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.2,
            "std": 3.676955262170048,
            "coefficient_of_variation": 0.510688230856951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.9,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.7215375318230076
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.18608073189119673
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.30138977558770885
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.6386770926846236
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.699999999999999,
            "std": 3.535533905932737,
            "coefficient_of_variation": 0.5276916277511549
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.4,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.32141217326661253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.41594516540385146
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2474873734152917
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.30304576336566313
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.0,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.6599663291074443
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.4159451654038515
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.21910350966343722
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21998877636914818
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.5439282932204212
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 5.199999999999999,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.8158924398306318
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 3.5999999999999996,
            "std": 4.242640687119285,
            "coefficient_of_variation": 1.1785113019775793
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.39283710065919303
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.48613591206575135
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02571297386132891
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 6.800000000000001,
            "std": 4.242640687119286,
            "coefficient_of_variation": 0.6239177481057773
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.5,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6309568201356885
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.1966422450849266
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.42178299228671257
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.5474375080153917
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.539972451087909
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.4159451654038515
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3122289683161379
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.100000000000001,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.29681025383139026
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.16444343748524362
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.21910350966343722
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.35954582094231224
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2987775131774144
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.538748023761179
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.300000000000001,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.29059182788488264
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6353713106313905
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0800498242852695
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4854762975310624
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.7,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.6965529486315244
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.6,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 0.7285342594043218
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.7000000000000002,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3721614637823934
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05892556509887902
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          }
        }
      }
    },
    "lightrag-4.1-nano": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.5795957222840553
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 2.9000000000000004,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3413618943659194
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5237828008789241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5018177156807757
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14886458551295745
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.21910350966343722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 3.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.979070927796758
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02571297386132891
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3636549160387959
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1616244071283537
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.4536156709498607
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.16444343748524362
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14886458551295745
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.23570226039551587
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.2074179891480539
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3721614637823934
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10878565864408422
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.4,
            "std": 4.525483399593904,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.29182184620397195
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.51425947722658
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 5.699999999999999,
            "std": 3.535533905932737,
            "coefficient_of_variation": 0.620269106303989
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.6,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3689252771408074
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.8,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.48765984909417076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.5999999999999996,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.47140452079103173
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 2.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4388938641847536
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15713484026367727
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2743996464306005
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.1,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.39412509115315764
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12297509238026912
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3205550741379015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.3,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.2555807642842943
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.3299831645537223
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.8999999999999995,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.19284730395996752
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.5
            },
            "mean": 8.25,
            "std": 1.7677669529663689,
            "coefficient_of_variation": 0.21427478217774168
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.19110994086122907
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.6,
            "std": 3.6769552621700465,
            "coefficient_of_variation": 0.5571144336621283
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2209708691207961
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.647182477696162
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.4000000000000001,
            "std": 1.1313708498984762,
            "coefficient_of_variation": 0.8081220356417687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.26516504294495524
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02571297386132891
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6121222881913397
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.6000000000000005,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.5,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.4133855028475202
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.1,
            "std": 2.969848480983499,
            "coefficient_of_variation": 0.4868604067186064
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3057759053779665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.4074852637346205
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3074377309506728
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.9,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.375199516547964
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11223917161691231
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.2636669353576957
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 7.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.37845151669139165
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.027729677693590127
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.3604858100166714
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20797258270192576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.29009508971755793
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.0,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.6,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4285495643554834
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.5,
            "std": 2.969848480983499,
            "coefficient_of_variation": 0.4568997663051537
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.3299831645537223
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.4074852637346205
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.42426406871192857
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0800498242852695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.4,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11223917161691231
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.031426968052735337
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        }
      }
    },
    "llasmol-top1": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3193385463423118
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.8879945624203155
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.8000000000000003,
            "std": 2.2627416997969525,
            "coefficient_of_variation": 0.8081220356417687
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.34569664858008986
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.5091168824543142
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 2.6999999999999997,
            "std": 2.68700576850888,
            "coefficient_of_variation": 0.9951873216699557
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.1966422450849266
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.6,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 0.7285342594043218
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.5733298225836871
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.6869037302955033
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7999999999999999,
            "std": 0.8485281374238569,
            "coefficient_of_variation": 1.0606601717798212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.6763630080914803
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.4419417382415922
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.2,
            "std": 3.3941125496954276,
            "coefficient_of_variation": 0.8081220356417684
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5603487699968867
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.0999999999999996,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.6842968850192396
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.565685424949238
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.5761610809668164
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.6809176411426013
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.25
            },
            "mean": 0.125,
            "std": 0.1767766952966369,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.5163001894377965
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.9000000000000004,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.9065471553673685
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.4,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.32141217326661253
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.8,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.5050762722761054
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.6000000000000005,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.8608256466618839
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6377825869525724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.7542472332656507
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.4000000000000001,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.2121830534626528
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2232968782694361
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.9265537132789243
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.6932419423397524
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.9790709277967582
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.0,
            "std": 5.656854249492381,
            "coefficient_of_variation": 0.9428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.6,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4351426345763369
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.5,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.7228202652129152
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.0,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6222539674441618
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7954951288348658
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.5091168824543142
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.7999999999999998,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 5.8,
            "std": 3.3941125496954276,
            "coefficient_of_variation": 0.5851918189130048
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16778804977307898
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.6809176411426013
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      }
    },
    "llasmol-top5": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5018177156807757
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.41594516540385146
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.6763630080914803
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.1,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.899954085146515
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5364258340035878
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.6599663291074443
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.4,
            "std": 5.091168824543142,
            "coefficient_of_variation": 0.7954951288348658
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 5.1,
            "std": 6.929646455628165,
            "coefficient_of_variation": 1.3587542069859149
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.8485281374238571
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6763630080914803
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.9,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.5210260492953509
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 5.0,
            "std": 7.0710678118654755,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.8,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.5050762722761054
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.3999999999999995,
            "std": 3.9597979746446654,
            "coefficient_of_variation": 0.7332959212304937
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.8791057279616536
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 3.4,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 1.1646464631307842
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 5.3,
            "std": 6.646803743153547,
            "coefficient_of_variation": 1.254113913802556
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.6285393610547089
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.20203050891044205
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.6428243465332251
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 3.8000000000000003,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5954583420518295
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 4.2,
            "std": 4.808326112068523,
            "coefficient_of_variation": 1.1448395504925053
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3477574333704332
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.24692617755720705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.3309861528958307
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.0,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.28284271247461895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 0.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 0.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6353713106313905
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.4,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.7071067811865477
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.8838834764831844
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.3626188621469475
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.0,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.7000000000000002,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.7487012977269325
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 4.0,
            "std": 5.091168824543142,
            "coefficient_of_variation": 1.2727922061357855
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4242640687119285
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 3.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.322973977703863
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 2.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.8290217434600902
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 2.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.3856946079199349
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2618914004394621
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.30304576336566313
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 4.3,
            "std": 6.081118318204308,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 1.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 3.1,
            "std": 4.384062043356595,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18856180831641264
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 2.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.9223131928520185
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.7407785326716212
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.28284271247461895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 2.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.9790709277967582
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 4.0,
            "std": 5.656854249492381,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3636549160387959
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.41057813101154367
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.7000000000000002,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.7487012977269325
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 5.7,
            "std": 6.081118318204309,
            "coefficient_of_variation": 1.0668628628428611
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2525381361380527
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          }
        }
      }
    },
    "MOSES": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.42242742772183356
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.4581255202053689
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.42178299228671257
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.2605130246476754
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.8485281374238569
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.8,
            "std": 3.394112549695427,
            "coefficient_of_variation": 0.49913419848462165
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2743996464306005
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.30138977558770885
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.4321208107251124
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12297509238026912
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.6,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3205550741379015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2327186874791169
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.6000000000000005,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.40937761016063273
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14886458551295745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3122289683161379
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.9,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.5123962182511214
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.14142135623730942
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5303300858899107
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.32253993527807423
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18446263857040368
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.2538332035028631
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.6000000000000005,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16835875742536846
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.5,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.43369215912774917
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.4848732213850612
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.2605130246476754
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.6,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3205550741379015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3232488142567074
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 7.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.45866385806694976
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.3,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.8805480671379649
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2685215624759041
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.34896178811803646
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.5,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.35826743580118414
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2518462508335648
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.34896178811803646
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.75
            },
            "mean": 9.375,
            "std": 0.5303300858899106,
            "coefficient_of_variation": 0.056568542494923796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.75
            },
            "mean": 6.775,
            "std": 1.3788582233137678,
            "coefficient_of_variation": 0.2035215089762019
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.4581255202053689
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2987775131774144
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4242640687119285
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.27549614851423926
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.1000000000000005,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.33861451493440303
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.27498597046143514
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.5,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5004140297627875
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2518462508335648
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 6.5,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.5874425566780548
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          }
        }
      }
    },
    "MOSES-nano": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4351426345763369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3636549160387959
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.3299831645537223
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 3.5,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 1.0909647481163876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2518462508335648
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3721614637823934
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 4.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 1.0606601717798212
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.6788225099390857
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.5733298225836871
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.7195121633126274
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.5,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.9113820735293279
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.7000000000000002,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.7487012977269325
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.7,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.0,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6222539674441618
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 3.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.8667760543577034
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.23141676475196113
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.7071067811865476
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.026683274761756533
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.2074179891480539
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.1697056274847713
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6377825869525724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5364258340035878
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.14430750636460155
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.14629795472825122
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.4536156709498607
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.300000000000001,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.29059182788488264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2685215624759041
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.36987123938988636
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.33941125496954283
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.3999999999999995,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3439978935502123
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.26516504294495524
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.19110994086122907
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.28284271247461906
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.9,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.375199516547964
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 5.0,
            "std": 6.505382386916237,
            "coefficient_of_variation": 1.3010764773832473
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 4.6,
            "std": 6.505382386916237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17367534976511698
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.3166149766506929
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.0,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.6128258770283411
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.5,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.9742360096347988
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11223917161691231
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.1,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.49796252196235746
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.2636669353576957
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5018177156807757
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 5.300000000000001,
            "std": 4.949747468305834,
            "coefficient_of_variation": 0.9339146166614779
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.6698906348083082
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.77138921583987
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.2,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.24145109601491868
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.8667760543577034
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15044825131628672
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.800000000000001,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3413618943659194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.32253993527807423
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.30138977558770885
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.6,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3999999999999995,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.15288795268898323
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.49333031245573084
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.7204484185674258
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.4000000000000001,
            "std": 1.1313708498984762,
            "coefficient_of_variation": 0.8081220356417687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3349453174041541
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.261891400439462
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16835875742536846
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.555583899503716
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.7,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6318826555284042
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.8702852691526738
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.582323231565392
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.5163001894377965
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2685215624759041
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.9,
            "std": 4.101219330881976,
            "coefficient_of_variation": 0.6951219204884705
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6599663291074443
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.6137153195203997
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.7071067811865476
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.5533879157112113
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6363961030678927
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.4000000000000001,
            "std": 1.1313708498984762,
            "coefficient_of_variation": 0.8081220356417687
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.5,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.6222539674441618
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13199326582148882
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.18608073189119673
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.39283710065919303
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.19110994086122907
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.27371875400769585
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.33275613232308116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.2538332035028631
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.30304576336566313
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3636549160387959
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4388938641847536
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 5.2,
            "std": 6.222539674441618,
            "coefficient_of_variation": 1.1966422450849266
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 4.4,
            "std": 6.222539674441618,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.300000000000001,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.29059182788488264
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2232968782694361
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.6428243465332251
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6527139518645055
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.37216146378239345
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.100000000000001,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.29681025383139026
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4388938641847536
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5603487699968867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.3,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6670818690439128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.300000000000001,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.9000000000000004,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.8340233829379791
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.4000000000000004,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 1.178511301977579
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.5,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.848528137423857
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 3.3,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 1.0713739108887084
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.1253100624887552
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5033641493192372
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.526863876178212
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.20203050891044205
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18996898599041576
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.43712055564259306
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.5,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6744710835933223
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.6,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.2977291710259147
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.48613591206575135
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 7.2,
            "std": 3.676955262170048,
            "coefficient_of_variation": 0.510688230856951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3477574333704332
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3721614637823934
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.4204418698947039
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 3.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6889758380792002
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.2538332035028631
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3232488142567074
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.4545686450484949
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05892556509887902
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.3309861528958307
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.031426968052735337
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3232488142567074
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.29182184620397195
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6060915267313264
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 5.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.7738149680909389
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.044194173824159154
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.2074179891480539
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.300000000000001,
            "std": 2.1213203435596433,
            "coefficient_of_variation": 0.29059182788488264
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.29182184620397195
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3367175148507369
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.10878565864408424
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.9000000000000004,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3413618943659194
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2209708691207961
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3666479606152468
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          }
        }
      }
    },
    "o1": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.7564398124321206
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 4.8,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.7071067811865477
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5717033550018896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12297509238026912
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.2142747821777417
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.4,
            "std": 1.4142135623730954,
            "coefficient_of_variation": 0.32141217326661253
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.5210260492953509
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.4536156709498607
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.9,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.5123962182511214
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 5.2,
            "std": 6.505382386916238,
            "coefficient_of_variation": 1.2510350744069687
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.30502645462949113
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15713484026367727
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 5.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.7614996105085896
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.642824346533225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2946278254943948
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.582323231565392
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.5,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6309568201356885
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5656854249492381
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2232968782694361
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.33941125496954283
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.20203050891044205
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.9000000000000004,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6248850624439257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.8,
            "std": 3.394112549695427,
            "coefficient_of_variation": 0.49913419848462165
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 4.5,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.9742360096347988
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3349453174041541
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.08081220356417694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.0,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6222539674441618
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15713484026367727
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.7564398124321206
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 3.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6889758380792002
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 4.9,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.43292251909380464
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.34842942841076247
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.199999999999999,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1964185503295965
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6723310378495042
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.9000000000000004,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.4714045207910317
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.6,
            "std": 3.6769552621700465,
            "coefficient_of_variation": 0.5571144336621283
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18446263857040368
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 4.7,
            "std": 5.232590180780452,
            "coefficient_of_variation": 1.1133170597405218
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15713484026367727
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.4561979233461596
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.27371875400769585
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.9,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.375199516547964
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.24692617755720705
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.8249579113843054
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.5999999999999996,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.39283710065919303
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.1,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.2693740118805895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.6999999999999997,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3439978935502123
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.1830158727776946
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11223917161691231
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.12856486930664496
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4351426345763369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.19410774385513063
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03721614637823929
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.553387915711211
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2664460334905831
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 4.7,
            "std": 5.232590180780452,
            "coefficient_of_variation": 1.1133170597405218
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16444343748524362
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.1,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.39412509115315764
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07856742013183861
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.5,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5004140297627875
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.565685424949238
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.800000000000001,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.540728715025007
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 5.7,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.7691336918169465
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20797258270192576
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.41247895569215276
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.4000000000000001,
            "std": 1.1313708498984762,
            "coefficient_of_variation": 0.8081220356417687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 5.699999999999999,
            "std": 3.535533905932737,
            "coefficient_of_variation": 0.620269106303989
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.8999999999999995,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.51425947722658
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 3.7,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.9555497043061453
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5717033550018896
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5018177156807757
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.4561979233461596
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.1,
            "std": 2.969848480983499,
            "coefficient_of_variation": 0.4868604067186064
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.3999999999999995,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.397747564417433
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.3299831645537223
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          }
        }
      }
    },
    "o3": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 2.6999999999999997,
            "std": 2.68700576850888,
            "coefficient_of_variation": 0.9951873216699557
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 5.6000000000000005,
            "std": 5.374011537017761,
            "coefficient_of_variation": 0.9596449173246001
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.9,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.375199516547964
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.8041606531141129
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.800000000000001,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3413618943659194
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 2.5,
            "std": 1.8384776310850235,
            "coefficient_of_variation": 0.7353910524340094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.14142135623730942
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.2538332035028631
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6121222881913397
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.553387915711211
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.2,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.5499719409228703
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 4.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.9537719374144129
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5237828008789241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.261891400439462
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.5776365254763346
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.45753968194423666
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 5.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.7430613632807788
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 4.4,
            "std": 5.091168824543142,
            "coefficient_of_variation": 1.157083823759805
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 3.4,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023969721396154175
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.3,
            "std": 2.6870057685088797,
            "coefficient_of_variation": 0.36808298198751777
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.3626188621469475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.7999999999999998,
            "std": 1.414213562373095,
            "coefficient_of_variation": 0.7856742013183862
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.261891400439462
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.1,
            "std": 4.666904755831214,
            "coefficient_of_variation": 0.7650663534149532
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.699999999999999,
            "std": 3.535533905932737,
            "coefficient_of_variation": 0.5276916277511549
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6121222881913397
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.647182477696162
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.0,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.6128258770283411
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.49688584623919557
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4854762975310624
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.5,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.7971021897011991
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 4.800000000000001,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.7660323462854264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 2.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2495670992423109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.2,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 3.7,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.802661751617162
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6121222881913397
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.8,
            "std": 1.9798989873223323,
            "coefficient_of_variation": 0.2538332035028631
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.24513035081133652
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.014886458551295684
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.18446263857040368
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.0,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.6128258770283411
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 5.5,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.6942502942558829
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 2.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.9265537132789243
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0800498242852695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.2605130246476754
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.300000000000001,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6958828640248562
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.8,
            "std": 4.525483399593904,
            "coefficient_of_variation": 0.6655122646461624
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 5.1,
            "std": 5.5154328932550705,
            "coefficient_of_variation": 1.081457430050014
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07443229275647864
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.7,
            "std": 2.9698484809835004,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11785113019775795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.5474375080153917
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.6869037302955033
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4242640687119285
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 5.7,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.7691336918169465
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 4.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.7378505542816148
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1477536557703234
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.42242742772183356
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 4.8,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5892556509887896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.29351602237932156
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 6.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5699069579712474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.1,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.533228064501331
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.0,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.2,
            "std": 4.808326112068523,
            "coefficient_of_variation": 0.9246780984747158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.5795957222840553
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.1,
            "std": 4.384062043356594,
            "coefficient_of_variation": 0.7186986956322285
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 6.800000000000001,
            "std": 4.242640687119286,
            "coefficient_of_variation": 0.6239177481057773
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04040610178208847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.31426968052735454
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 4.2,
            "std": 4.242640687119286,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.699999999999999,
            "std": 3.535533905932737,
            "coefficient_of_variation": 0.5276916277511549
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 5.8,
            "std": 4.525483399593904,
            "coefficient_of_variation": 0.7802557585506732
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.199999999999999,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6842968850192396
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.2605130246476754
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.49688584623919557
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.6285393610547089
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.023183828891362238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.2605130246476754
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 5.8999999999999995,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.7910008060730871
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.07071067811865482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 5.0,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 0.9616652224137047
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        }
      }
    },
    "spark-chem13b-nothink": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.15713484026367722
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.6285393610547089
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.8187552203212656
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7999999999999999,
            "std": 0.8485281374238569,
            "coefficient_of_variation": 1.0606601717798212
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5033641493192372
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.5533879157112113
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.48613591206575135
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.7071067811865476
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2020305089104421
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.9000000000000004,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.4714045207910317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.2538332035028631
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.7000000000000002,
            "std": 0.7071067811865477,
            "coefficient_of_variation": 0.4159451654038515
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.2685215624759041
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.5,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3205550741379015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06577737499409751
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.9150793638884732
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.6788225099390857
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.8187552203212656
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.4000000000000004,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7487012977269325
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 4.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.8124205571079482
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3856946079199351
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6428243465332251
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.1,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.5930573003500076
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6481812160876687
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.4419417382415922
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 6.0,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.5185449728701348
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.49913419848462165
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.0,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.7778174593052023
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06148754619013462
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 2.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 1.0240856830977585
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 2.0,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.9899494936611665
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.6599663291074443
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.6,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.7071067811865475
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 4.8,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.7071067811865477
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 2.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.8702852691526738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.21757131728816842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.027729677693590127
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.6599663291074443
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6698906348083081
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.8187552203212656
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.157083823759805
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.4714045207910317
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.42242742772183356
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.6827237887318389
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 3.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.9856639980176117
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.6000000000000005,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.22329687826943606
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.07190916418846252
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2999999999999998,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.10878565864408422
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 6.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.49913419848462165
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5656854249492381
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10878565864408424
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 7.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4040610178208843
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 6.1,
            "std": 4.384062043356594,
            "coefficient_of_variation": 0.7186986956322285
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.0,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14142135623730956
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 7.1000000000000005,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.537799523719346
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6060915267313264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5069822204733737
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4388938641847536
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.538748023761179
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 6.0,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.6128258770283411
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.6428243465332251
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 5.0,
            "std": 4.525483399593903,
            "coefficient_of_variation": 0.9050966799187806
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.2074179891480539
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5656854249492381
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 4.1,
            "std": 4.384062043356595,
            "coefficient_of_variation": 1.0692834252089256
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.6000000000000005,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.4714045207910317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3122289683161379
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.0,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.14142135623730956
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5717033550018896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.5999999999999996,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.7856742013183862
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.2,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.899954085146515
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.4561979233461596
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.16835875742536846
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 3.8,
            "std": 4.525483399593904,
            "coefficient_of_variation": 1.1909166841036591
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.9000000000000001,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.9676198058342229
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 1.2047004420215253
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.20203050891044225
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.5999999999999996,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.39283710065919303
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.3,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.822217187426218
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03367175148507373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 4.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6763630080914803
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.31426968052735454
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.8,
            "std": 2.82842712474619,
            "coefficient_of_variation": 1.0101525445522108
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.49913419848462165
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.3856946079199349
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02571297386132891
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3856946079199349
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.7407785326716212
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4040610178208843
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.6285393610547089
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.6000000000000005,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.22329687826943606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03288868749704875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.07071067811865482
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.7,
            "std": 2.9698484809835004,
            "coefficient_of_variation": 0.3856946079199351
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.3317291072233186
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.7,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.6965529486315244
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.018366409900949305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.03822198817224576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.33941125496954283
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.699999999999999,
            "std": 2.121320343559642,
            "coefficient_of_variation": 0.3166149766506929
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3666479606152468
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4854762975310624
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5364258340035878
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.23218431621050817
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.4,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.49333031245573084
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.7,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.4854762975310624
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 7.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2424366106925305
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.6698906348083082
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3477574333704332
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.38074980525429486
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.5,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.6942502942558829
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.2,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.8754655386119159
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5303300858899107
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 2.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4388938641847536
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08318903308077027
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.1,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.47140452079103173
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6363961030678927
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.6,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.7614996105085896
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.20203050891044216
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01703871761895304
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.20865446002225993
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.5,
            "std": 1.8384776310850235,
            "coefficient_of_variation": 1.2256517540566823
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.2,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.7071067811865477
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08570991287109665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08127664151569514
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 4.3,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.6248850624439257
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.07252377242938939
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 3.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.5210260492953509
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 2.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.9091372900969897
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.131370849898476
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6527139518645055
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.8999999999999995,
            "std": 3.252691193458118,
            "coefficient_of_variation": 0.5513035921115454
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5785419118799024
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.49688584623919557
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.6,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2142747821777417
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10553832555023097
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.4545686450484949
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.2357022603955158
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4513447539488601
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.18742589380848246
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13864838846795052
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015890040026663933
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.9000000000000004,
            "std": 0.7071067811865478,
            "coefficient_of_variation": 0.18130943107347378
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.052378280087892456
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.33426866019727697
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.8,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 1.0999438818457405
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 2.6999999999999997,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 1.0999438818457408
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.049913419848462294
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.4159451654038515
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.43712055564259306
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2474873734152917
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.800000000000001,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3413618943659194
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.2357022603955159
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.9,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3074377309506728
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.26516504294495524
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.699999999999999,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.7195121633126274
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 7.1000000000000005,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.33861451493440303
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 4.0,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.9899494936611665
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3617755624675359
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.14430750636460155
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.3,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6670818690439128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.6,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3428396514843866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.6999999999999997,
            "std": 2.68700576850888,
            "coefficient_of_variation": 0.7262177752726704
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 3.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.979070927796758
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 2.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.8999999999999995,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.9524295420063702
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 4.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.0002973977760916
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 3.4,
            "std": 3.676955262170047,
            "coefficient_of_variation": 1.081457430050014
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 1.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.22697254704753383
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1497402595453866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.19506393963766824
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 6.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5018177156807757
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5091168824543142
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6599663291074443
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.7999999999999998,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.37216146378239345
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 3.6999999999999997,
            "std": 4.384062043356595,
            "coefficient_of_variation": 1.1848816333396204
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.9,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 1.0999438818457405
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.157083823759805
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.1,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.24145109601491868
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17367534976511698
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07945020013331995
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.31747651400212334
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 3.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.5999999999999999,
            "std": 1.6970562748477138,
            "coefficient_of_variation": 1.0606601717798212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 2.1999999999999997,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.02851895445316
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.23876332871234068
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.34896178811803646
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.3104371234477526
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.300000000000001,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.24014947285580854
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.8,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09642365197998383
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.4,
            "std": 1.414213562373095,
            "coefficient_of_variation": 1.0101525445522108
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.5018177156807757
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.8,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.2570787221094177
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.9000000000000004,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.6339578038224218
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.6363961030678927
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0872971334798207
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.4,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.7071067811865477
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3901278792753365
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.23932844901698525
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3193385463423118
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.430412823330942
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5999999999999996,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10878565864408422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.1,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5863812331790883
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.199999999999999,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.3649583386769278
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.5091168824543142
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.6164520656498106
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4351426345763369
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.14430750636460155
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09428090415820635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.6920619560549188
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.5,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.4714045207910317
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.17880861133452935
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.8485281374238571
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.048765984909417116
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.6,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 0.7285342594043218
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.5999999999999996,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.39283710065919303
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.07603298722435994
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.23570226039551587
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.11466596451673744
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.0,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03535533905932733
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 5.1000000000000005,
            "std": 5.232590180780452,
            "coefficient_of_variation": 1.0259980746628337
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.9,
            "std": 4.101219330881976,
            "coefficient_of_variation": 0.6951219204884705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.8124205571079482
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.09428090415820643
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.41902624070313926
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.14629795472825122
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.07443229275647867
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5342584568965026
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5656854249492381
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.8187552203212656
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 2.7,
            "std": 3.818376618407357,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21998877636914818
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.014579521261578377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.16111293748554253
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.131370849898476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.7999999999999998,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.2,
            "std": 0.5656854249492372,
            "coefficient_of_variation": 0.06148754619013449
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06332299533013855
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.04159451654038519
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4242640687119285
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.44840917831342053
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4002491214263476
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.21757131728816842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.2,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.33275613232308116
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.1,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.9313113703432578
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3367175148507369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.1,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.3604858100166714
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2293319290334748
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2175713172881685
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.12121830534626522
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 5.4,
            "std": 4.242640687119286,
            "coefficient_of_variation": 0.7856742013183862
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.6809176411426013
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.16444343748524362
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.09686394262829419
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.4465937565388721
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05439282932204217
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.17435509673092944
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.8791057279616536
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.1,
            "std": 3.5355339059327373,
            "coefficient_of_variation": 0.6932419423397524
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.5999999999999996,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.7856742013183862
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.33275613232308116
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.12405382126079782
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030743773095067317
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        }
      }
    },
    "spark-chem13b-think": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 6.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.02175713172881677
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.7542472332656507
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.16228680223953545
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.7071067811865475
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17999081702930306
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5033641493192372
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.6428243465332251
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6481812160876687
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.1000000000000005,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.6259633800667798
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.5999999999999996,
            "std": 1.4142135623730947,
            "coefficient_of_variation": 0.39283710065919303
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 6.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3894211258708523
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.19284730395996752
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5717033550018896
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.015540808377726265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11591914445681109
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.22627416997969516
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.6,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4351426345763369
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2708068523693161
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.1697056274847713
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5656854249492381
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.7993381004717495
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.7407785326716212
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.14886458551295745
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.17367534976511698
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06734350297014745
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.8485281374238571
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.6,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.2977291710259147
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.13155474998819494
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.01937278852565885
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.199999999999999,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.16317848796612638
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 6.1,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4404927489358821
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 4.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5303300858899106
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 2.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.7993381004717495
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 3.7,
            "std": 4.666904755831213,
            "coefficient_of_variation": 1.2613256096841117
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 3.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.322973977703863
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4002491214263476
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 6.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.20865446002225993
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.22150332904638834
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.045619792334616015
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.3367175148507369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3142696805273543
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.8,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.8931875130777444
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.5,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4444671196029727
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.1,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.1,
            "std": 0.4242640687119284,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.3000000000000003,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.4714045207910317
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.0,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.3181980515339464
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.0,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.2121320343559642
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0995925043924715
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.17677669529663687
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.282842712474619
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.4,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4513447539488601
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.75
            },
            "mean": 6.675,
            "std": 1.5202795795510775,
            "coefficient_of_variation": 0.2277572403821839
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.16637806616154058
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06148754619013449
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6060915267313264
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.28284271247461895
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.20203050891044208
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.5474375080153917
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.7,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.1966422450849266
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.5,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.6942502942558829
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 5.7,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.8187552203212655
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.4419417382415922
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.6060915267313264
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 5.3,
            "std": 3.2526911934581184,
            "coefficient_of_variation": 0.6137153195203997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12856486930664504
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1616244071283537
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.800000000000001,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.18130943107347378
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2327186874791169
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 6.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.23932844901698525
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.4554247065269289
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.3367175148507369
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 4.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.10347904114925097
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.4040610178208843
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02481076425215959
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.7,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.19110994086122907
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.9,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.311606378150004
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 1.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.0,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.4848732213850612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.1,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.13942950614946004
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 3.4000000000000004,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.33275613232308116
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.899999999999999,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04767012007999195
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.5823232315653921
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.0,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.28284271247461895
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3617755624675359
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.4,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.5761610809668164
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.07856742013183861
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4275529374616333
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.4000000000000004,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7487012977269325
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.4,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2618914004394621
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.3,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.9223131928520185
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.048765984909417116
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.128564869306645
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20203050891044216
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17677669529663695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.2,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5439282932204212
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.2,
            "std": 2.262741699796953,
            "coefficient_of_variation": 0.27594410973133576
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3104371234477526
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.5,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.488546503365251
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.22545433603049342
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.28284271247461906
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5591076874498283
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.1,
            "std": 0.7071067811865472,
            "coefficient_of_variation": 0.1724650685820847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.05050762722761047
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.030089650263257377
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.14504754485877897
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 6.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.6509871953780914
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 3.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.7285342594043218
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 3.4000000000000004,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.8318903308077029
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.43712055564259306
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.045619792334616015
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.5,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1379720548656678
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.14886458551295745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.699999999999999,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.021107665110046216
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.43712055564259306
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.1,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.39412509115315764
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 4.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.35355339059327373
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.5115240544753749
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.5237828008789241
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.9000000000000001,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.6285393610547089
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.09026895078977197
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.10347904114925097
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.2,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.17677669529663695
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.49688584623919557
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.2693740118805895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.2,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4351426345763369
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.4,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.25712973861328997
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2999999999999998,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.10878565864408422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.8702852691526738
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.4000000000000001,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.2121830534626528
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.49913419848462165
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.0,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.565685424949238
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.9000000000000004,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.14629795472825122
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3093592167691145
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.13258252147247768
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.06734350297014745
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2357022603955158
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 8.2,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1724650685820847
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.0,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4525483399593904
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.7,
            "std": 1.5556349186104053,
            "coefficient_of_variation": 0.20203050891044225
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.8999999999999995,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.053704312495180796
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 4.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3856946079199349
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1356095196796119
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.8,
            "std": 0.5656854249492382,
            "coefficient_of_variation": 0.20203050891044225
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.199999999999999,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09123958466923193
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.6,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4351426345763369
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.3856946079199351
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 4.9,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.086584503818761
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2999999999999998,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.10878565864408422
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.7,
            "std": 2.9698484809835004,
            "coefficient_of_variation": 0.3856946079199351
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 6.199999999999999,
            "std": 2.8284271247461894,
            "coefficient_of_variation": 0.4561979233461596
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.8999999999999995,
            "std": 4.666904755831213,
            "coefficient_of_variation": 0.7910008060730871
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.399999999999999,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.13468700594029476
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 5.1000000000000005,
            "std": 4.666904755831214,
            "coefficient_of_variation": 0.9150793638884733
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.199999999999999,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.3535533905932738
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.6827237887318389
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10101525445522108
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.5,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.11646464631307837
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.2977291710259147
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.199999999999999,
            "std": 1.4142135623730945,
            "coefficient_of_variation": 0.1964185503295965
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 4.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.7071067811865476
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.3,
            "std": 3.8183766184073566,
            "coefficient_of_variation": 0.7204484185674258
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.3,
            "std": 0.7071067811865481,
            "coefficient_of_variation": 0.08519358809476482
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.4,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.8356716504931925
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 6.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.34842942841076247
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.9,
            "std": 0.9899494936611661,
            "coefficient_of_variation": 0.20203050891044205
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 3.3,
            "std": 4.666904755831213,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 7.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.10878565864408424
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.17926650790644866
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 3.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.128564869306645
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 2.1999999999999997,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.02851895445316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.28284271247461895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4204418698947039
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.27371875400769585
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.5999999999999996,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.5785419118799024
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05656854249492386
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 9.0,
            "std": 0.282842712474618,
            "coefficient_of_variation": 0.031426968052735337
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.5,
            "std": 0.4242640687119295,
            "coefficient_of_variation": 0.044659375653887314
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.7,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.05237828008789233
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.10606601717798207
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.1,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.5776365254763346
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.44997704257325744
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.4085505846855607
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3043244374726913
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 9.75
            },
            "mean": 6.075,
            "std": 5.197234841721125,
            "coefficient_of_variation": 0.8555119081022428
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.1,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.19205369365560546
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.4,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.26755391720572075
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.699999999999999,
            "std": 0.7071067811865469,
            "coefficient_of_variation": 0.09183204950474635
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.21062755184280144
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 8.7,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.01625532830313911
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.05892556509887902
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.8,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.48765984909417076
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.29998469504883835
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03626188621469478
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.06955148667408671
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.24382992454708538
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.9,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.5483685241854858
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.8,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4388938641847536
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.5142594772265799
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.5,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.6869037302955033
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 7.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.1969158124823297
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 5.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0800498242852695
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.019918500878494318
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 8.1,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.017459426695964075
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 5.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.08318903308077037
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.3,
            "std": 5.232590180780452,
            "coefficient_of_variation": 0.8305698699651511
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.20203050891044208
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.03822198817224576
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 4.7,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.39116545342234543
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.09428090415820643
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 2.5999999999999996,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6527139518645055
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.9,
            "std": 2.4041630560342613,
            "coefficient_of_variation": 0.3043244374726913
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.2000000000000002,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.178511301977579
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.6,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.04285495643554828
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.4465937565388721
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.6698906348083081
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.4000000000000001,
            "std": 1.1313708498984762,
            "coefficient_of_variation": 0.8081220356417687
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10644618211410395
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1616244071283537
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 9.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04561979233461594
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.6,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.05892556509887902
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.600000000000001,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.029462782549439504
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.8,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.399999999999999,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.030089650263257377
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09866606249114612
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.2,
            "std": 0.8485281374238578,
            "coefficient_of_variation": 0.09223131928520194
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.26755391720572075
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.300000000000001,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.02244783432338248
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 2.9,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 4.8,
            "std": 4.525483399593904,
            "coefficient_of_variation": 0.9428090415820634
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 2.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.4,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5745242597140698
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.6,
            "std": 4.8083261120685235,
            "coefficient_of_variation": 0.7285342594043218
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 2.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.9790709277967582
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.282842712474619
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.21572749256538737
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.039283710065919346
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.18678292333229563
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 4.3,
            "std": 1.2727922061357857,
            "coefficient_of_variation": 0.29599818747343853
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2743996464306005
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04876598490941705
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.9000000000000004,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3413618943659194
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4919003695210766
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 4.5,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.9742360096347988
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 2.8000000000000003,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 1.313198307917874
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 4.5,
            "std": 4.949747468305833,
            "coefficient_of_variation": 1.0999438818457405
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.32635697593225266
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.5761610809668164
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 3.7,
            "std": 4.666904755831213,
            "coefficient_of_variation": 1.2613256096841117
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 5.0,
            "std": 4.242640687119285,
            "coefficient_of_variation": 0.8485281374238569
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 4.9,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.8369835369146889
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.1,
            "std": 0.4242640687119289,
            "coefficient_of_variation": 0.059755502635482946
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.09753196981883411
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.3,
            "std": 0.14142135623731025,
            "coefficient_of_variation": 0.015206597444872069
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 6.2,
            "std": 3.9597979746446663,
            "coefficient_of_variation": 0.6386770926846236
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.051116152856858825
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.3,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.13341637380878257
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.9,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.14301036023997596
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.8,
            "std": 1.4142135623730956,
            "coefficient_of_variation": 0.1607060866333063
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.06148754619013449
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.5,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.15229992210171797
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.7,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.4513447539488601
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.7999999999999998,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.47140452079103173
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.5571144336621283
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 7.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.23570226039551587
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.6,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.23022081247934104
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.7,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.1137872981219732
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.23022081247934104
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 5.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.33941125496954283
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 4.7,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.7522412565814335
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 5.4,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6285393610547089
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.157083823759805
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.22809896167307986
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.1,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 1.157083823759805
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 1.5,
            "std": 1.8384776310850235,
            "coefficient_of_variation": 1.2256517540566823
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.05656854249492386
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 7.6,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.37216146378239345
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.800000000000001,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.11785113019775788
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.6,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.25712973861328997
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 7.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.08950718749196804
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08838834764831842
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.4,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20951312035156971
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10475656017578479
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 5.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.24382992454708538
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 7.0,
            "std": 1.9798989873223336,
            "coefficient_of_variation": 0.28284271247461906
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0872971334798207
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.23570226039551587
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.128564869306645
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 2.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 1.1966422450849266
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.29351602237932156
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 4.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.8947065394605295
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 6.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.10247924365022427
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.5533879157112113
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 5.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.07713892158398696
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 7.699999999999999,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.1652976891085436
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 8.2,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0689860274328339
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.17141982574219342
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 4.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.2597535114562827
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.0999999999999996,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.3193385463423118
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 7.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.12221598687174896
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.3856946079199349
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 6.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.18247916933846384
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 9.0
            },
            "mean": 8.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0831890330807703
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.3,
            "std": 0.1414213562373093,
            "coefficient_of_variation": 0.04285495643554828
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.1,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.5173952057462543
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 7.1000000000000005,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.33861451493440303
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 4.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.24595018476053837
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05811836557697648
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 4.1,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.03449301371641699
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 6.0
            },
            "mean": 6.3,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.06734350297014735
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.8,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.3988807483616422
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.9,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.3759301874662657
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.2693740118805895
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.016637806616154
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.020495848730044876
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.100000000000001,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.04662242513317893
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 3.8000000000000003,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.22329687826943606
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 8.3,
            "std": 1.2727922061357848,
            "coefficient_of_variation": 0.15334845857057647
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 3.1,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 0.5930573003500076
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 3.6999999999999997,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 1.1084376569951286
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 5.3,
            "std": 4.1012193308819755,
            "coefficient_of_variation": 0.7738149680909389
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.0,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.565685424949238
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 2.1,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 6.8,
            "std": 3.959797974644666,
            "coefficient_of_variation": 0.582323231565392
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 8.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2113192679408073
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.24692617755720705
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 3.4000000000000004,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7487012977269325
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 2.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.15713484026367724
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.5999999999999996,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.8642416214502249
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 9.0,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06285393610547095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 8.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03214121732666128
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.899999999999999,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11123028018664798
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.9,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.3263569759322527
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 7.6,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.11164843913471797
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.043738563784734875
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 6.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3816131834975019
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.7,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.32253993527807423
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 5.6,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.10101525445522105
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 8.3,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.11927102333267069
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.41057813101154367
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.800000000000001,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.12478354962115545
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 5.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.15152288168283165
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.6,
            "std": 2.2627416997969516,
            "coefficient_of_variation": 0.3428396514843866
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 5.6
            },
            "mean": 6.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.24956709924231094
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 6.4,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.5303300858899107
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.699999999999999,
            "std": 2.6870057685088806,
            "coefficient_of_variation": 0.40104563709087776
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 4.7,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.6318826555284042
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.2,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.12297509238026912
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.6,
            "std": 1.1313708498984765,
            "coefficient_of_variation": 0.20203050891044225
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 7.300000000000001,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.21310067378224717
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 7.6
            },
            "mean": 7.5,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.0188561808316412
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 6.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.3193385463423118
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 7.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.056568542494923775
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.18446263857040374
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.4,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.3856946079199349
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.28284271247461895
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 4.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.7378505542816148
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 2.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.8249579113843054
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.2,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.7954951288348658
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 5.3,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.3468825719028346
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.6,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.6285393610547089
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.5303300858899107
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.8485281374238571
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.5,
            "std": 1.8384776310850235,
            "coefficient_of_variation": 0.7353910524340094
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.2000000000000002,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 1.178511301977579
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 0.7999999999999999,
            "std": 0.8485281374238569,
            "coefficient_of_variation": 1.0606601717798212
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 4.4,
            "std": 0.2828427124746186,
            "coefficient_of_variation": 0.06428243465332241
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.9,
            "std": 2.687005768508881,
            "coefficient_of_variation": 0.3401273124694786
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.282842712474619
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.9,
            "std": 0.141421356237309,
            "coefficient_of_variation": 0.01428498547851606
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.0,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09428090415820628
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 3.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.11466596451673744
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.5,
            "std": 1.838477631085024,
            "coefficient_of_variation": 0.2162914860100028
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 7.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.0764439763444916
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 5.9,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.11984860698077077
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 7.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.017901437498393624
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.12121830534626522
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 6.2
            },
            "mean": 6.5,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.0652713951864505
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 3.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.582323231565392
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 4.9,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.0,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.565685424949238
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.3,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.6763630080914803
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.100000000000001,
            "std": 0.9899494936611668,
            "coefficient_of_variation": 0.10878565864408425
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.9,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.17479044029330387
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.0,
            "std": 1.1313708498984771,
            "coefficient_of_variation": 0.1257078722109419
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 6.4,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.08838834764831842
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 8.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 5.4
            },
            "mean": 3.6,
            "std": 2.5455844122715714,
            "coefficient_of_variation": 0.7071067811865476
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 9.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0777040418886316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 8.2
            },
            "mean": 6.0,
            "std": 3.1112698372208087,
            "coefficient_of_variation": 0.5185449728701348
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 8.2,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.24145109601491868
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 3.0,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.28284271247461906
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.22809896167307986
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.2,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.8,
            "std": 3.3941125496954285,
            "coefficient_of_variation": 0.7071067811865477
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 5.7,
            "std": 3.252691193458119,
            "coefficient_of_variation": 0.5706475777996699
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.2,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 2.7,
            "std": 2.121320343559643,
            "coefficient_of_variation": 0.7856742013183862
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 3.5,
            "std": 2.9698484809834995,
            "coefficient_of_variation": 0.848528137423857
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 3.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.29998469504883835
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 3.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.08318903308077027
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 6.2,
            "std": 3.6769552621700474,
            "coefficient_of_variation": 0.5930573003500076
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 8.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.1767766952966369
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 4.2
            },
            "mean": 5.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.2719641466102106
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 5.3,
            "std": 3.5355339059327378,
            "coefficient_of_variation": 0.6670818690439128
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.4,
            "std": 0.8485281374238566,
            "coefficient_of_variation": 0.09026895078977197
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.8,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.2925959094565025
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.2,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.1,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.13986727539953694
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 4.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.3535533905932738
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 8.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.20695808229850177
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.10101525445522105
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 4.8
            },
            "mean": 3.7,
            "std": 1.5556349186104044,
            "coefficient_of_variation": 0.4204418698947039
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.4,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 4.800000000000001,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 5.0
            },
            "mean": 6.2,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.27371875400769585
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.2,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 4.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.37942315088058653
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.25502211780498435
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 5.2,
            "std": 3.676955262170047,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.8,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 6.5,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.28284271247461895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 2.3,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.430412823330942
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.5142594772265799
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 5.5,
            "std": 1.5556349186104041,
            "coefficient_of_variation": 0.28284271247461895
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 3.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 5.6,
            "std": 3.394112549695428,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 6.6
            },
            "mean": 7.5,
            "std": 1.2727922061357861,
            "coefficient_of_variation": 0.1697056274847715
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.04040610178208847
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 4.6,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.6763630080914803
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.0,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 4.4,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.19284730395996752
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0999999999999996,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.13685937700384795
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 5.2,
            "std": 3.111269837220809,
            "coefficient_of_variation": 0.5983211225424633
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 5.699999999999999,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.27291840677375523
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.8,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.30304576336566313
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.6,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 4.199999999999999,
            "std": 1.979898987322333,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 2.4000000000000004,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.11785113019775788
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.5,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 0.39597979746446665
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.4,
              "fxx_gemini2.5-pro": 4.4
            },
            "mean": 6.4,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 0.4419417382415922
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.2,
              "fxx_gemini2.5-pro": 6.8
            },
            "mean": 8.0,
            "std": 1.6970562748477136,
            "coefficient_of_variation": 0.2121320343559642
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 3.0,
            "std": 1.1313708498984758,
            "coefficient_of_variation": 0.3771236166328253
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 3.6
            },
            "mean": 5.6,
            "std": 2.82842712474619,
            "coefficient_of_variation": 0.5050762722761054
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 8.4
            },
            "mean": 8.2,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.03449301371641699
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.2,
              "fxx_gemini2.5-pro": 5.8
            },
            "mean": 6.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.047140452079103216
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.0744322927564787
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.0,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 9.4,
            "std": 0.5656854249492386,
            "coefficient_of_variation": 0.06017930052651474
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 7.8
            },
            "mean": 6.8,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.20797258270192576
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 8.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.15713484026367724
          }
        }
      }
    },
    "darwin": {
      "1": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.5,
            "std": 4.949747468305833,
            "coefficient_of_variation": 0.7614996105085896
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 2.3,
            "std": 0.7071067811865474,
            "coefficient_of_variation": 0.3074377309506728
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 5.0,
            "std": 7.0710678118654755,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 10.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 10.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.8999999999999999,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 0.7856742013183862
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.8,
            "std": 4.525483399593904,
            "coefficient_of_variation": 0.6655122646461624
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.5303300858899107
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.6,
            "std": 0.2828427124746191,
            "coefficient_of_variation": 0.17677669529663695
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.6
            },
            "mean": 1.3,
            "std": 1.8384776310850237,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.5,
            "std": 1.2727922061357855,
            "coefficient_of_variation": 0.5091168824543142
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 3.0,
            "std": 0.5656854249492379,
            "coefficient_of_variation": 0.18856180831641264
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.5,
            "std": 0.14142135623730964,
            "coefficient_of_variation": 0.09428090415820643
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.6,
            "std": 0.848528137423857,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 0.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.07443229275647867
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.1,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.6428243465332251
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          }
        }
      },
      "2": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.3,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.3263569759322527
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 5.6,
            "std": 5.65685424949238,
            "coefficient_of_variation": 1.0101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 2.8,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.4040610178208843
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.3,
            "std": 5.232590180780452,
            "coefficient_of_variation": 0.8305698699651511
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 8.0
            },
            "mean": 7.7,
            "std": 0.4242640687119283,
            "coefficient_of_variation": 0.05509922970284783
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 9.2
            },
            "mean": 7.8999999999999995,
            "std": 1.8384776310850233,
            "coefficient_of_variation": 0.2327186874791169
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.8,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 6.0,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.28284271247461906
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7,
            "std": 0.7071067811865475,
            "coefficient_of_variation": 1.0101525445522108
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "3": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.28284271247461906
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 2.1999999999999997,
            "std": 2.262741699796952,
            "coefficient_of_variation": 1.02851895445316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.4,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 8.600000000000001,
            "std": 1.6970562748477143,
            "coefficient_of_variation": 0.19733212498229233
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.9,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 1.0999438818457405
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.2,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.6428243465332251
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.9428090415820634
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 4.1,
            "std": 4.949747468305833,
            "coefficient_of_variation": 1.2072554800745934
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 9.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 9.8,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.028861501272920333
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7999999999999999,
            "std": 0.8485281374238569,
            "coefficient_of_variation": 1.0606601717798212
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.0,
            "std": 1.4142135623730951,
            "coefficient_of_variation": 0.7071067811865476
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.7,
            "std": 0.14142135623730956,
            "coefficient_of_variation": 0.20203050891044225
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 5.0,
            "std": 7.0710678118654755,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.8,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.6060915267313265
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 4.6
            },
            "mean": 3.1999999999999997,
            "std": 1.9798989873223327,
            "coefficient_of_variation": 0.618718433538229
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 1.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4040610178208843
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.4,
            "std": 5.091168824543142,
            "coefficient_of_variation": 0.7954951288348658
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 7.0
            },
            "mean": 5.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.385694607919935
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 5.8,
              "fxx_gemini2.5-pro": 9.8
            },
            "mean": 7.800000000000001,
            "std": 2.8284271247461907,
            "coefficient_of_variation": 0.3626188621469475
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          }
        }
      },
      "4": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 1.6
            },
            "mean": 1.2000000000000002,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.30000000000000004,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 0.4714045207910316
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 8.8
            },
            "mean": 5.1000000000000005,
            "std": 5.232590180780452,
            "coefficient_of_variation": 1.0259980746628337
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 1.2,
            "std": 1.131370849898476,
            "coefficient_of_variation": 0.9428090415820634
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.0,
              "fxx_gemini2.5-pro": 9.4
            },
            "mean": 7.7,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.3122289683161379
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.2,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.8,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.30304576336566313
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 8.6
            },
            "mean": 5.9,
            "std": 3.818376618407356,
            "coefficient_of_variation": 0.647182477696162
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 7.2
            },
            "mean": 5.4,
            "std": 2.545584412271571,
            "coefficient_of_variation": 0.4714045207910316
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.8,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.1571348402636772
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.2,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 3.0,
            "std": 0.2828427124746193,
            "coefficient_of_variation": 0.09428090415820643
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 6.9,
            "std": 4.384062043356595,
            "coefficient_of_variation": 0.6353713106313905
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 4.0
            },
            "mean": 2.0,
            "std": 2.8284271247461903,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 7.4
            },
            "mean": 4.2,
            "std": 4.525483399593904,
            "coefficient_of_variation": 1.077496047522358
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.6,
              "fxx_gemini2.5-pro": 6.4
            },
            "mean": 5.0,
            "std": 1.9798989873223332,
            "coefficient_of_variation": 0.39597979746446665
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 5.2
            },
            "mean": 4.6,
            "std": 0.8485281374238571,
            "coefficient_of_variation": 0.18446263857040374
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 3.8
            },
            "mean": 3.2,
            "std": 0.8485281374238568,
            "coefficient_of_variation": 0.26516504294495524
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 7.0,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 0.24956709924231088
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 9.6
            },
            "mean": 6.1,
            "std": 4.949747468305833,
            "coefficient_of_variation": 0.8114340111976776
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 7.3,
            "std": 3.818376618407357,
            "coefficient_of_variation": 0.5230652901927887
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.5,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.28284271247461895
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.8,
              "fxx_gemini2.5-pro": 3.0
            },
            "mean": 2.4,
            "std": 0.848528137423857,
            "coefficient_of_variation": 0.3535533905932738
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.0,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        }
      },
      "5": {
        "q_1": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_2": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.4,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.7071067811865475
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.4,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.1,
            "std": 1.8384776310850235,
            "coefficient_of_variation": 0.8754655386119159
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_3": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_4": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 6.6,
              "fxx_gemini2.5-pro": 10.0
            },
            "mean": 8.3,
            "std": 2.4041630560342617,
            "coefficient_of_variation": 0.2896581995222002
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_5": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_6": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 2.0,
            "std": 2.545584412271571,
            "coefficient_of_variation": 1.2727922061357855
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_7": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.2,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.1,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.12856486930664496
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 1.0,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.565685424949238
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.3,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_8": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 3.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 1.5,
            "std": 2.1213203435596424,
            "coefficient_of_variation": 1.414213562373095
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_9": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.7,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.24956709924231094
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.4
            },
            "mean": 2.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 0.77138921583987
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        },
        "q_10": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_11": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_12": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.2
            },
            "mean": 1.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.8,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 0.9,
            "std": 0.14142135623730948,
            "coefficient_of_variation": 0.1571348402636772
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.4,
            "std": 0.565685424949238,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_13": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.7407785326716212
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 2.0
            },
            "mean": 1.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 0.47140452079103173
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          }
        },
        "q_14": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.7999999999999999,
            "std": 0.8485281374238569,
            "coefficient_of_variation": 1.0606601717798212
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          }
        },
        "q_15": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_16": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_17": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_18": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.8
            },
            "mean": 1.4,
            "std": 1.979898987322333,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.4,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.4,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.6,
              "fxx_gemini2.5-pro": 1.8
            },
            "mean": 2.2,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.25712973861328997
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.2,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.8,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.35355339059327373
          }
        },
        "q_19": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_20": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.6,
            "std": 0.565685424949238,
            "coefficient_of_variation": 0.9428090415820634
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_21": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 3.2
            },
            "mean": 2.1,
            "std": 1.5556349186104046,
            "coefficient_of_variation": 0.7407785326716212
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 2.4,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 1.7,
            "std": 0.9899494936611665,
            "coefficient_of_variation": 0.582323231565392
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 4.0,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 2.4,
            "std": 2.262741699796952,
            "coefficient_of_variation": 0.9428090415820634
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.6,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.9,
            "std": 0.9899494936611666,
            "coefficient_of_variation": 1.0999438818457405
          }
        },
        "q_22": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.6000000000000001,
            "std": 0.282842712474619,
            "coefficient_of_variation": 0.4714045207910316
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.7,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.6060915267313265
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.8,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.5,
            "std": 0.42426406871192857,
            "coefficient_of_variation": 0.8485281374238571
          }
        },
        "q_23": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_24": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          }
        },
        "q_25": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          }
        },
        "q_26": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.4
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.0,
              "fxx_gemini2.5-pro": 1.4
            },
            "mean": 1.2,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.2357022603955158
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 1.2,
              "fxx_gemini2.5-pro": 0.8
            },
            "mean": 1.0,
            "std": 0.28284271247461895,
            "coefficient_of_variation": 0.28284271247461895
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.0,
            "std": 0.0,
            "coefficient_of_variation": 0
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.4,
              "fxx_gemini2.5-pro": 0.0
            },
            "mean": 0.2,
            "std": 0.282842712474619,
            "coefficient_of_variation": 1.414213562373095
          }
        },
        "q_27": {
          "correctness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 2.4
            },
            "mean": 1.2,
            "std": 1.697056274847714,
            "coefficient_of_variation": 1.4142135623730951
          },
          "completeness": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 1.0
            },
            "mean": 0.5,
            "std": 0.7071067811865476,
            "coefficient_of_variation": 1.4142135623730951
          },
          "logic": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          },
          "clarity": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 1.2
            },
            "mean": 0.8999999999999999,
            "std": 0.4242640687119285,
            "coefficient_of_variation": 0.47140452079103173
          },
          "theoretical_depth": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.0,
              "fxx_gemini2.5-pro": 0.2
            },
            "mean": 0.1,
            "std": 0.1414213562373095,
            "coefficient_of_variation": 1.414213562373095
          },
          "rigor_and_information_density": {
            "judge_scores": {
              "Doubao-Seed-1.6-combined": 0.6,
              "fxx_gemini2.5-pro": 0.6
            },
            "mean": 0.6,
            "std": 0.0,
            "coefficient_of_variation": 0.0
          }
        }
      }
    }
  },
  "data_quality_report": {
    "summary": {
      "total_records": 40500,
      "array_wrapped_scores": 18622,
      "invalid_scores_count": 0,
      "missing_evaluations_count": 4,
      "json_parse_errors_count": 4,
      "incomplete_evaluation_sets_count": 21
    },
    "details": {
      "array_wrapped_scores": 18622,
      "invalid_scores": [],
      "missing_evaluations": [
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "clarity",
          "eval_round": 1,
          "available_dimensions": [
            "logic"
          ],
          "original_answer": "```json\n{\n  \"logic\": "
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "theoretical_depth",
          "eval_round": 1,
          "available_dimensions": [
            "logic"
          ],
          "original_answer": "```json\n{\n  \"logic\": "
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "rigor_and_information_density",
          "eval_round": 1,
          "available_dimensions": [
            "logic"
          ],
          "original_answer": "```json\n{\n  \"logic\": "
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "spark-chem13b-think",
          "round": "3",
          "question": "q_23",
          "dimension": "completeness",
          "eval_round": 5,
          "available_dimensions": [
            "correctness"
          ],
          "original_answer": "```json\n{\n  \"correctness\": "
        }
      ],
      "json_parse_errors": [
        {
          "record": "fxx_gemini2.5-pro_llasmol-top5_1_q_27_3",
          "answer": "```json\n{\n  \"logic\": 0,\n  \"clarity\": 0,\n  \"theoretical_depth\": 0,\n  \"",
          "cleaned_answer": "{\n  \"logic\": 0,\n  \"clarity\": 0,\n  \"theoretical_depth\": 0,\n  \"",
          "fix_attempted": "{\n  \"logic\": 0,\n  \"clarity\": 0,\n  \"theoretical_depth\": 0,\n  \"\"}"
        },
        {
          "record": "fxx_gemini2.5-pro_llasmol-top5_5_q_19_3",
          "answer": "```json",
          "cleaned_answer": "```json"
        },
        {
          "record": "fxx_gemini2.5-pro_MOSES_5_q_2_3",
          "answer": "```json\n{\n  \"logic\": 10,\n  \"clarity\": 10,\n  \"theoretical_depth",
          "cleaned_answer": "{\n  \"logic\": 10,\n  \"clarity\": 10,\n  \"theoretical_depth",
          "fix_attempted": "{\n  \"logic\": 10,\n  \"clarity\": 10,\n  \"theoretical_depth}"
        },
        {
          "record": "fxx_gemini2.5-pro_spark-chem13b-think_1_q_27_3",
          "answer": "```",
          "cleaned_answer": "```"
        }
      ],
      "incomplete_evaluation_sets": [
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "logic",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "clarity",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "theoretical_depth",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "lightrag-4.1-nano",
          "round": "3",
          "question": "q_9",
          "dimension": "rigor_and_information_density",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top1",
          "round": "2",
          "question": "q_13",
          "dimension": "correctness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top1",
          "round": "2",
          "question": "q_13",
          "dimension": "completeness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "1",
          "question": "q_27",
          "dimension": "logic",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "1",
          "question": "q_27",
          "dimension": "clarity",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "1",
          "question": "q_27",
          "dimension": "theoretical_depth",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "1",
          "question": "q_27",
          "dimension": "rigor_and_information_density",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "5",
          "question": "q_19",
          "dimension": "correctness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "llasmol-top5",
          "round": "5",
          "question": "q_19",
          "dimension": "completeness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "MOSES",
          "round": "5",
          "question": "q_2",
          "dimension": "logic",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "MOSES",
          "round": "5",
          "question": "q_2",
          "dimension": "clarity",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "MOSES",
          "round": "5",
          "question": "q_2",
          "dimension": "theoretical_depth",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "MOSES",
          "round": "5",
          "question": "q_2",
          "dimension": "rigor_and_information_density",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "o1",
          "round": "5",
          "question": "q_14",
          "dimension": "rigor_and_information_density",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "spark-chem13b-think",
          "round": "1",
          "question": "q_27",
          "dimension": "correctness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "spark-chem13b-think",
          "round": "1",
          "question": "q_27",
          "dimension": "completeness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "spark-chem13b-think",
          "round": "3",
          "question": "q_23",
          "dimension": "correctness",
          "count": 4,
          "expected": 5
        },
        {
          "judge": "fxx_gemini2.5-pro",
          "model": "spark-chem13b-think",
          "round": "3",
          "question": "q_23",
          "dimension": "completeness",
          "count": 4,
          "expected": 5
        }
      ]
    }
  },
  "model_average_scores_no_lc": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "overall_average": 6.4837037037037035,
        "overall_std": 1.823502386281503,
        "dimension_averages": {
          "correctness": 8.465185185185184,
          "completeness": 6.622222222222222,
          "theoretical_depth": 3.2,
          "rigor_and_information_density": 7.647407407407408
        },
        "total_evaluations": 270
      },
      "gpt-4.1-nano": {
        "overall_average": 5.805185185185185,
        "overall_std": 1.7779031347790095,
        "dimension_averages": {
          "correctness": 7.857777777777778,
          "completeness": 5.348148148148148,
          "theoretical_depth": 3.1155555555555554,
          "rigor_and_information_density": 6.899259259259259
        },
        "total_evaluations": 270
      },
      "gpt-4o": {
        "overall_average": 5.195925925925926,
        "overall_std": 1.8993302659560418,
        "dimension_averages": {
          "correctness": 6.96,
          "completeness": 4.88,
          "theoretical_depth": 2.745185185185185,
          "rigor_and_information_density": 6.198518518518519
        },
        "total_evaluations": 270
      },
      "gpt-4o-mini": {
        "overall_average": 4.422592592592593,
        "overall_std": 1.9585786755189882,
        "dimension_averages": {
          "correctness": 5.84,
          "completeness": 4.065185185185185,
          "theoretical_depth": 2.4340740740740743,
          "rigor_and_information_density": 5.351111111111111
        },
        "total_evaluations": 270
      },
      "lightrag-4.1": {
        "overall_average": 6.09,
        "overall_std": 1.5582172159854057,
        "dimension_averages": {
          "correctness": 7.6251851851851855,
          "completeness": 5.522962962962963,
          "theoretical_depth": 4.025185185185185,
          "rigor_and_information_density": 7.1866666666666665
        },
        "total_evaluations": 270
      },
      "lightrag-4.1-nano": {
        "overall_average": 6.351851851851852,
        "overall_std": 1.5168619462047763,
        "dimension_averages": {
          "correctness": 8.016296296296296,
          "completeness": 6.0162962962962965,
          "theoretical_depth": 4.025185185185185,
          "rigor_and_information_density": 7.3496296296296295
        },
        "total_evaluations": 270
      },
      "llasmol-top1": {
        "overall_average": 0.5192592592592593,
        "overall_std": 1.114520337675534,
        "dimension_averages": {
          "correctness": 1.1733333333333333,
          "completeness": 0.14074074074074075,
          "theoretical_depth": 0.16,
          "rigor_and_information_density": 0.6029629629629629
        },
        "total_evaluations": 270
      },
      "llasmol-top5": {
        "overall_average": 0.5077777777777778,
        "overall_std": 1.2064782795017661,
        "dimension_averages": {
          "correctness": 1.2148148148148148,
          "completeness": 0.10518518518518519,
          "theoretical_depth": 0.09925925925925926,
          "rigor_and_information_density": 0.6118518518518519
        },
        "total_evaluations": 270
      },
      "MOSES": {
        "overall_average": 8.344444444444445,
        "overall_std": 1.410194780327684,
        "dimension_averages": {
          "correctness": 9.564444444444444,
          "completeness": 8.722962962962963,
          "theoretical_depth": 6.508148148148148,
          "rigor_and_information_density": 8.582222222222223
        },
        "total_evaluations": 270
      },
      "MOSES-nano": {
        "overall_average": 5.936296296296296,
        "overall_std": 1.9011679997262887,
        "dimension_averages": {
          "correctness": 7.546666666666667,
          "completeness": 5.312592592592592,
          "theoretical_depth": 4.001481481481481,
          "rigor_and_information_density": 6.884444444444444
        },
        "total_evaluations": 270
      },
      "o1": {
        "overall_average": 6.325555555555556,
        "overall_std": 1.7988572029705576,
        "dimension_averages": {
          "correctness": 8.287407407407407,
          "completeness": 5.505185185185185,
          "theoretical_depth": 3.6014814814814815,
          "rigor_and_information_density": 7.908148148148148
        },
        "total_evaluations": 270
      },
      "o3": {
        "overall_average": 7.635185185185185,
        "overall_std": 1.8783205944518064,
        "dimension_averages": {
          "correctness": 9.285925925925927,
          "completeness": 7.871111111111111,
          "theoretical_depth": 4.545185185185185,
          "rigor_and_information_density": 8.83851851851852
        },
        "total_evaluations": 270
      },
      "spark-chem13b-nothink": {
        "overall_average": 4.697777777777778,
        "overall_std": 2.4182535916865873,
        "dimension_averages": {
          "correctness": 6.342222222222222,
          "completeness": 4.337777777777778,
          "theoretical_depth": 2.5896296296296297,
          "rigor_and_information_density": 5.521481481481482
        },
        "total_evaluations": 270
      },
      "spark-chem13b-think": {
        "overall_average": 4.814444444444445,
        "overall_std": 2.273602897673055,
        "dimension_averages": {
          "correctness": 6.3348148148148145,
          "completeness": 4.373333333333333,
          "theoretical_depth": 2.788148148148148,
          "rigor_and_information_density": 5.761481481481481
        },
        "total_evaluations": 270
      },
      "darwin": {
        "overall_average": 0.16777777777777778,
        "overall_std": 0.47600282193825766,
        "dimension_averages": {
          "correctness": 0.21037037037037037,
          "completeness": 0.035555555555555556,
          "theoretical_depth": 0.04148148148148148,
          "rigor_and_information_density": 0.3837037037037037
        },
        "total_evaluations": 270
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "overall_average": 7.1648148148148145,
        "overall_std": 1.8380486468079384,
        "dimension_averages": {
          "correctness": 9.177777777777777,
          "completeness": 6.5851851851851855,
          "theoretical_depth": 4.271111111111111,
          "rigor_and_information_density": 8.625185185185185
        },
        "total_evaluations": 270
      },
      "gpt-4.1-nano": {
        "overall_average": 5.744074074074074,
        "overall_std": 2.2186649243491554,
        "dimension_averages": {
          "correctness": 8.52,
          "completeness": 5.017777777777778,
          "theoretical_depth": 3.185185185185185,
          "rigor_and_information_density": 6.253333333333333
        },
        "total_evaluations": 270
      },
      "gpt-4o": {
        "overall_average": 4.997777777777777,
        "overall_std": 2.4179768699766866,
        "dimension_averages": {
          "correctness": 7.841481481481481,
          "completeness": 4.742222222222222,
          "theoretical_depth": 2.3466666666666667,
          "rigor_and_information_density": 5.060740740740741
        },
        "total_evaluations": 270
      },
      "gpt-4o-mini": {
        "overall_average": 4.021111111111111,
        "overall_std": 2.611818767421235,
        "dimension_averages": {
          "correctness": 6.321481481481482,
          "completeness": 4.074074074074074,
          "theoretical_depth": 1.854814814814815,
          "rigor_and_information_density": 3.834074074074074
        },
        "total_evaluations": 270
      },
      "lightrag-4.1": {
        "overall_average": 7.3100000000000005,
        "overall_std": 1.7976678406834532,
        "dimension_averages": {
          "correctness": 9.16,
          "completeness": 5.463703703703704,
          "theoretical_depth": 6.232592592592592,
          "rigor_and_information_density": 8.383703703703704
        },
        "total_evaluations": 270
      },
      "lightrag-4.1-nano": {
        "overall_average": 7.452037037037037,
        "overall_std": 1.7455321401093584,
        "dimension_averages": {
          "correctness": 9.102222222222222,
          "completeness": 5.955555555555556,
          "theoretical_depth": 6.1866666666666665,
          "rigor_and_information_density": 8.563703703703704
        },
        "total_evaluations": 270
      },
      "llasmol-top1": {
        "overall_average": 0.41638888888888886,
        "overall_std": 1.03009203791095,
        "dimension_averages": {
          "correctness": 1.2344444444444445,
          "completeness": 0.010370370370370372,
          "theoretical_depth": 0.05925925925925926,
          "rigor_and_information_density": 0.36148148148148146
        },
        "total_evaluations": 270
      },
      "llasmol-top5": {
        "overall_average": 0.5951851851851852,
        "overall_std": 1.380727329397089,
        "dimension_averages": {
          "correctness": 1.8325925925925926,
          "completeness": 0.01925925925925926,
          "theoretical_depth": 0.031111111111111114,
          "rigor_and_information_density": 0.49777777777777776
        },
        "total_evaluations": 270
      },
      "MOSES": {
        "overall_average": 9.142870370370371,
        "overall_std": 1.1538434246297027,
        "dimension_averages": {
          "correctness": 9.666666666666666,
          "completeness": 8.59851851851852,
          "theoretical_depth": 8.684074074074074,
          "rigor_and_information_density": 9.622222222222222
        },
        "total_evaluations": 270
      },
      "MOSES-nano": {
        "overall_average": 6.156666666666666,
        "overall_std": 2.4944421491964355,
        "dimension_averages": {
          "correctness": 7.899259259259259,
          "completeness": 5.488888888888889,
          "theoretical_depth": 4.745185185185186,
          "rigor_and_information_density": 6.493333333333333
        },
        "total_evaluations": 270
      },
      "o1": {
        "overall_average": 7.477407407407408,
        "overall_std": 2.0556859085453123,
        "dimension_averages": {
          "correctness": 9.114074074074074,
          "completeness": 5.42962962962963,
          "theoretical_depth": 6.026666666666666,
          "rigor_and_information_density": 9.33925925925926
        },
        "total_evaluations": 270
      },
      "o3": {
        "overall_average": 8.914814814814815,
        "overall_std": 1.635253741256297,
        "dimension_averages": {
          "correctness": 9.379259259259259,
          "completeness": 8.16,
          "theoretical_depth": 8.256296296296297,
          "rigor_and_information_density": 9.863703703703704
        },
        "total_evaluations": 270
      },
      "spark-chem13b-nothink": {
        "overall_average": 4.547037037037037,
        "overall_std": 2.849778331971469,
        "dimension_averages": {
          "correctness": 6.2340740740740745,
          "completeness": 4.602962962962963,
          "theoretical_depth": 2.6785185185185187,
          "rigor_and_information_density": 4.672592592592593
        },
        "total_evaluations": 270
      },
      "spark-chem13b-think": {
        "overall_average": 4.850925925925926,
        "overall_std": 2.75458793936266,
        "dimension_averages": {
          "correctness": 6.408148148148148,
          "completeness": 4.902222222222222,
          "theoretical_depth": 3.0148148148148146,
          "rigor_and_information_density": 5.078518518518519
        },
        "total_evaluations": 270
      },
      "darwin": {
        "overall_average": 0.2725925925925926,
        "overall_std": 0.9031073684252798,
        "dimension_averages": {
          "correctness": 0.7496296296296296,
          "completeness": 0.06666666666666667,
          "theoretical_depth": 0.03851851851851852,
          "rigor_and_information_density": 0.23555555555555555
        },
        "total_evaluations": 270
      }
    }
  },
  "best_answer_rounds_no_lc": {
    "Doubao-Seed-1.6-combined": {
      "gpt-4.1": {
        "best_round": "4",
        "best_score": 6.57962962962963,
        "best_std": 0.9930778940766448,
        "all_round_stats": {
          "1": {
            "mean": 6.468518518518518,
            "std": 1.019063309363078,
            "count": 27
          },
          "2": {
            "mean": 6.425925925925926,
            "std": 1.1375894191709444,
            "count": 27
          },
          "3": {
            "mean": 6.453703703703704,
            "std": 1.050817074574126,
            "count": 27
          },
          "4": {
            "mean": 6.57962962962963,
            "std": 0.9930778940766448,
            "count": 27
          },
          "5": {
            "mean": 6.4907407407407405,
            "std": 1.1133833603581953,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.468518518518518,
          "2": 6.425925925925926,
          "3": 6.453703703703704,
          "4": 6.57962962962963,
          "5": 6.4907407407407405
        }
      },
      "gpt-4.1-nano": {
        "best_round": "2",
        "best_score": 5.857407407407408,
        "best_std": 1.193709772073187,
        "all_round_stats": {
          "1": {
            "mean": 5.805555555555555,
            "std": 1.4152557076191983,
            "count": 27
          },
          "2": {
            "mean": 5.857407407407408,
            "std": 1.193709772073187,
            "count": 27
          },
          "3": {
            "mean": 5.82037037037037,
            "std": 1.3667239929607122,
            "count": 27
          },
          "4": {
            "mean": 5.7407407407407405,
            "std": 1.3290422402441828,
            "count": 27
          },
          "5": {
            "mean": 5.801851851851851,
            "std": 1.2150088224973672,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.805555555555555,
          "2": 5.857407407407408,
          "3": 5.82037037037037,
          "4": 5.7407407407407405,
          "5": 5.801851851851851
        }
      },
      "gpt-4o": {
        "best_round": "1",
        "best_score": 5.294444444444444,
        "best_std": 1.5441162105861306,
        "all_round_stats": {
          "1": {
            "mean": 5.294444444444444,
            "std": 1.5441162105861306,
            "count": 27
          },
          "2": {
            "mean": 5.214814814814814,
            "std": 1.4038282011106693,
            "count": 27
          },
          "3": {
            "mean": 5.17962962962963,
            "std": 1.536996275160597,
            "count": 27
          },
          "4": {
            "mean": 5.162962962962963,
            "std": 1.486515840726006,
            "count": 27
          },
          "5": {
            "mean": 5.127777777777778,
            "std": 1.4016702490883821,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.294444444444444,
          "2": 5.214814814814814,
          "3": 5.17962962962963,
          "4": 5.162962962962963,
          "5": 5.127777777777778
        }
      },
      "gpt-4o-mini": {
        "best_round": "2",
        "best_score": 4.4981481481481485,
        "best_std": 1.5849339637712574,
        "all_round_stats": {
          "1": {
            "mean": 4.468518518518518,
            "std": 1.6762565979992987,
            "count": 27
          },
          "2": {
            "mean": 4.4981481481481485,
            "std": 1.5849339637712574,
            "count": 27
          },
          "3": {
            "mean": 4.387037037037037,
            "std": 1.6819512816844946,
            "count": 27
          },
          "4": {
            "mean": 4.366666666666666,
            "std": 1.6005407739982038,
            "count": 27
          },
          "5": {
            "mean": 4.392592592592592,
            "std": 1.64934157630481,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.468518518518518,
          "2": 4.4981481481481485,
          "3": 4.387037037037037,
          "4": 4.366666666666666,
          "5": 4.392592592592592
        }
      },
      "lightrag-4.1": {
        "best_round": "1",
        "best_score": 6.701851851851852,
        "best_std": 0.8515236888029556,
        "all_round_stats": {
          "1": {
            "mean": 6.701851851851852,
            "std": 0.8515236888029556,
            "count": 27
          },
          "2": {
            "mean": 5.979629629629629,
            "std": 1.181122287297068,
            "count": 27
          },
          "3": {
            "mean": 5.790740740740741,
            "std": 1.4099943423943393,
            "count": 27
          },
          "4": {
            "mean": 6.094444444444444,
            "std": 1.1503622617824931,
            "count": 27
          },
          "5": {
            "mean": 5.883333333333334,
            "std": 1.3246189582726742,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.701851851851852,
          "2": 5.979629629629629,
          "3": 5.790740740740741,
          "4": 6.094444444444444,
          "5": 5.883333333333334
        }
      },
      "lightrag-4.1-nano": {
        "best_round": "4",
        "best_score": 6.555555555555555,
        "best_std": 1.0531393849356143,
        "all_round_stats": {
          "1": {
            "mean": 5.67962962962963,
            "std": 0.9783482213226752,
            "count": 27
          },
          "2": {
            "mean": 6.487037037037037,
            "std": 1.063429918181349,
            "count": 27
          },
          "3": {
            "mean": 6.531481481481482,
            "std": 1.13053324281306,
            "count": 27
          },
          "4": {
            "mean": 6.555555555555555,
            "std": 1.0531393849356143,
            "count": 27
          },
          "5": {
            "mean": 6.5055555555555555,
            "std": 1.0755439887772476,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.67962962962963,
          "2": 6.487037037037037,
          "3": 6.531481481481482,
          "4": 6.555555555555555,
          "5": 6.5055555555555555
        }
      },
      "llasmol-top1": {
        "best_round": "2",
        "best_score": 0.5388888888888889,
        "best_std": 1.0210602835112639,
        "all_round_stats": {
          "1": {
            "mean": 0.5018518518518519,
            "std": 0.9573080728996003,
            "count": 27
          },
          "2": {
            "mean": 0.5388888888888889,
            "std": 1.0210602835112639,
            "count": 27
          },
          "3": {
            "mean": 0.5037037037037037,
            "std": 0.9636594840189148,
            "count": 27
          },
          "4": {
            "mean": 0.5166666666666667,
            "std": 0.963567090794169,
            "count": 27
          },
          "5": {
            "mean": 0.5351851851851852,
            "std": 1.014018549255199,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.5018518518518519,
          "2": 0.5388888888888889,
          "3": 0.5037037037037037,
          "4": 0.5166666666666667,
          "5": 0.5351851851851852
        }
      },
      "llasmol-top5": {
        "best_round": "5",
        "best_score": 0.6907407407407408,
        "best_std": 1.131628934582377,
        "all_round_stats": {
          "1": {
            "mean": 0.37222222222222223,
            "std": 0.8602697847559814,
            "count": 27
          },
          "2": {
            "mean": 0.5,
            "std": 1.0089026788469815,
            "count": 27
          },
          "3": {
            "mean": 0.5703703703703704,
            "std": 1.2489425726820067,
            "count": 27
          },
          "4": {
            "mean": 0.40555555555555556,
            "std": 0.8649514585460999,
            "count": 27
          },
          "5": {
            "mean": 0.6907407407407408,
            "std": 1.131628934582377,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.37222222222222223,
          "2": 0.5,
          "3": 0.5703703703703704,
          "4": 0.40555555555555556,
          "5": 0.6907407407407408
        }
      },
      "MOSES": {
        "best_round": "4",
        "best_score": 8.375925925925927,
        "best_std": 0.7660300218020142,
        "all_round_stats": {
          "1": {
            "mean": 8.366666666666667,
            "std": 0.9612691930663656,
            "count": 27
          },
          "2": {
            "mean": 8.314814814814815,
            "std": 0.7707158030057831,
            "count": 27
          },
          "3": {
            "mean": 8.337037037037037,
            "std": 1.1778070065716815,
            "count": 27
          },
          "4": {
            "mean": 8.375925925925927,
            "std": 0.7660300218020142,
            "count": 27
          },
          "5": {
            "mean": 8.327777777777778,
            "std": 0.8452734347459285,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 8.366666666666667,
          "2": 8.314814814814815,
          "3": 8.337037037037037,
          "4": 8.375925925925927,
          "5": 8.327777777777778
        }
      },
      "MOSES-nano": {
        "best_round": "5",
        "best_score": 6.1722222222222225,
        "best_std": 1.563239568562013,
        "all_round_stats": {
          "1": {
            "mean": 6.02962962962963,
            "std": 1.7002408912619082,
            "count": 27
          },
          "2": {
            "mean": 5.885185185185185,
            "std": 1.5197934332573226,
            "count": 27
          },
          "3": {
            "mean": 5.781481481481482,
            "std": 1.3328070542560329,
            "count": 27
          },
          "4": {
            "mean": 5.812962962962963,
            "std": 1.647118263968491,
            "count": 27
          },
          "5": {
            "mean": 6.1722222222222225,
            "std": 1.563239568562013,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.02962962962963,
          "2": 5.885185185185185,
          "3": 5.781481481481482,
          "4": 5.812962962962963,
          "5": 6.1722222222222225
        }
      },
      "o1": {
        "best_round": "4",
        "best_score": 6.374074074074074,
        "best_std": 1.265091237996628,
        "all_round_stats": {
          "1": {
            "mean": 6.3314814814814815,
            "std": 1.40665643857423,
            "count": 27
          },
          "2": {
            "mean": 6.3,
            "std": 1.317923775080743,
            "count": 27
          },
          "3": {
            "mean": 6.307407407407408,
            "std": 1.340974366165234,
            "count": 27
          },
          "4": {
            "mean": 6.374074074074074,
            "std": 1.265091237996628,
            "count": 27
          },
          "5": {
            "mean": 6.314814814814815,
            "std": 1.398612967771368,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.3314814814814815,
          "2": 6.3,
          "3": 6.307407407407408,
          "4": 6.374074074074074,
          "5": 6.314814814814815
        }
      },
      "o3": {
        "best_round": "1",
        "best_score": 7.718518518518518,
        "best_std": 1.237176387224317,
        "all_round_stats": {
          "1": {
            "mean": 7.718518518518518,
            "std": 1.237176387224317,
            "count": 27
          },
          "2": {
            "mean": 7.590740740740741,
            "std": 1.2055191219040673,
            "count": 27
          },
          "3": {
            "mean": 7.633333333333333,
            "std": 1.0040303397193346,
            "count": 27
          },
          "4": {
            "mean": 7.5814814814814815,
            "std": 1.195516314928098,
            "count": 27
          },
          "5": {
            "mean": 7.651851851851852,
            "std": 1.247804339436143,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.718518518518518,
          "2": 7.590740740740741,
          "3": 7.633333333333333,
          "4": 7.5814814814814815,
          "5": 7.651851851851852
        }
      },
      "spark-chem13b-nothink": {
        "best_round": "3",
        "best_score": 4.905555555555556,
        "best_std": 2.1612110802356472,
        "all_round_stats": {
          "1": {
            "mean": 4.324074074074074,
            "std": 1.9812763160285949,
            "count": 27
          },
          "2": {
            "mean": 4.838888888888889,
            "std": 2.170622621300756,
            "count": 27
          },
          "3": {
            "mean": 4.905555555555556,
            "std": 2.1612110802356472,
            "count": 27
          },
          "4": {
            "mean": 4.590740740740741,
            "std": 1.7031101096476544,
            "count": 27
          },
          "5": {
            "mean": 4.82962962962963,
            "std": 2.1012477638874874,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.324074074074074,
          "2": 4.838888888888889,
          "3": 4.905555555555556,
          "4": 4.590740740740741,
          "5": 4.82962962962963
        }
      },
      "spark-chem13b-think": {
        "best_round": "2",
        "best_score": 5.053703703703704,
        "best_std": 1.9160218164571095,
        "all_round_stats": {
          "1": {
            "mean": 4.622222222222222,
            "std": 2.090285834950539,
            "count": 27
          },
          "2": {
            "mean": 5.053703703703704,
            "std": 1.9160218164571095,
            "count": 27
          },
          "3": {
            "mean": 4.772222222222222,
            "std": 2.0424501374825685,
            "count": 27
          },
          "4": {
            "mean": 4.5777777777777775,
            "std": 1.7297917467659183,
            "count": 27
          },
          "5": {
            "mean": 5.046296296296297,
            "std": 1.7089091170598776,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.622222222222222,
          "2": 5.053703703703704,
          "3": 4.772222222222222,
          "4": 4.5777777777777775,
          "5": 5.046296296296297
        }
      },
      "darwin": {
        "best_round": "4",
        "best_score": 0.2814814814814815,
        "best_std": 0.6922855330420096,
        "all_round_stats": {
          "1": {
            "mean": 0.14629629629629629,
            "std": 0.27767091961734053,
            "count": 27
          },
          "2": {
            "mean": 0.12407407407407409,
            "std": 0.43398209161261003,
            "count": 27
          },
          "3": {
            "mean": 0.17222222222222222,
            "std": 0.3160249908975473,
            "count": 27
          },
          "4": {
            "mean": 0.2814814814814815,
            "std": 0.6922855330420096,
            "count": 27
          },
          "5": {
            "mean": 0.11481481481481481,
            "std": 0.1633865254393908,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.14629629629629629,
          "2": 0.12407407407407409,
          "3": 0.17222222222222222,
          "4": 0.2814814814814815,
          "5": 0.11481481481481481
        }
      }
    },
    "fxx_gemini2.5-pro": {
      "gpt-4.1": {
        "best_round": "1",
        "best_score": 7.285185185185185,
        "best_std": 1.0048741611625207,
        "all_round_stats": {
          "1": {
            "mean": 7.285185185185185,
            "std": 1.0048741611625207,
            "count": 27
          },
          "2": {
            "mean": 7.025925925925926,
            "std": 1.3172940191026197,
            "count": 27
          },
          "3": {
            "mean": 7.088888888888889,
            "std": 1.2612061788783886,
            "count": 27
          },
          "4": {
            "mean": 7.166666666666667,
            "std": 1.0926326710501355,
            "count": 27
          },
          "5": {
            "mean": 7.257407407407407,
            "std": 1.2006171252657865,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.285185185185185,
          "2": 7.025925925925926,
          "3": 7.088888888888889,
          "4": 7.166666666666667,
          "5": 7.257407407407407
        }
      },
      "gpt-4.1-nano": {
        "best_round": "2",
        "best_score": 5.825925925925926,
        "best_std": 1.7370604242164167,
        "all_round_stats": {
          "1": {
            "mean": 5.785185185185185,
            "std": 1.8693795152616208,
            "count": 27
          },
          "2": {
            "mean": 5.825925925925926,
            "std": 1.7370604242164167,
            "count": 27
          },
          "3": {
            "mean": 5.646296296296296,
            "std": 1.7375892852502643,
            "count": 27
          },
          "4": {
            "mean": 5.785185185185185,
            "std": 1.80957999880629,
            "count": 27
          },
          "5": {
            "mean": 5.677777777777778,
            "std": 1.731680671693817,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.785185185185185,
          "2": 5.825925925925926,
          "3": 5.646296296296296,
          "4": 5.785185185185185,
          "5": 5.677777777777778
        }
      },
      "gpt-4o": {
        "best_round": "1",
        "best_score": 5.038888888888889,
        "best_std": 1.7412602273891111,
        "all_round_stats": {
          "1": {
            "mean": 5.038888888888889,
            "std": 1.7412602273891111,
            "count": 27
          },
          "2": {
            "mean": 5.037037037037037,
            "std": 1.6408014336687253,
            "count": 27
          },
          "3": {
            "mean": 4.942592592592592,
            "std": 1.543827891445338,
            "count": 27
          },
          "4": {
            "mean": 4.961111111111111,
            "std": 1.8593492106593337,
            "count": 27
          },
          "5": {
            "mean": 5.0092592592592595,
            "std": 1.8518720796616053,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.038888888888889,
          "2": 5.037037037037037,
          "3": 4.942592592592592,
          "4": 4.961111111111111,
          "5": 5.0092592592592595
        }
      },
      "gpt-4o-mini": {
        "best_round": "1",
        "best_score": 4.111111111111111,
        "best_std": 2.1904659380234666,
        "all_round_stats": {
          "1": {
            "mean": 4.111111111111111,
            "std": 2.1904659380234666,
            "count": 27
          },
          "2": {
            "mean": 4.103703703703704,
            "std": 2.1741013141563856,
            "count": 27
          },
          "3": {
            "mean": 3.9518518518518517,
            "std": 2.2060006224928284,
            "count": 27
          },
          "4": {
            "mean": 3.8796296296296298,
            "std": 2.020198926765308,
            "count": 27
          },
          "5": {
            "mean": 4.059259259259259,
            "std": 2.084052867198036,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.111111111111111,
          "2": 4.103703703703704,
          "3": 3.9518518518518517,
          "4": 3.8796296296296298,
          "5": 4.059259259259259
        }
      },
      "lightrag-4.1": {
        "best_round": "1",
        "best_score": 7.837037037037037,
        "best_std": 1.4134377481184899,
        "all_round_stats": {
          "1": {
            "mean": 7.837037037037037,
            "std": 1.4134377481184899,
            "count": 27
          },
          "2": {
            "mean": 7.161111111111111,
            "std": 1.746663486002193,
            "count": 27
          },
          "3": {
            "mean": 7.0,
            "std": 1.7415731173502083,
            "count": 27
          },
          "4": {
            "mean": 7.381481481481481,
            "std": 1.7420781217661228,
            "count": 27
          },
          "5": {
            "mean": 7.17037037037037,
            "std": 1.327175953442501,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.837037037037037,
          "2": 7.161111111111111,
          "3": 7.0,
          "4": 7.381481481481481,
          "5": 7.17037037037037
        }
      },
      "lightrag-4.1-nano": {
        "best_round": "2",
        "best_score": 7.809259259259259,
        "best_std": 1.0828236802486,
        "all_round_stats": {
          "1": {
            "mean": 6.451851851851852,
            "std": 1.8965799929124185,
            "count": 27
          },
          "2": {
            "mean": 7.809259259259259,
            "std": 1.0828236802486,
            "count": 27
          },
          "3": {
            "mean": 7.775,
            "std": 1.5927570436196476,
            "count": 27
          },
          "4": {
            "mean": 7.5962962962962965,
            "std": 1.5459578762003041,
            "count": 27
          },
          "5": {
            "mean": 7.627777777777778,
            "std": 1.317218320007774,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 6.451851851851852,
          "2": 7.809259259259259,
          "3": 7.775,
          "4": 7.5962962962962965,
          "5": 7.627777777777778
        }
      },
      "llasmol-top1": {
        "best_round": "5",
        "best_score": 0.42962962962962964,
        "best_std": 0.8562282292863163,
        "all_round_stats": {
          "1": {
            "mean": 0.40925925925925927,
            "std": 0.8782672010005135,
            "count": 27
          },
          "2": {
            "mean": 0.4078703703703704,
            "std": 0.8236815524567428,
            "count": 27
          },
          "3": {
            "mean": 0.4074074074074074,
            "std": 0.7891942950425043,
            "count": 27
          },
          "4": {
            "mean": 0.42777777777777776,
            "std": 0.7778586649026303,
            "count": 27
          },
          "5": {
            "mean": 0.42962962962962964,
            "std": 0.8562282292863163,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.40925925925925927,
          "2": 0.4078703703703704,
          "3": 0.4074074074074074,
          "4": 0.42777777777777776,
          "5": 0.42962962962962964
        }
      },
      "llasmol-top5": {
        "best_round": "5",
        "best_score": 0.8240740740740741,
        "best_std": 1.3367131308930853,
        "all_round_stats": {
          "1": {
            "mean": 0.5037037037037038,
            "std": 1.0000890273760945,
            "count": 27
          },
          "2": {
            "mean": 0.5277777777777778,
            "std": 1.1109016896659056,
            "count": 27
          },
          "3": {
            "mean": 0.575925925925926,
            "std": 1.228693049417392,
            "count": 27
          },
          "4": {
            "mean": 0.5444444444444444,
            "std": 1.0177589760514683,
            "count": 27
          },
          "5": {
            "mean": 0.8240740740740741,
            "std": 1.3367131308930853,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.5037037037037038,
          "2": 0.5277777777777778,
          "3": 0.575925925925926,
          "4": 0.5444444444444444,
          "5": 0.8240740740740741
        }
      },
      "MOSES": {
        "best_round": "1",
        "best_score": 9.231481481481481,
        "best_std": 0.9189404588718286,
        "all_round_stats": {
          "1": {
            "mean": 9.231481481481481,
            "std": 0.9189404588718286,
            "count": 27
          },
          "2": {
            "mean": 9.0,
            "std": 1.0572096800977124,
            "count": 27
          },
          "3": {
            "mean": 9.148148148148149,
            "std": 1.2018800562101317,
            "count": 27
          },
          "4": {
            "mean": 9.194444444444445,
            "std": 0.9795341645329066,
            "count": 27
          },
          "5": {
            "mean": 9.140277777777778,
            "std": 0.9260410808022144,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 9.231481481481481,
          "2": 9.0,
          "3": 9.148148148148149,
          "4": 9.194444444444445,
          "5": 9.140277777777778
        }
      },
      "MOSES-nano": {
        "best_round": "2",
        "best_score": 6.527777777777778,
        "best_std": 2.1372310147142284,
        "all_round_stats": {
          "1": {
            "mean": 5.937037037037037,
            "std": 2.4807176610731245,
            "count": 27
          },
          "2": {
            "mean": 6.527777777777778,
            "std": 2.1372310147142284,
            "count": 27
          },
          "3": {
            "mean": 6.090740740740741,
            "std": 2.33555449836111,
            "count": 27
          },
          "4": {
            "mean": 5.964814814814814,
            "std": 2.488118489588793,
            "count": 27
          },
          "5": {
            "mean": 6.262962962962963,
            "std": 2.062606295283889,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 5.937037037037037,
          "2": 6.527777777777778,
          "3": 6.090740740740741,
          "4": 5.964814814814814,
          "5": 6.262962962962963
        }
      },
      "o1": {
        "best_round": "4",
        "best_score": 7.546296296296297,
        "best_std": 1.2796428232072241,
        "all_round_stats": {
          "1": {
            "mean": 7.512962962962963,
            "std": 1.318915098494606,
            "count": 27
          },
          "2": {
            "mean": 7.405555555555556,
            "std": 1.3323153164938482,
            "count": 27
          },
          "3": {
            "mean": 7.4148148148148145,
            "std": 1.50255006410991,
            "count": 27
          },
          "4": {
            "mean": 7.546296296296297,
            "std": 1.2796428232072241,
            "count": 27
          },
          "5": {
            "mean": 7.507407407407407,
            "std": 1.516239256015185,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 7.512962962962963,
          "2": 7.405555555555556,
          "3": 7.4148148148148145,
          "4": 7.546296296296297,
          "5": 7.507407407407407
        }
      },
      "o3": {
        "best_round": "3",
        "best_score": 9.012962962962963,
        "best_std": 1.030738401546024,
        "all_round_stats": {
          "1": {
            "mean": 9.007407407407408,
            "std": 1.1976502397887694,
            "count": 27
          },
          "2": {
            "mean": 8.703703703703704,
            "std": 1.3587271989400966,
            "count": 27
          },
          "3": {
            "mean": 9.012962962962963,
            "std": 1.030738401546024,
            "count": 27
          },
          "4": {
            "mean": 8.898148148148149,
            "std": 1.1975521091506014,
            "count": 27
          },
          "5": {
            "mean": 8.951851851851853,
            "std": 1.1512937105609533,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 9.007407407407408,
          "2": 8.703703703703704,
          "3": 9.012962962962963,
          "4": 8.898148148148149,
          "5": 8.951851851851853
        }
      },
      "spark-chem13b-nothink": {
        "best_round": "3",
        "best_score": 4.8462962962962965,
        "best_std": 2.4394169356128907,
        "all_round_stats": {
          "1": {
            "mean": 4.4037037037037035,
            "std": 2.500343281274898,
            "count": 27
          },
          "2": {
            "mean": 4.425925925925926,
            "std": 2.781095881481669,
            "count": 27
          },
          "3": {
            "mean": 4.8462962962962965,
            "std": 2.4394169356128907,
            "count": 27
          },
          "4": {
            "mean": 4.383333333333334,
            "std": 2.646659723558677,
            "count": 27
          },
          "5": {
            "mean": 4.675925925925926,
            "std": 2.5751911941853343,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.4037037037037035,
          "2": 4.425925925925926,
          "3": 4.8462962962962965,
          "4": 4.383333333333334,
          "5": 4.675925925925926
        }
      },
      "spark-chem13b-think": {
        "best_round": "2",
        "best_score": 5.192592592592592,
        "best_std": 2.4059503041619266,
        "all_round_stats": {
          "1": {
            "mean": 4.955092592592592,
            "std": 2.6464831963200823,
            "count": 27
          },
          "2": {
            "mean": 5.192592592592592,
            "std": 2.4059503041619266,
            "count": 27
          },
          "3": {
            "mean": 4.786574074074074,
            "std": 2.3772381304609302,
            "count": 27
          },
          "4": {
            "mean": 4.642592592592592,
            "std": 2.5525139833755337,
            "count": 27
          },
          "5": {
            "mean": 4.677777777777778,
            "std": 2.5355295810560476,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 4.955092592592592,
          "2": 5.192592592592592,
          "3": 4.786574074074074,
          "4": 4.642592592592592,
          "5": 4.677777777777778
        }
      },
      "darwin": {
        "best_round": "4",
        "best_score": 0.45555555555555555,
        "best_std": 1.0938054015272811,
        "all_round_stats": {
          "1": {
            "mean": 0.23148148148148148,
            "std": 0.5645195622671868,
            "count": 27
          },
          "2": {
            "mean": 0.17777777777777776,
            "std": 0.8365834058070927,
            "count": 27
          },
          "3": {
            "mean": 0.3388888888888889,
            "std": 0.8417396515702709,
            "count": 27
          },
          "4": {
            "mean": 0.45555555555555555,
            "std": 1.0938054015272811,
            "count": 27
          },
          "5": {
            "mean": 0.15925925925925927,
            "std": 0.31714987873881584,
            "count": 27
          }
        },
        "all_round_scores": {
          "1": 0.23148148148148148,
          "2": 0.17777777777777776,
          "3": 0.3388888888888889,
          "4": 0.45555555555555555,
          "5": 0.15925925925925927
        }
      }
    }
  },
  "model_stability_no_lc": {
    "answer_round_volatility": {
      "Doubao-Seed-1.6-combined": {
        "gpt-4.1": {
          "round_scores": [
            6.468518518518518,
            6.425925925925926,
            6.453703703703703,
            6.57962962962963,
            6.4907407407407405
          ],
          "mean": 6.4837037037037035,
          "std": 0.058566553187492654,
          "coefficient_of_variation": 0.009032885502469449
        },
        "gpt-4.1-nano": {
          "round_scores": [
            5.805555555555555,
            5.857407407407408,
            5.82037037037037,
            5.7407407407407405,
            5.801851851851852
          ],
          "mean": 5.805185185185185,
          "std": 0.04219622331444791,
          "coefficient_of_variation": 0.00726871270569155
        },
        "gpt-4o": {
          "round_scores": [
            5.294444444444444,
            5.214814814814814,
            5.17962962962963,
            5.162962962962963,
            5.127777777777778
          ],
          "mean": 5.195925925925926,
          "std": 0.0633620251231695,
          "coefficient_of_variation": 0.012194558973024282
        },
        "gpt-4o-mini": {
          "round_scores": [
            4.468518518518518,
            4.4981481481481485,
            4.387037037037037,
            4.366666666666666,
            4.392592592592592
          ],
          "mean": 4.422592592592593,
          "std": 0.057248885074293654,
          "coefficient_of_variation": 0.012944643639610825
        },
        "lightrag-4.1": {
          "round_scores": [
            6.701851851851852,
            5.979629629629629,
            5.790740740740741,
            6.094444444444444,
            5.883333333333334
          ],
          "mean": 6.09,
          "std": 0.36014971738159257,
          "coefficient_of_variation": 0.05913788462751931
        },
        "lightrag-4.1-nano": {
          "round_scores": [
            5.67962962962963,
            6.487037037037037,
            6.531481481481482,
            6.555555555555555,
            6.5055555555555555
          ],
          "mean": 6.351851851851852,
          "std": 0.37667777397221613,
          "coefficient_of_variation": 0.0593020402172002
        },
        "llasmol-top1": {
          "round_scores": [
            0.5018518518518519,
            0.5388888888888889,
            0.5037037037037037,
            0.5166666666666667,
            0.5351851851851852
          ],
          "mean": 0.5192592592592593,
          "std": 0.017253058813114244,
          "coefficient_of_variation": 0.033226290153643694
        },
        "llasmol-top5": {
          "round_scores": [
            0.37222222222222223,
            0.5,
            0.5703703703703704,
            0.40555555555555556,
            0.6907407407407408
          ],
          "mean": 0.5077777777777778,
          "std": 0.12872302013318562,
          "coefficient_of_variation": 0.25350266547016864
        },
        "MOSES": {
          "round_scores": [
            8.366666666666667,
            8.314814814814815,
            8.337037037037037,
            8.375925925925927,
            8.327777777777778
          ],
          "mean": 8.344444444444445,
          "std": 0.025958973646067363,
          "coefficient_of_variation": 0.0031109289322850367
        },
        "MOSES-nano": {
          "round_scores": [
            6.029629629629629,
            5.885185185185185,
            5.781481481481482,
            5.812962962962963,
            6.1722222222222225
          ],
          "mean": 5.936296296296296,
          "std": 0.162913502595231,
          "coefficient_of_variation": 0.02744362721531842
        },
        "o1": {
          "round_scores": [
            6.3314814814814815,
            6.3,
            6.307407407407408,
            6.374074074074074,
            6.314814814814815
          ],
          "mean": 6.325555555555556,
          "std": 0.029525279210921195,
          "coefficient_of_variation": 0.00466761835408907
        },
        "o3": {
          "round_scores": [
            7.718518518518518,
            7.590740740740741,
            7.633333333333334,
            7.5814814814814815,
            7.651851851851852
          ],
          "mean": 7.635185185185185,
          "std": 0.054966007702428876,
          "coefficient_of_variation": 0.0071990405431267505
        },
        "spark-chem13b-nothink": {
          "round_scores": [
            4.324074074074074,
            4.838888888888889,
            4.905555555555556,
            4.590740740740741,
            4.82962962962963
          ],
          "mean": 4.697777777777778,
          "std": 0.24061392671330356,
          "coefficient_of_variation": 0.05121866935713652
        },
        "spark-chem13b-think": {
          "round_scores": [
            4.622222222222222,
            5.053703703703704,
            4.772222222222222,
            4.5777777777777775,
            5.046296296296297
          ],
          "mean": 4.814444444444445,
          "std": 0.22679477740071874,
          "coefficient_of_variation": 0.04710715431817375
        },
        "darwin": {
          "round_scores": [
            0.1462962962962963,
            0.12407407407407407,
            0.17222222222222222,
            0.2814814814814815,
            0.11481481481481481
          ],
          "mean": 0.16777777777777778,
          "std": 0.06731424168078391,
          "coefficient_of_variation": 0.40121071200467234
        }
      },
      "fxx_gemini2.5-pro": {
        "gpt-4.1": {
          "round_scores": [
            7.285185185185185,
            7.025925925925926,
            7.088888888888889,
            7.166666666666667,
            7.257407407407407
          ],
          "mean": 7.1648148148148145,
          "std": 0.10968216959230238,
          "coefficient_of_variation": 0.015308444450721968
        },
        "gpt-4.1-nano": {
          "round_scores": [
            5.785185185185185,
            5.825925925925926,
            5.646296296296296,
            5.785185185185185,
            5.677777777777778
          ],
          "mean": 5.744074074074074,
          "std": 0.07751720017990273,
          "coefficient_of_variation": 0.013495160260863844
        },
        "gpt-4o": {
          "round_scores": [
            5.038888888888889,
            5.037037037037037,
            4.942592592592592,
            4.961111111111111,
            5.0092592592592595
          ],
          "mean": 4.997777777777777,
          "std": 0.04402580612479778,
          "coefficient_of_variation": 0.008809076370012894
        },
        "gpt-4o-mini": {
          "round_scores": [
            4.111111111111111,
            4.103703703703704,
            3.9518518518518517,
            3.8796296296296298,
            4.059259259259259
          ],
          "mean": 4.021111111111111,
          "std": 0.10147742625041667,
          "coefficient_of_variation": 0.02523616568813899
        },
        "lightrag-4.1": {
          "round_scores": [
            7.837037037037037,
            7.161111111111111,
            7.0,
            7.381481481481481,
            7.17037037037037
          ],
          "mean": 7.31,
          "std": 0.32428458771141205,
          "coefficient_of_variation": 0.044361776704707534
        },
        "lightrag-4.1-nano": {
          "round_scores": [
            6.451851851851852,
            7.809259259259259,
            7.775,
            7.5962962962962965,
            7.627777777777778
          ],
          "mean": 7.452037037037037,
          "std": 0.5665639279898733,
          "coefficient_of_variation": 0.0760280612098436
        },
        "llasmol-top1": {
          "round_scores": [
            0.40925925925925927,
            0.4078703703703704,
            0.4074074074074074,
            0.42777777777777776,
            0.42962962962962964
          ],
          "mean": 0.41638888888888886,
          "std": 0.01128148695313697,
          "coefficient_of_variation": 0.027093631108267575
        },
        "llasmol-top5": {
          "round_scores": [
            0.5037037037037037,
            0.5277777777777778,
            0.575925925925926,
            0.5444444444444444,
            0.8240740740740741
          ],
          "mean": 0.5951851851851852,
          "std": 0.1306218639071953,
          "coefficient_of_variation": 0.21946423929647002
        },
        "MOSES": {
          "round_scores": [
            9.231481481481481,
            9.0,
            9.148148148148149,
            9.194444444444445,
            9.140277777777778
          ],
          "mean": 9.142870370370371,
          "std": 0.08797490172391963,
          "coefficient_of_variation": 0.009622240954987512
        },
        "MOSES-nano": {
          "round_scores": [
            5.937037037037037,
            6.527777777777778,
            6.090740740740741,
            5.964814814814814,
            6.262962962962963
          ],
          "mean": 6.156666666666666,
          "std": 0.24413771440308926,
          "coefficient_of_variation": 0.039654203747117915
        },
        "o1": {
          "round_scores": [
            7.512962962962963,
            7.405555555555556,
            7.4148148148148145,
            7.546296296296297,
            7.507407407407407
          ],
          "mean": 7.477407407407408,
          "std": 0.06322657235137104,
          "coefficient_of_variation": 0.008455681081217584
        },
        "o3": {
          "round_scores": [
            9.007407407407408,
            8.703703703703704,
            9.012962962962963,
            8.898148148148149,
            8.951851851851853
          ],
          "mean": 8.914814814814815,
          "std": 0.1269092868082918,
          "coefficient_of_variation": 0.01423577375913535
        },
        "spark-chem13b-nothink": {
          "round_scores": [
            4.4037037037037035,
            4.425925925925926,
            4.8462962962962965,
            4.383333333333334,
            4.675925925925926
          ],
          "mean": 4.547037037037037,
          "std": 0.20504859071494208,
          "coefficient_of_variation": 0.04509499021995143
        },
        "spark-chem13b-think": {
          "round_scores": [
            4.955092592592592,
            5.192592592592592,
            4.786574074074074,
            4.642592592592592,
            4.677777777777778
          ],
          "mean": 4.850925925925925,
          "std": 0.22644592366458532,
          "coefficient_of_variation": 0.04668096918452991
        },
        "darwin": {
          "round_scores": [
            0.23148148148148148,
            0.17777777777777778,
            0.3388888888888889,
            0.45555555555555555,
            0.15925925925925927
          ],
          "mean": 0.2725925925925926,
          "std": 0.1238901345228859,
          "coefficient_of_variation": 0.4544882652334129
        }
      }
    },
    "question_volatility": {
      "Doubao-Seed-1.6-combined": {
        "gpt-4.1": {
          "question_scores": {
            "q_1": 5.4,
            "q_2": 6.79,
            "q_3": 6.88,
            "q_4": 6.67,
            "q_5": 3.0300000000000002,
            "q_6": 6.83,
            "q_7": 4.91,
            "q_8": 7.24,
            "q_9": 6.32,
            "q_10": 6.37,
            "q_11": 6.72,
            "q_12": 9.14,
            "q_13": 6.98,
            "q_14": 6.29,
            "q_15": 6.4799999999999995,
            "q_16": 7.2700000000000005,
            "q_17": 6.46,
            "q_18": 6.23,
            "q_19": 6.1000000000000005,
            "q_20": 7.12,
            "q_21": 6.71,
            "q_22": 6.64,
            "q_23": 7.04,
            "q_24": 6.91,
            "q_25": 6.27,
            "q_26": 5.46,
            "q_27": 6.8
          },
          "mean": 6.4837037037037035,
          "std": 1.018815859213899,
          "coefficient_of_variation": 0.15713485775605662
        },
        "gpt-4.1-nano": {
          "question_scores": {
            "q_1": 5.03,
            "q_2": 6.18,
            "q_3": 6.13,
            "q_4": 6.26,
            "q_5": 5.71,
            "q_6": 5.95,
            "q_7": 4.88,
            "q_8": 6.49,
            "q_9": 6.51,
            "q_10": 6.34,
            "q_11": 6.1499999999999995,
            "q_12": 8.200000000000001,
            "q_13": 6.62,
            "q_14": 4.98,
            "q_15": 4.88,
            "q_16": 6.67,
            "q_17": 6.35,
            "q_18": 5.71,
            "q_19": 6.55,
            "q_20": 7.42,
            "q_21": 1.52,
            "q_22": 5.38,
            "q_23": 6.25,
            "q_24": 5.64,
            "q_25": 4.67,
            "q_26": 3.73,
            "q_27": 6.54
          },
          "mean": 5.805185185185185,
          "std": 1.2460132434484807,
          "coefficient_of_variation": 0.2146379837508548
        },
        "gpt-4o": {
          "question_scores": {
            "q_1": 5.0,
            "q_2": 6.15,
            "q_3": 5.53,
            "q_4": 6.67,
            "q_5": 2.03,
            "q_6": 6.03,
            "q_7": 3.9,
            "q_8": 5.68,
            "q_9": 5.93,
            "q_10": 5.1499999999999995,
            "q_11": 5.59,
            "q_12": 8.53,
            "q_13": 6.67,
            "q_14": 6.04,
            "q_15": 3.75,
            "q_16": 4.89,
            "q_17": 5.72,
            "q_18": 5.3100000000000005,
            "q_19": 5.44,
            "q_20": 5.83,
            "q_21": 1.48,
            "q_22": 3.89,
            "q_23": 5.93,
            "q_24": 4.84,
            "q_25": 4.04,
            "q_26": 4.1,
            "q_27": 6.17
          },
          "mean": 5.195925925925926,
          "std": 1.4299280758116129,
          "coefficient_of_variation": 0.27520178235735654
        },
        "gpt-4o-mini": {
          "question_scores": {
            "q_1": 5.28,
            "q_2": 4.83,
            "q_3": 5.95,
            "q_4": 3.04,
            "q_5": 4.02,
            "q_6": 4.29,
            "q_7": 4.11,
            "q_8": 5.43,
            "q_9": 5.5,
            "q_10": 5.52,
            "q_11": 4.92,
            "q_12": 8.21,
            "q_13": 6.66,
            "q_14": 5.87,
            "q_15": 2.82,
            "q_16": 4.02,
            "q_17": 5.22,
            "q_18": 2.34,
            "q_19": 3.37,
            "q_20": 5.86,
            "q_21": 0.99,
            "q_22": 2.78,
            "q_23": 4.07,
            "q_24": 4.66,
            "q_25": 2.77,
            "q_26": 1.72,
            "q_27": 5.16
          },
          "mean": 4.422592592592593,
          "std": 1.6006455250510956,
          "coefficient_of_variation": 0.3619247062756853
        },
        "lightrag-4.1": {
          "question_scores": {
            "q_1": 6.08,
            "q_2": 7.35,
            "q_3": 5.76,
            "q_4": 7.33,
            "q_5": 7.36,
            "q_6": 5.22,
            "q_7": 5.53,
            "q_8": 6.09,
            "q_9": 6.83,
            "q_10": 5.86,
            "q_11": 7.32,
            "q_12": 8.79,
            "q_13": 5.82,
            "q_14": 5.91,
            "q_15": 6.63,
            "q_16": 4.18,
            "q_17": 6.7700000000000005,
            "q_18": 5.8999999999999995,
            "q_19": 7.12,
            "q_20": 6.44,
            "q_21": 5.0200000000000005,
            "q_22": 5.22,
            "q_23": 5.27,
            "q_24": 4.4799999999999995,
            "q_25": 4.26,
            "q_26": 5.23,
            "q_27": 6.66
          },
          "mean": 6.09,
          "std": 1.0853499967220783,
          "coefficient_of_variation": 0.1782183902663511
        },
        "lightrag-4.1-nano": {
          "question_scores": {
            "q_1": 5.72,
            "q_2": 7.32,
            "q_3": 6.15,
            "q_4": 7.36,
            "q_5": 7.8,
            "q_6": 5.11,
            "q_7": 6.67,
            "q_8": 6.8999999999999995,
            "q_9": 6.08,
            "q_10": 6.3100000000000005,
            "q_11": 5.96,
            "q_12": 8.209999999999999,
            "q_13": 6.86,
            "q_14": 6.3100000000000005,
            "q_15": 6.23,
            "q_16": 4.12,
            "q_17": 6.7,
            "q_18": 4.71,
            "q_19": 6.93,
            "q_20": 7.21,
            "q_21": 6.61,
            "q_22": 6.38,
            "q_23": 6.36,
            "q_24": 5.9799999999999995,
            "q_25": 5.22,
            "q_26": 4.84,
            "q_27": 7.45
          },
          "mean": 6.351851851851852,
          "std": 0.9655330812841841,
          "coefficient_of_variation": 0.15200812358409896
        },
        "llasmol-top1": {
          "question_scores": {
            "q_1": 1.63,
            "q_2": 2.58,
            "q_3": 2.25,
            "q_4": 2.71,
            "q_5": 0.0,
            "q_6": 0.0,
            "q_7": 0.0,
            "q_8": 0.0,
            "q_9": 0.11,
            "q_10": 0.0,
            "q_11": 0.0,
            "q_12": 0.0,
            "q_13": 0.27,
            "q_14": 0.0,
            "q_15": 0.0,
            "q_16": 0.0,
            "q_17": 0.0,
            "q_18": 2.75,
            "q_19": 0.0,
            "q_20": 1.72,
            "q_21": 0.0,
            "q_22": 0.0,
            "q_23": 0.0,
            "q_24": 0.0,
            "q_25": 0.0,
            "q_26": 0.0,
            "q_27": 0.0
          },
          "mean": 0.5192592592592593,
          "std": 0.9814781863085038,
          "coefficient_of_variation": 1.890150572776719
        },
        "llasmol-top5": {
          "question_scores": {
            "q_1": 2.72,
            "q_2": 0.19,
            "q_3": 1.39,
            "q_4": 2.84,
            "q_5": 0.0,
            "q_6": 0.0,
            "q_7": 0.0,
            "q_8": 0.0,
            "q_9": 1.1400000000000001,
            "q_10": 0.02,
            "q_11": 0.0,
            "q_12": 0.0,
            "q_13": 0.1,
            "q_14": 0.05,
            "q_15": 0.47,
            "q_16": 0.0,
            "q_17": 0.56,
            "q_18": 1.0,
            "q_19": 0.0,
            "q_20": 2.91,
            "q_21": 0.03,
            "q_22": 0.0,
            "q_23": 0.0,
            "q_24": 0.0,
            "q_25": 0.0,
            "q_26": 0.02,
            "q_27": 0.27
          },
          "mean": 0.5077777777777778,
          "std": 0.916931896360958,
          "coefficient_of_variation": 1.805773975327926
        },
        "MOSES": {
          "question_scores": {
            "q_1": 8.75,
            "q_2": 8.62,
            "q_3": 8.31,
            "q_4": 7.619999999999999,
            "q_5": 8.1,
            "q_6": 8.25,
            "q_7": 8.98,
            "q_8": 8.3,
            "q_9": 7.720000000000001,
            "q_10": 9.02,
            "q_11": 8.46,
            "q_12": 8.72,
            "q_13": 8.32,
            "q_14": 8.74,
            "q_15": 8.82,
            "q_16": 6.95,
            "q_17": 8.19,
            "q_18": 7.82,
            "q_19": 8.22,
            "q_20": 8.6,
            "q_21": 8.86,
            "q_22": 8.32,
            "q_23": 8.6,
            "q_24": 8.76,
            "q_25": 7.970000000000001,
            "q_26": 7.97,
            "q_27": 8.31
          },
          "mean": 8.344444444444445,
          "std": 0.46965890514232467,
          "coefficient_of_variation": 0.05628402325274197
        },
        "MOSES-nano": {
          "question_scores": {
            "q_1": 6.36,
            "q_2": 5.859999999999999,
            "q_3": 5.8,
            "q_4": 6.88,
            "q_5": 6.74,
            "q_6": 6.16,
            "q_7": 6.1899999999999995,
            "q_8": 5.55,
            "q_9": 5.08,
            "q_10": 7.64,
            "q_11": 5.78,
            "q_12": 5.91,
            "q_13": 5.800000000000001,
            "q_14": 5.7,
            "q_15": 5.0600000000000005,
            "q_16": 5.48,
            "q_17": 5.62,
            "q_18": 5.65,
            "q_19": 6.48,
            "q_20": 5.52,
            "q_21": 6.65,
            "q_22": 5.85,
            "q_23": 4.67,
            "q_24": 6.029999999999999,
            "q_25": 5.5600000000000005,
            "q_26": 5.76,
            "q_27": 6.5
          },
          "mean": 5.936296296296296,
          "std": 0.6197403187449261,
          "coefficient_of_variation": 0.10439848144567634
        },
        "o1": {
          "question_scores": {
            "q_1": 5.74,
            "q_2": 6.75,
            "q_3": 6.7299999999999995,
            "q_4": 6.17,
            "q_5": 3.01,
            "q_6": 6.91,
            "q_7": 5.76,
            "q_8": 6.85,
            "q_9": 5.9,
            "q_10": 5.359999999999999,
            "q_11": 5.91,
            "q_12": 8.19,
            "q_13": 6.6499999999999995,
            "q_14": 6.95,
            "q_15": 5.31,
            "q_16": 7.74,
            "q_17": 6.87,
            "q_18": 6.63,
            "q_19": 6.21,
            "q_20": 8.15,
            "q_21": 2.29,
            "q_22": 6.32,
            "q_23": 7.1,
            "q_24": 7.23,
            "q_25": 6.02,
            "q_26": 6.55,
            "q_27": 7.49
          },
          "mean": 6.325555555555556,
          "std": 1.298873278989061,
          "coefficient_of_variation": 0.20533742334272875
        },
        "o3": {
          "question_scores": {
            "q_1": 7.43,
            "q_2": 8.27,
            "q_3": 7.7,
            "q_4": 6.2700000000000005,
            "q_5": 5.279999999999999,
            "q_6": 5.819999999999999,
            "q_7": 7.56,
            "q_8": 8.27,
            "q_9": 9.01,
            "q_10": 7.39,
            "q_11": 6.72,
            "q_12": 9.51,
            "q_13": 7.62,
            "q_14": 6.83,
            "q_15": 7.32,
            "q_16": 7.86,
            "q_17": 8.14,
            "q_18": 7.47,
            "q_19": 9.27,
            "q_20": 9.76,
            "q_21": 7.43,
            "q_22": 6.89,
            "q_23": 7.529999999999999,
            "q_24": 7.39,
            "q_25": 7.25,
            "q_26": 7.3999999999999995,
            "q_27": 8.76
          },
          "mean": 7.635185185185185,
          "std": 1.042771345872181,
          "coefficient_of_variation": 0.13657446683749158
        },
        "spark-chem13b-nothink": {
          "question_scores": {
            "q_1": 2.35,
            "q_2": 7.03,
            "q_3": 1.54,
            "q_4": 2.86,
            "q_5": 4.88,
            "q_6": 6.73,
            "q_7": 6.1,
            "q_8": 6.51,
            "q_9": 5.91,
            "q_10": 6.59,
            "q_11": 4.75,
            "q_12": 4.91,
            "q_13": 2.0300000000000002,
            "q_14": 6.63,
            "q_15": 5.57,
            "q_16": 1.99,
            "q_17": 6.8,
            "q_18": 7.22,
            "q_19": 2.9899999999999998,
            "q_20": 5.78,
            "q_21": 5.53,
            "q_22": 3.31,
            "q_23": 2.69,
            "q_24": 3.96,
            "q_25": 4.3100000000000005,
            "q_26": 2.7399999999999998,
            "q_27": 5.13
          },
          "mean": 4.697777777777778,
          "std": 1.7973021092167203,
          "coefficient_of_variation": 0.3825855956232375
        },
        "spark-chem13b-think": {
          "question_scores": {
            "q_1": 2.17,
            "q_2": 6.8,
            "q_3": 1.85,
            "q_4": 6.6499999999999995,
            "q_5": 5.13,
            "q_6": 6.64,
            "q_7": 6.24,
            "q_8": 6.22,
            "q_9": 5.35,
            "q_10": 6.2,
            "q_11": 3.5,
            "q_12": 5.49,
            "q_13": 1.52,
            "q_14": 7.2,
            "q_15": 5.42,
            "q_16": 1.94,
            "q_17": 6.31,
            "q_18": 5.13,
            "q_19": 3.08,
            "q_20": 5.2,
            "q_21": 5.54,
            "q_22": 4.02,
            "q_23": 3.87,
            "q_24": 5.63,
            "q_25": 4.37,
            "q_26": 3.4099999999999997,
            "q_27": 5.109999999999999
          },
          "mean": 4.814444444444445,
          "std": 1.6473996603816699,
          "coefficient_of_variation": 0.3421785585837763
        },
        "darwin": {
          "question_scores": {
            "q_1": 0.0,
            "q_2": 0.06,
            "q_3": 0.0,
            "q_4": 0.02,
            "q_5": 0.12,
            "q_6": 0.06,
            "q_7": 0.56,
            "q_8": 0.11,
            "q_9": 0.2,
            "q_10": 0.13,
            "q_11": 0.11000000000000001,
            "q_12": 0.24,
            "q_13": 0.17,
            "q_14": 0.07,
            "q_15": 0.09,
            "q_16": 0.0,
            "q_17": 0.0,
            "q_18": 0.13,
            "q_19": 0.01,
            "q_20": 0.22999999999999998,
            "q_21": 0.88,
            "q_22": 0.54,
            "q_23": 0.05,
            "q_24": 0.11000000000000001,
            "q_25": 0.22,
            "q_26": 0.02,
            "q_27": 0.4
          },
          "mean": 0.16777777777777778,
          "std": 0.2071293414815103,
          "coefficient_of_variation": 1.2345457439295315
        }
      },
      "fxx_gemini2.5-pro": {
        "gpt-4.1": {
          "question_scores": {
            "q_1": 4.5,
            "q_2": 7.06,
            "q_3": 7.56,
            "q_4": 7.5,
            "q_5": 4.45,
            "q_6": 7.55,
            "q_7": 7.55,
            "q_8": 8.23,
            "q_9": 5.91,
            "q_10": 7.97,
            "q_11": 7.0,
            "q_12": 9.24,
            "q_13": 7.59,
            "q_14": 5.97,
            "q_15": 7.08,
            "q_16": 8.25,
            "q_17": 7.46,
            "q_18": 7.61,
            "q_19": 6.49,
            "q_20": 7.07,
            "q_21": 7.78,
            "q_22": 7.51,
            "q_23": 7.72,
            "q_24": 7.94,
            "q_25": 7.97,
            "q_26": 5.85,
            "q_27": 6.64
          },
          "mean": 7.1648148148148145,
          "std": 1.0802576984945282,
          "coefficient_of_variation": 0.15077259167408769
        },
        "gpt-4.1-nano": {
          "question_scores": {
            "q_1": 3.97,
            "q_2": 6.640000000000001,
            "q_3": 6.41,
            "q_4": 6.97,
            "q_5": 6.91,
            "q_6": 7.0,
            "q_7": 4.81,
            "q_8": 7.28,
            "q_9": 6.04,
            "q_10": 6.39,
            "q_11": 6.91,
            "q_12": 9.27,
            "q_13": 6.97,
            "q_14": 2.09,
            "q_15": 4.3999999999999995,
            "q_16": 5.59,
            "q_17": 6.319999999999999,
            "q_18": 5.11,
            "q_19": 6.0200000000000005,
            "q_20": 7.0,
            "q_21": 1.69,
            "q_22": 3.67,
            "q_23": 6.5,
            "q_24": 6.109999999999999,
            "q_25": 5.3,
            "q_26": 3.28,
            "q_27": 6.44
          },
          "mean": 5.744074074074074,
          "std": 1.6828255983024487,
          "coefficient_of_variation": 0.29296725226749704
        },
        "gpt-4o": {
          "question_scores": {
            "q_1": 4.1,
            "q_2": 6.9399999999999995,
            "q_3": 5.47,
            "q_4": 5.74,
            "q_5": 1.28,
            "q_6": 6.58,
            "q_7": 3.28,
            "q_8": 4.71,
            "q_9": 5.64,
            "q_10": 5.5,
            "q_11": 5.93,
            "q_12": 8.65,
            "q_13": 7.68,
            "q_14": 5.42,
            "q_15": 3.0100000000000002,
            "q_16": 4.53,
            "q_17": 4.88,
            "q_18": 5.5,
            "q_19": 4.2,
            "q_20": 5.54,
            "q_21": 1.5999999999999999,
            "q_22": 2.71,
            "q_23": 5.19,
            "q_24": 4.83,
            "q_25": 5.25,
            "q_26": 4.5,
            "q_27": 6.279999999999999
          },
          "mean": 4.997777777777777,
          "std": 1.6539238502918152,
          "coefficient_of_variation": 0.3309318508809768
        },
        "gpt-4o-mini": {
          "question_scores": {
            "q_1": 4.12,
            "q_2": 5.03,
            "q_3": 6.49,
            "q_4": 4.17,
            "q_5": 4.86,
            "q_6": 5.51,
            "q_7": 3.17,
            "q_8": 4.04,
            "q_9": 4.57,
            "q_10": 6.19,
            "q_11": 5.05,
            "q_12": 8.92,
            "q_13": 7.4799999999999995,
            "q_14": 3.66,
            "q_15": 2.25,
            "q_16": 2.88,
            "q_17": 5.38,
            "q_18": 1.07,
            "q_19": 1.72,
            "q_20": 4.14,
            "q_21": 0.37,
            "q_22": 1.09,
            "q_23": 3.54,
            "q_24": 4.96,
            "q_25": 2.38,
            "q_26": 0.77,
            "q_27": 4.76
          },
          "mean": 4.021111111111111,
          "std": 2.057777616252232,
          "coefficient_of_variation": 0.5117435354039815
        },
        "lightrag-4.1": {
          "question_scores": {
            "q_1": 8.05,
            "q_2": 8.14,
            "q_3": 7.82,
            "q_4": 8.46,
            "q_5": 9.11,
            "q_6": 7.09,
            "q_7": 6.7,
            "q_8": 7.4,
            "q_9": 7.95,
            "q_10": 8.91,
            "q_11": 8.5,
            "q_12": 8.84,
            "q_13": 7.680000000000001,
            "q_14": 6.82,
            "q_15": 8.05,
            "q_16": 3.57,
            "q_17": 7.7,
            "q_18": 6.76,
            "q_19": 8.14,
            "q_20": 8.31,
            "q_21": 6.11,
            "q_22": 6.83,
            "q_23": 6.49,
            "q_24": 6.24,
            "q_25": 5.99,
            "q_26": 5.17,
            "q_27": 6.54
          },
          "mean": 7.3100000000000005,
          "std": 1.250732093308063,
          "coefficient_of_variation": 0.17109878157429043
        },
        "lightrag-4.1-nano": {
          "question_scores": {
            "q_1": 7.79,
            "q_2": 8.44,
            "q_3": 8.45,
            "q_4": 9.19,
            "q_5": 9.52,
            "q_6": 6.59,
            "q_7": 7.08,
            "q_8": 8.43,
            "q_9": 7.075,
            "q_10": 8.77,
            "q_11": 6.74,
            "q_12": 9.059999999999999,
            "q_13": 7.99,
            "q_14": 6.67,
            "q_15": 7.47,
            "q_16": 3.56,
            "q_17": 7.74,
            "q_18": 5.05,
            "q_19": 8.21,
            "q_20": 7.96,
            "q_21": 7.83,
            "q_22": 7.66,
            "q_23": 7.75,
            "q_24": 6.7,
            "q_25": 6.12,
            "q_26": 5.5600000000000005,
            "q_27": 7.8
          },
          "mean": 7.452037037037037,
          "std": 1.3191345205061091,
          "coefficient_of_variation": 0.1770166350421955
        },
        "llasmol-top1": {
          "question_scores": {
            "q_1": 2.51,
            "q_2": 0.78,
            "q_3": 1.2,
            "q_4": 3.12,
            "q_5": 0.1,
            "q_6": 0.0,
            "q_7": 0.35,
            "q_8": 0.1,
            "q_9": 0.25,
            "q_10": 0.03,
            "q_11": 0.0,
            "q_12": 0.01,
            "q_13": 0.1225,
            "q_14": 0.0,
            "q_15": 0.0,
            "q_16": 0.0,
            "q_17": 0.0,
            "q_18": 1.24,
            "q_19": 0.0,
            "q_20": 1.43,
            "q_21": 0.0,
            "q_22": 0.0,
            "q_23": 0.0,
            "q_24": 0.0,
            "q_25": 0.0,
            "q_26": 0.0,
            "q_27": 0.0
          },
          "mean": 0.41638888888888886,
          "std": 0.8134720024179989,
          "coefficient_of_variation": 1.9536352292893904
        },
        "llasmol-top5": {
          "question_scores": {
            "q_1": 2.3200000000000003,
            "q_2": 0.06999999999999999,
            "q_3": 0.89,
            "q_4": 2.92,
            "q_5": 0.0,
            "q_6": 0.0,
            "q_7": 0.1,
            "q_8": 0.22000000000000003,
            "q_9": 1.66,
            "q_10": 0.02,
            "q_11": 0.12,
            "q_12": 0.0,
            "q_13": 0.53,
            "q_14": 0.08,
            "q_15": 0.85,
            "q_16": 0.46,
            "q_17": 0.72,
            "q_18": 0.62,
            "q_19": 0.0,
            "q_20": 2.95,
            "q_21": 0.52,
            "q_22": 0.0,
            "q_23": 0.0,
            "q_24": 0.0,
            "q_25": 0.01,
            "q_26": 0.32,
            "q_27": 0.6900000000000001
          },
          "mean": 0.5951851851851852,
          "std": 0.869254191248365,
          "coefficient_of_variation": 1.4604768614627166
        },
        "MOSES": {
          "question_scores": {
            "q_1": 9.89,
            "q_2": 9.4875,
            "q_3": 9.78,
            "q_4": 9.64,
            "q_5": 9.36,
            "q_6": 9.65,
            "q_7": 9.77,
            "q_8": 9.09,
            "q_9": 8.49,
            "q_10": 9.54,
            "q_11": 9.68,
            "q_12": 9.4,
            "q_13": 9.120000000000001,
            "q_14": 9.459999999999999,
            "q_15": 9.55,
            "q_16": 7.97,
            "q_17": 8.64,
            "q_18": 7.57,
            "q_19": 9.11,
            "q_20": 9.370000000000001,
            "q_21": 9.49,
            "q_22": 8.959999999999999,
            "q_23": 9.34,
            "q_24": 9.22,
            "q_25": 8.52,
            "q_26": 8.309999999999999,
            "q_27": 8.450000000000001
          },
          "mean": 9.142870370370371,
          "std": 0.5932775260655615,
          "coefficient_of_variation": 0.06488963553373975
        },
        "MOSES-nano": {
          "question_scores": {
            "q_1": 7.87,
            "q_2": 6.48,
            "q_3": 7.34,
            "q_4": 7.720000000000001,
            "q_5": 6.31,
            "q_6": 7.25,
            "q_7": 7.5,
            "q_8": 7.43,
            "q_9": 7.94,
            "q_10": 7.99,
            "q_11": 6.14,
            "q_12": 5.07,
            "q_13": 6.21,
            "q_14": 5.59,
            "q_15": 3.33,
            "q_16": 4.22,
            "q_17": 5.62,
            "q_18": 6.2700000000000005,
            "q_19": 6.13,
            "q_20": 5.16,
            "q_21": 4.72,
            "q_22": 5.64,
            "q_23": 5.03,
            "q_24": 6.12,
            "q_25": 5.49,
            "q_26": 5.4,
            "q_27": 6.26
          },
          "mean": 6.156666666666666,
          "std": 1.1968002211024067,
          "coefficient_of_variation": 0.1943909400816037
        },
        "o1": {
          "question_scores": {
            "q_1": 4.2700000000000005,
            "q_2": 7.15,
            "q_3": 8.07,
            "q_4": 7.72,
            "q_5": 4.62,
            "q_6": 7.8,
            "q_7": 7.86,
            "q_8": 8.04,
            "q_9": 6.8,
            "q_10": 7.05,
            "q_11": 7.83,
            "q_12": 9.32,
            "q_13": 7.33,
            "q_14": 7.68,
            "q_15": 6.38,
            "q_16": 9.29,
            "q_17": 7.98,
            "q_18": 8.040000000000001,
            "q_19": 7.5,
            "q_20": 8.84,
            "q_21": 4.39,
            "q_22": 7.44,
            "q_23": 8.36,
            "q_24": 8.32,
            "q_25": 7.840000000000001,
            "q_26": 7.63,
            "q_27": 8.34
          },
          "mean": 7.477407407407408,
          "std": 1.2799867565198073,
          "coefficient_of_variation": 0.17118055587713504
        },
        "o3": {
          "question_scores": {
            "q_1": 7.52,
            "q_2": 9.62,
            "q_3": 9.61,
            "q_4": 7.94,
            "q_5": 5.13,
            "q_6": 6.82,
            "q_7": 9.65,
            "q_8": 9.74,
            "q_9": 9.46,
            "q_10": 9.47,
            "q_11": 7.970000000000001,
            "q_12": 9.51,
            "q_13": 8.91,
            "q_14": 7.69,
            "q_15": 8.74,
            "q_16": 9.77,
            "q_17": 9.75,
            "q_18": 9.1,
            "q_19": 9.26,
            "q_20": 9.94,
            "q_21": 9.56,
            "q_22": 9.24,
            "q_23": 9.53,
            "q_24": 9.25,
            "q_25": 8.76,
            "q_26": 9.15,
            "q_27": 9.61
          },
          "mean": 8.914814814814815,
          "std": 1.0989973415323426,
          "coefficient_of_variation": 0.12327764113574263
        },
        "spark-chem13b-nothink": {
          "question_scores": {
            "q_1": 4.35,
            "q_2": 7.83,
            "q_3": 0.5700000000000001,
            "q_4": 2.71,
            "q_5": 4.98,
            "q_6": 7.65,
            "q_7": 6.34,
            "q_8": 3.44,
            "q_9": 8.01,
            "q_10": 5.91,
            "q_11": 3.3200000000000003,
            "q_12": 2.31,
            "q_13": 3.25,
            "q_14": 8.18,
            "q_15": 5.21,
            "q_16": 0.5800000000000001,
            "q_17": 6.87,
            "q_18": 8.25,
            "q_19": 1.27,
            "q_20": 3.38,
            "q_21": 6.71,
            "q_22": 4.0200000000000005,
            "q_23": 3.29,
            "q_24": 3.35,
            "q_25": 4.3,
            "q_26": 2.06,
            "q_27": 4.63
          },
          "mean": 4.547037037037037,
          "std": 2.3296843873643795,
          "coefficient_of_variation": 0.5123521907537529
        },
        "spark-chem13b-think": {
          "question_scores": {
            "q_1": 4.08,
            "q_2": 7.71,
            "q_3": 0.77,
            "q_4": 4.890000000000001,
            "q_5": 5.8,
            "q_6": 7.5200000000000005,
            "q_7": 6.32,
            "q_8": 4.31,
            "q_9": 7.35,
            "q_10": 5.26,
            "q_11": 2.84,
            "q_12": 2.94,
            "q_13": 3.0300000000000002,
            "q_14": 8.86,
            "q_15": 5.109999999999999,
            "q_16": 0.65,
            "q_17": 6.66,
            "q_18": 7.78,
            "q_19": 1.44,
            "q_20": 4.15,
            "q_21": 6.7,
            "q_22": 5.26,
            "q_23": 3.6675,
            "q_24": 5.45,
            "q_25": 4.46,
            "q_26": 2.46,
            "q_27": 5.5075
          },
          "mean": 4.850925925925926,
          "std": 2.1726914304599956,
          "coefficient_of_variation": 0.44789210629830023
        },
        "darwin": {
          "question_scores": {
            "q_1": 0.0,
            "q_2": 0.02,
            "q_3": 0.0,
            "q_4": 0.0,
            "q_5": 0.06999999999999999,
            "q_6": 0.04,
            "q_7": 0.75,
            "q_8": 0.06999999999999999,
            "q_9": 0.07,
            "q_10": 0.18,
            "q_11": 0.32,
            "q_12": 0.14,
            "q_13": 0.49,
            "q_14": 0.01,
            "q_15": 0.05,
            "q_16": 0.0,
            "q_17": 0.0,
            "q_18": 0.18,
            "q_19": 0.0,
            "q_20": 0.52,
            "q_21": 1.68,
            "q_22": 0.6900000000000001,
            "q_23": 0.12,
            "q_24": 0.26,
            "q_25": 0.74,
            "q_26": 0.02,
            "q_27": 0.94
          },
          "mean": 0.2725925925925926,
          "std": 0.39637387743423597,
          "coefficient_of_variation": 1.454088952544072
        }
      }
    }
  }
}