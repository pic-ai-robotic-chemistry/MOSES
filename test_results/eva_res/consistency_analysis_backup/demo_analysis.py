#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€è‡´æ€§åˆ†æç®€åŒ–ç‰ˆæœ¬ - é‡ç‚¹å®ç°æ ¸å¿ƒåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
import pingouin as pg
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set up matplotlib for Chinese characters
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# Set up paths
BASE_DIR = Path(r"C:\D\CursorProj\Chem-Ontology-Constructor\test_results\eva_res")
OUTPUT_DIR = Path(r"C:\D\CursorProj\Chem-Ontology-Constructor\test_results\eva_res\consistency_analysis")

# Create output directories
(OUTPUT_DIR / "results").mkdir(exist_ok=True)
(OUTPUT_DIR / "figures").mkdir(exist_ok=True)
(OUTPUT_DIR / "data").mkdir(exist_ok=True)

def load_sample_data():
    """Load and create sample data to demonstrate the analysis framework"""
    print("=" * 60)
    print("åˆ›å»ºæ¼”ç¤ºæ•°æ®ä»¥å±•ç¤ºåˆ†ææ¡†æ¶")
    print("=" * 60)
    
    # Create sample data structure based on the plan
    np.random.seed(42)
    
    # Sample configuration
    models = ['gpt-4.1', 'gpt-4o', 'o1', 'lightrag', 'MOSES', 'llasmol', 'darwin', 'gpt-4o-mini', 'chem13b']
    questions = [f'q_{i}' for i in range(1, 28)]  # 27 questions
    dimensions = ['æ­£ç¡®æ€§', 'å®Œå¤‡æ€§', 'ç†è®ºæ·±åº¦', 'è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦']  # 4 dimensions as per plan
    
    # Generate sample human scores (3 evaluators per model-question-dimension)
    human_data = []
    for model in models:
        for question in questions:
            for dimension in dimensions:
                # Generate scores for 3 human evaluators with some correlation
                base_score = np.random.uniform(3, 9)
                for evaluator in range(1, 4):
                    score = max(1, min(10, base_score + np.random.normal(0, 1)))
                    human_data.append({
                        'model': model,
                        'question': question, 
                        'dimension': dimension,
                        'human_evaluator': evaluator,
                        'score': round(score, 1)
                    })
    
    human_df = pd.DataFrame(human_data)
    
    # Generate sample LLM scores (5 answer rounds Ã— 5 evaluation rounds)
    llm_data = []
    for model in models:
        for question in questions:
            for dimension in dimensions:
                # Generate base score correlated with human score
                human_avg = human_df[(human_df['model'] == model) & 
                                   (human_df['question'] == question) & 
                                   (human_df['dimension'] == dimension)]['score'].mean()
                
                for answer_round in range(1, 6):
                    for eval_round in range(1, 6):
                        # LLM score with some correlation to human score + noise
                        llm_score = human_avg + np.random.normal(0, 1.5)
                        llm_score = max(1, min(10, llm_score))
                        
                        llm_data.append({
                            'model': model,
                            'question': question,
                            'dimension': dimension,
                            'answer_round': answer_round,
                            'evaluation_round': eval_round,
                            'score': round(llm_score, 1)
                        })
    
    llm_df = pd.DataFrame(llm_data)
    
    print(f"ç”Ÿæˆæ¼”ç¤ºæ•°æ®:")
    print(f"- äººç±»è¯„åˆ†: {len(human_df)} æ¡ ({len(models)} æ¨¡å‹ Ã— {len(questions)} é—®é¢˜ Ã— {len(dimensions)} ç»´åº¦ Ã— 3 è¯„å§”)")
    print(f"- LLMè¯„åˆ†: {len(llm_df)} æ¡ ({len(models)} æ¨¡å‹ Ã— {len(questions)} é—®é¢˜ Ã— {len(dimensions)} ç»´åº¦ Ã— 5 è½®å›ç­” Ã— 5 æ¬¡è¯„åˆ†)")
    
    return human_df, llm_df, models, questions, dimensions

def preprocess_and_aggregate(human_df, llm_df):
    """ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†ä¸èšåˆ"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†ä¸èšåˆ")
    print("=" * 60)
    
    # 1. Human_Avg: Average of 3 human evaluators
    print("1. è®¡ç®— Human_Avg...")
    human_avg = human_df.groupby(['model', 'question', 'dimension'])['score'].mean().reset_index()
    human_avg.rename(columns={'score': 'Human_Avg'}, inplace=True)
    
    # 2. LLM_Avg_Overall: Overall average across all rounds
    print("2. è®¡ç®— LLM_Avg_Overall...")
    llm_avg_overall = llm_df.groupby(['model', 'question', 'dimension'])['score'].mean().reset_index()
    llm_avg_overall.rename(columns={'score': 'LLM_Avg_Overall'}, inplace=True)
    
    # 3. LLM_Repetition_Avg: Average across evaluation rounds for each answer round
    print("3. è®¡ç®— LLM_Repetition_Avg...")
    llm_rep_avg = llm_df.groupby(['model', 'question', 'dimension', 'answer_round'])['score'].mean().reset_index()
    llm_rep_avg.rename(columns={'score': 'LLM_Repetition_Avg'}, inplace=True)
    
    # 4. Merge human and LLM data
    print("4. åˆå¹¶æ•°æ®...")
    merged = pd.merge(human_avg, llm_avg_overall, on=['model', 'question', 'dimension'], how='inner')
    
    # 5. Disagreement_Score
    print("5. è®¡ç®— Disagreement_Score...")
    merged['Disagreement_Score'] = merged['LLM_Avg_Overall'] - merged['Human_Avg']
    
    print(f"èšåˆæ•°æ®æ¡ç›®æ•°: {len(merged)}")
    print(f"è¦†ç›–çš„æ¨¡å‹æ•°: {merged['model'].nunique()}")
    print(f"è¦†ç›–çš„é—®é¢˜æ•°: {merged['question'].nunique()}")
    print(f"è¦†ç›–çš„ç»´åº¦æ•°: {merged['dimension'].nunique()}")
    
    # Save data
    merged.to_csv(OUTPUT_DIR / "data" / "aggregated_scores_demo.csv", index=False, encoding='utf-8-sig')
    llm_rep_avg.to_csv(OUTPUT_DIR / "data" / "llm_repetition_scores_demo.csv", index=False, encoding='utf-8-sig')
    
    return merged, llm_rep_avg

def analyze_ranking_consistency(merged_data, dimensions):
    """ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§åˆ†æ (è£åˆ¤è§†è§’)")
    print("=" * 60)
    
    # 2.1 LLM-äººç±»ä¸€è‡´æ€§
    print("2.1 è®¡ç®—LLM-äººç±»æ’åºä¸€è‡´æ€§...")
    
    correlation_results = []
    
    # For each question and dimension combination
    questions = merged_data['question'].unique()
    
    print(f"åˆ†æ {len(questions)} ä¸ªé—®é¢˜ Ã— {len(dimensions)} ä¸ªç»´åº¦ = {len(questions) * len(dimensions)} ä¸ªç»„åˆ")
    
    for question in questions:
        for dimension in dimensions:
            subset = merged_data[
                (merged_data['question'] == question) & 
                (merged_data['dimension'] == dimension)
            ]
            
            if len(subset) >= 3:  # Need at least 3 models
                human_scores = subset['Human_Avg'].values
                llm_scores = subset['LLM_Avg_Overall'].values
                
                # Calculate correlations with p-values
                kendall_tau, kendall_p = kendalltau(human_scores, llm_scores)
                spearman_rho, spearman_p = spearmanr(human_scores, llm_scores)
                
                # Calculate ICC
                try:
                    icc_data = []
                    for i, model in enumerate(subset['model'].values):
                        icc_data.append({'model': model, 'rater': 'human', 'score': human_scores[i]})
                        icc_data.append({'model': model, 'rater': 'llm', 'score': llm_scores[i]})
                    
                    icc_df = pd.DataFrame(icc_data)
                    icc_result = pg.intraclass_corr(data=icc_df, targets='model', 
                                                  raters='rater', ratings='score')
                    
                    icc_row = icc_result[icc_result['Type'] == 'ICC1']
                    icc_val = float(icc_row['ICC'].iloc[0]) if len(icc_row) > 0 else np.nan
                    icc_p = float(icc_row['pval'].iloc[0]) if len(icc_row) > 0 else np.nan
                except:
                    icc_val, icc_p = np.nan, np.nan
                
                correlation_results.append({
                    'question': question,
                    'dimension': dimension,
                    'kendall_tau': float(kendall_tau),
                    'kendall_p': float(kendall_p),
                    'spearman_rho': float(spearman_rho),
                    'spearman_p': float(spearman_p),
                    'icc': icc_val,
                    'icc_p': icc_p,
                    'n_models': len(subset)
                })
    
    correlation_df = pd.DataFrame(correlation_results)
    
    # Calculate summary statistics by dimension
    print("2.2 è®¡ç®—å„ç»´åº¦ç»Ÿè®¡æ‘˜è¦...")
    ranking_summary = {}
    
    for dimension in dimensions:
        dim_data = correlation_df[correlation_df['dimension'] == dimension]
        
        if len(dim_data) > 0:
            kendall_vals = dim_data['kendall_tau'].dropna()
            spearman_vals = dim_data['spearman_rho'].dropna()
            icc_vals = dim_data['icc'].dropna()
            
            ranking_summary[dimension] = {
                'kendall_mean': float(kendall_vals.mean()) if len(kendall_vals) > 0 else np.nan,
                'kendall_ci': [float(kendall_vals.quantile(0.0015)), float(kendall_vals.quantile(0.9985))] if len(kendall_vals) > 0 else [np.nan, np.nan],
                'kendall_significant_ratio': float((dim_data['kendall_p'] < 0.05).mean()),
                'kendall_avg_p': float(dim_data['kendall_p'].mean()),
                
                'spearman_mean': float(spearman_vals.mean()) if len(spearman_vals) > 0 else np.nan,
                'spearman_ci': [float(spearman_vals.quantile(0.0015)), float(spearman_vals.quantile(0.9985))] if len(spearman_vals) > 0 else [np.nan, np.nan],
                'spearman_significant_ratio': float((dim_data['spearman_p'] < 0.05).mean()),
                
                'icc_mean': float(icc_vals.mean()) if len(icc_vals) > 0 else np.nan,
                'icc_ci': [float(icc_vals.quantile(0.0015)), float(icc_vals.quantile(0.9985))] if len(icc_vals) > 0 else [np.nan, np.nan],
                
                'n_comparisons': len(dim_data)
            }
    
    # Save results
    correlation_df.to_csv(OUTPUT_DIR / "results" / "ranking_correlations_demo.csv", 
                        index=False, encoding='utf-8-sig')
    
    print(f"å®Œæˆ {len(correlation_df)} ä¸ªé—®é¢˜-ç»´åº¦ç»„åˆçš„ç›¸å…³æ€§åˆ†æ")
    
    # Print summary
    print("\nå„ç»´åº¦LLM-äººç±»æ’åºä¸€è‡´æ€§æ‘˜è¦:")
    for dim, stats in ranking_summary.items():
        print(f"  {dim}:")
        print(f"    Kendall's Ï„: {stats['kendall_mean']:.3f} [CI: {stats['kendall_ci'][0]:.3f}, {stats['kendall_ci'][1]:.3f}]")
        print(f"    æ˜¾è‘—æ€§æ¯”ä¾‹: {stats['kendall_significant_ratio']:.2%} (å¹³å‡p={stats['kendall_avg_p']:.4f})")
    
    return correlation_df, ranking_summary

def create_visualizations(merged_data, correlation_df, ranking_summary, dimensions):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # Figure A: Overall scatter plots by dimension
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å›¾A: LLMä¸äººç±»è¯„åˆ†æ€»ä½“å…³ç³» (æŒ‰ç»´åº¦åˆ†é¢)', fontsize=16, fontweight='bold')
    
    for i, dimension in enumerate(dimensions):
        row, col = i // 2, i % 2
        ax = axes[row, col]
        
        # Get data for this dimension
        dim_data = merged_data[merged_data['dimension'] == dimension]
        
        # Scatter plot
        ax.scatter(dim_data['Human_Avg'], dim_data['LLM_Avg_Overall'], 
                  alpha=0.6, s=50, c='steelblue')
        
        # Add trend line
        if len(dim_data) > 1:
            z = np.polyfit(dim_data['Human_Avg'], dim_data['LLM_Avg_Overall'], 1)
            p = np.poly1d(z)
            x_line = np.linspace(dim_data['Human_Avg'].min(), dim_data['Human_Avg'].max(), 100)
            ax.plot(x_line, p(x_line), "red", alpha=0.8, linewidth=2, label='è¶‹åŠ¿çº¿')
        
        # Add y=x reference line
        min_val = min(dim_data['Human_Avg'].min(), dim_data['LLM_Avg_Overall'].min())
        max_val = max(dim_data['Human_Avg'].max(), dim_data['LLM_Avg_Overall'].max())
        ax.plot([min_val, max_val], [min_val, max_val], 'k:', alpha=0.5, linewidth=1, label='y=xå‚è€ƒçº¿')
        
        ax.set_xlabel('äººç±»å¹³å‡è¯„åˆ†')
        ax.set_ylabel('LLMå¹³å‡è¯„åˆ†')
        ax.set_title(f'{dimension}')
        ax.grid(True, alpha=0.3)
        ax.legend()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "figures" / "å›¾A_æ’åºä¸€è‡´æ€§æ•£ç‚¹å›¾_æ€»ä½“è¶‹åŠ¿.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # Figure C: Forest plot for correlation coefficients
    fig, ax = plt.subplots(figsize=(12, 8))
    
    x_pos = np.arange(len(dimensions))
    width = 0.25
    
    # Prepare data
    kendall_means = []
    kendall_errs = []
    spearman_means = []
    spearman_errs = []
    icc_means = []
    icc_errs = []
    
    for dim in dimensions:
        if dim in ranking_summary:
            s = ranking_summary[dim]
            
            kendall_means.append(s['kendall_mean'])
            kendall_errs.append([s['kendall_mean'] - s['kendall_ci'][0], s['kendall_ci'][1] - s['kendall_mean']])
            
            spearman_means.append(s['spearman_mean'])
            spearman_errs.append([s['spearman_mean'] - s['spearman_ci'][0], s['spearman_ci'][1] - s['spearman_mean']])
            
            icc_means.append(s['icc_mean'])
            icc_errs.append([s['icc_mean'] - s['icc_ci'][0], s['icc_ci'][1] - s['icc_mean']])
    
    # Convert to format needed for errorbar
    kendall_errs = np.array(kendall_errs).T
    spearman_errs = np.array(spearman_errs).T
    icc_errs = np.array(icc_errs).T
    
    # Plot error bars
    ax.errorbar(x_pos - width, kendall_means, yerr=kendall_errs, 
               fmt='o', capsize=5, capthick=2, label="Kendall's Ï„", markersize=8)
    ax.errorbar(x_pos, spearman_means, yerr=spearman_errs, 
               fmt='s', capsize=5, capthick=2, label="Spearman's Ï", markersize=8)
    ax.errorbar(x_pos + width, icc_means, yerr=icc_errs, 
               fmt='^', capsize=5, capthick=2, label="ICC(A,1)", markersize=8)
    
    ax.set_xlabel('è¯„ä¼°ç»´åº¦')
    ax.set_ylabel('ç›¸å…³ç³»æ•°å€¼')
    ax.set_title('å›¾C: LLM-äººç±»æ’åºä¸€è‡´æ€§æ£®æ—å›¾\n(99.7%ç½®ä¿¡åŒºé—´)', fontweight='bold')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(dimensions, rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "figures" / "å›¾C_æ’åºä¸€è‡´æ€§æ£®æ—å›¾.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # Violin plot for disagreement scores
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Prepare disagreement data by model
    models = merged_data['model'].unique()
    disagreement_by_model = []
    
    for model in models:
        model_data = merged_data[merged_data['model'] == model]
        disagreement_by_model.append(model_data['Disagreement_Score'].values)
    
    # Create violin plot
    parts = ax.violinplot(disagreement_by_model, positions=range(len(models)), showmeans=True)
    
    ax.set_xlabel('AIæ¨¡å‹')
    ax.set_ylabel('ä¸€è‡´æ€§è¯¯å·® (LLM - Human)')
    ax.set_title('å„æ¨¡å‹çš„LLM-äººç±»è¯„åˆ†å·®å¼‚åˆ†å¸ƒ', fontweight='bold')
    ax.set_xticks(range(len(models)))
    ax.set_xticklabels(models, rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='å®Œå…¨ä¸€è‡´çº¿')
    ax.legend()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "figures" / "ä¸€è‡´æ€§è¯¯å·®å°æç´å›¾.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")

def generate_final_report(ranking_summary, correlation_df, total_entries, models_count, questions_count, dimensions_count):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print("\nç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    
    report = f"""
# AIæ¨¡å‹è¯„ä¼°é¡¹ç›®ï¼šä¸€è‡´æ€§åˆ†ææŠ¥å‘Š

## æ‘˜è¦ä¸å¼•è¨€

æœ¬æŠ¥å‘Šåˆ†æäº†{models_count}ä¸ªAIæ¨¡å‹åœ¨{questions_count}ä¸ªåŒ–å­¦ç›¸å…³é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°äº†LLMè¯„å§”ä¸äººç±»è¯„å§”åœ¨è¯„åˆ†ä¸Šçš„ä¸€è‡´æ€§ã€‚åˆ†ææ¶µç›–{dimensions_count}ä¸ªè¯„ä¼°ç»´åº¦ï¼šæ­£ç¡®æ€§ã€å®Œå¤‡æ€§ã€ç†è®ºæ·±åº¦ã€è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦ã€‚

## æ•°æ®æ¦‚å†µ

- **è¯„ä¼°æ¨¡å‹æ•°é‡**: {models_count}ä¸ª
- **è¯„ä¼°é—®é¢˜æ•°é‡**: {questions_count}ä¸ª  
- **è¯„ä¼°ç»´åº¦**: {dimensions_count}ä¸ª
- **æ€»æ•°æ®æ¡ç›®**: {total_entries}æ¡

## æ–¹æ³•è®º

### æ•°æ®é¢„å¤„ç†æ–¹æ³•
1. **Human_Avg**: è®¡ç®—3ä½äººç±»è¯„å§”å¯¹æ¯ä¸ª{{æ¨¡å‹,é—®é¢˜,ç»´åº¦}}ç»„åˆçš„å¹³å‡åˆ†
2. **LLM_Avg_Overall**: è®¡ç®—LLMå¯¹æ¯ä¸ª{{æ¨¡å‹,é—®é¢˜,ç»´åº¦}}ç»„åˆçš„æ€»ä½“å¹³å‡åˆ†(5è½®å›ç­”Ã—5æ¬¡é‡å¤è¯„åˆ†)
3. **Disagreement_Score**: LLM_Avg_Overall - Human_Avg

### åˆ†ææ¡†æ¶
æœ¬ç ”ç©¶é‡‡ç”¨ä¸¤ä¸ªå±‚æ¬¡çš„åˆ†ææ¡†æ¶ï¼š
1. **æ¨¡å‹æ’åºä¸€è‡´æ€§ (è£åˆ¤è§†è§’)**: è¯„ä¼°LLMä½œä¸º"è£åˆ¤"ï¼Œå…¶ç»™å‡ºçš„æ¨¡å‹ä¼˜åŠ£æ’è¡Œæ¦œæ˜¯å¦ä¸äººç±»ä¸€è‡´
2. **æ¨¡å‹è¯Šæ–­ä¸€è‡´æ€§ (æ•™ç»ƒè§†è§’)**: è¯„ä¼°LLMä½œä¸º"æ•™ç»ƒ"ï¼Œå…¶å¯¹å•ä¸ªæ¨¡å‹èƒ½åŠ›ä¼˜åŠ£çš„è¯Šæ–­æ˜¯å¦ä¸äººç±»ä¸€è‡´

### ç»Ÿè®¡åˆ†ææ–¹æ³•
- **Kendall's Tau-b**: ä¸»è¦çš„æ’åºä¸€è‡´æ€§æŒ‡æ ‡
- **Spearman's Rho**: è¾…åŠ©çš„æ’åºä¸€è‡´æ€§æŒ‡æ ‡  
- **ICC(A,1)**: ç»å¯¹ä¸€è‡´æ€§ç³»æ•°
- **ç½®ä¿¡åŒºé—´**: 99.7%ç½®ä¿¡åŒºé—´ (0.15th - 99.85thç™¾åˆ†ä½æ•°)

## æ ¸å¿ƒå‘ç°ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§

### LLM-äººç±»æ’åºä¸€è‡´æ€§åˆ†æ

ä»¥é—®é¢˜å’Œç»´åº¦ä¸ºåˆ†æå•ä½ï¼Œè®¡ç®—LLMä¸äººç±»è¯„å§”å¯¹{models_count}ä¸ªAIæ¨¡å‹çš„æ’åºä¸€è‡´æ€§ï¼š

"""
    
    for dimension, stats in ranking_summary.items():
        kendall_mean = stats['kendall_mean']
        kendall_ci = stats['kendall_ci']
        sig_ratio = stats['kendall_significant_ratio']
        avg_p = stats['kendall_avg_p']
        
        # åˆ¤æ–­ä¸€è‡´æ€§æ°´å¹³
        if kendall_mean > 0.7:
            consistency_level = "é«˜åº¦ä¸€è‡´"
        elif kendall_mean > 0.5:
            consistency_level = "è¾ƒé«˜ä¸€è‡´æ€§"
        elif kendall_mean > 0.3:
            consistency_level = "ä¸­ç­‰ä¸€è‡´æ€§"
        else:
            consistency_level = "è¾ƒä½ä¸€è‡´æ€§"
        
        report += f"""
#### {dimension}

- **Kendall's Ï„å¹³å‡å€¼**: {kendall_mean:.3f} [99.7% CI: {kendall_ci[0]:.3f}, {kendall_ci[1]:.3f}]
- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {sig_ratio:.1%}çš„æ¯”è¾ƒè¾¾åˆ°p < 0.05 (å¹³å‡på€¼ = {avg_p:.4f})
- **ä¸€è‡´æ€§æ°´å¹³**: {consistency_level}
- **åˆ†æç»“è®º**: åœ¨'{dimension}'ç»´åº¦ä¸Šï¼ŒLLMä¸äººç±»è¯„å§”åœ¨æ¨¡å‹æ’åºæ–¹é¢è¡¨ç°å‡º{consistency_level}ï¼Œè¡¨æ˜{"LLMå¯ä»¥ä½œä¸ºè¯¥ç»´åº¦è¯„ä¼°çš„å¯é è¾…åŠ©å·¥å…·" if kendall_mean > 0.5 else "LLMåœ¨è¯¥ç»´åº¦çš„è¯„ä¼°éœ€è¦æ›´å¤šæ”¹è¿›"}ã€‚
"""
    
    # æ·»åŠ æ•´ä½“ç»“è®º
    overall_kendall = np.mean([stats['kendall_mean'] for stats in ranking_summary.values()])
    overall_sig_ratio = np.mean([stats['kendall_significant_ratio'] for stats in ranking_summary.values()])
    
    report += f"""

## è¯„å§”å¯é æ€§åˆ†æ

### ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
- **æ€»ä½“å¹³å‡Kendall's Ï„**: {overall_kendall:.3f}
- **æ˜¾è‘—æ€§ç»“æœæ¯”ä¾‹**: {overall_sig_ratio:.1%}
- **æ•°æ®è´¨é‡**: åŸºäº{len(correlation_df)}ä¸ªç‹¬ç«‹çš„é—®é¢˜-ç»´åº¦ç»„åˆåˆ†æ

## ç»“è®ºä¸å»ºè®®

### ä¸»è¦å‘ç°
1. **æ€»ä½“ä¸€è‡´æ€§**: LLMä¸äººç±»è¯„å§”åœ¨æ¨¡å‹æ’åºä¸Šè¡¨ç°å‡º{'å¯æ¥å—' if overall_kendall > 0.3 else 'æœ‰é™'}çš„ä¸€è‡´æ€§ï¼ˆå¹³å‡Ï„ = {overall_kendall:.3f}ï¼‰
2. **ç»´åº¦å·®å¼‚**: ä¸åŒè¯„ä¼°ç»´åº¦ä¸‹çš„ä¸€è‡´æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚
3. **ç»Ÿè®¡å¯é æ€§**: {overall_sig_ratio:.1%}çš„åˆ†æç»“æœå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰

### å®é™…åº”ç”¨å»ºè®®
1. **LLMè¾…åŠ©è¯„ä¼°**: LLMå¯ä»¥ä½œä¸ºè¯„ä¼°å·¥å…·çš„{'é‡è¦' if overall_kendall > 0.5 else 'è¾…åŠ©'}è¡¥å……
2. **äººç±»ä¸“å®¶å‚ä¸**: åœ¨å…³é”®è¯„ä¼°å†³ç­–ä¸­ä»éœ€äººç±»ä¸“å®¶çš„åˆ¤æ–­
3. **ç»´åº¦ç‰¹å¼‚æ€§**: é’ˆå¯¹ä¸€è‡´æ€§è¾ƒä½çš„ç»´åº¦åŠ å¼ºè¯„ä¼°æ ‡å‡†å’Œè®­ç»ƒ

### ç ”ç©¶å±€é™æ€§
- æœ¬åˆ†æåŸºäºæ¼”ç¤ºæ•°æ®å±•ç¤ºåˆ†ææ¡†æ¶
- å®é™…åº”ç”¨éœ€è¦åŸºäºçœŸå®è¯„ä¼°æ•°æ®è¿›è¡ŒéªŒè¯
- éœ€è¦æ›´å¤šæ ·æœ¬é‡ä»¥æé«˜ç»Ÿè®¡åŠŸæ•ˆ

---
*æœ¬æŠ¥å‘Šç”±AIä¸€è‡´æ€§åˆ†æç³»ç»Ÿç”Ÿæˆ - æ¼”ç¤ºç‰ˆæœ¬*
*ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # Save report
    with open(OUTPUT_DIR / "results" / "final_comprehensive_report_demo.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ")
    print(f"æŠ¥å‘Šä¿å­˜åœ¨: {OUTPUT_DIR / 'results' / 'final_comprehensive_report_demo.md'}")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ å¼€å§‹AIæ¨¡å‹è¯„ä¼°ä¸€è‡´æ€§åˆ†ææ¼”ç¤º...")
    
    # Step 0: Load sample data
    human_df, llm_df, models, questions, dimensions = load_sample_data()
    
    # Step 1: Preprocessing and aggregation  
    merged_data, llm_rep_data = preprocess_and_aggregate(human_df, llm_df)
    
    # Step 2: Ranking consistency analysis
    correlation_df, ranking_summary = analyze_ranking_consistency(merged_data, dimensions)
    
    # Create visualizations
    create_visualizations(merged_data, correlation_df, ranking_summary, dimensions)
    
    # Generate final report
    generate_final_report(
        ranking_summary, 
        correlation_df,
        len(merged_data),
        len(models),
        len(questions), 
        len(dimensions)
    )
    
    # Save comprehensive results
    results = {
        'preprocessing': {
            'total_entries': len(merged_data),
            'models_count': len(models),
            'questions_count': len(questions),
            'dimensions_count': len(dimensions)
        },
        'ranking_consistency': {
            'summary_by_dimension': ranking_summary,
            'correlation_details': correlation_df.to_dict('records')
        }
    }
    
    with open(OUTPUT_DIR / "results" / "comprehensive_analysis_results_demo.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print("\n" + "=" * 60)
    print("âœ… ä¸€è‡´æ€§åˆ†ææ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
    print("\nğŸ“‹ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
    print(f"  ğŸ“Š æœ€ç»ˆæŠ¥å‘Š: final_comprehensive_report_demo.md")
    print(f"  ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: figures/å›¾*.png") 
    print(f"  ğŸ“„ è¯¦ç»†ç»“æœ: comprehensive_analysis_results_demo.json")
    print(f"  ğŸ“Š æ•°æ®æ–‡ä»¶: data/*_demo.csv")
    print("=" * 60)
    
    # Print summary
    print("\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
    print(f"ğŸ¤– AIæ¨¡å‹æ•°é‡: {len(models)}")
    print(f"â“ é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"ğŸ“ è¯„ä¼°ç»´åº¦: {len(dimensions)}")
    print(f"ğŸ“ˆ æ•°æ®æ¡ç›®: {len(merged_data)}")
    
    print(f"\nğŸ¯ å„ç»´åº¦LLM-äººç±»æ’åºä¸€è‡´æ€§ (Kendall's Ï„):")
    for dim, stats in ranking_summary.items():
        kendall_mean = stats['kendall_mean']
        sig_ratio = stats['kendall_significant_ratio']
        status = "ğŸŸ¢ é«˜" if kendall_mean > 0.5 else "ğŸŸ¡ ä¸­" if kendall_mean > 0.3 else "ğŸ”´ ä½"
        print(f"  {dim}: {kendall_mean:.3f} ({status}, æ˜¾è‘—æ€§: {sig_ratio:.1%})")
    
    overall_kendall = np.mean([stats['kendall_mean'] for stats in ranking_summary.values()])
    print(f"\nğŸ–ï¸  æ€»ä½“å¹³å‡ä¸€è‡´æ€§: {overall_kendall:.3f}")

if __name__ == "__main__":
    main()