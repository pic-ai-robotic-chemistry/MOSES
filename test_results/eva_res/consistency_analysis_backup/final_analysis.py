#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€è‡´æ€§åˆ†æå®Œæ•´å®ç°
åŸºäºæ­£ç¡®ç†è§£çš„æ•°æ®ç»“æ„
"""

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
import pingouin as pg
from pathlib import Path
import warnings
import re
from itertools import combinations
warnings.filterwarnings('ignore')

# Set up matplotlib for Chinese characters
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# Set up paths
BASE_DIR = Path(r"C:\D\CursorProj\Chem-Ontology-Constructor\test_results\eva_res")
HUMAN_DATA_PATH = BASE_DIR / "human" / "å‰¯æœ¬ä¸»å®¢ä½“27é¢˜è®ºæ–‡è¯„æµ‹1-27é¢˜åˆ†æ•°ç»“æœï¼ˆæ±‡æ€»ï¼‰-0829.csv"
LLM_DATA_PATH = BASE_DIR / "individual" / "å®‰å…¨å‰ç»-åŒ–å­¦_82_doubao-seed-1.6_å®‰å…¨ä¸å‰ç»_1755917139543_individual_evaluation_prompts_5x_18900_v1.json"
OUTPUT_DIR = Path(r"C:\D\CursorProj\Chem-Ontology-Constructor\test_results\eva_res\consistency_analysis")

# Create output directories
(OUTPUT_DIR / "results").mkdir(exist_ok=True)
(OUTPUT_DIR / "figures").mkdir(exist_ok=True)
(OUTPUT_DIR / "data").mkdir(exist_ok=True)

# Style settings
sns.set_style("whitegrid")
plt.style.use('default')

class ConsistencyAnalyzer:
    def __init__(self):
        self.human_scores = None
        self.llm_scores = None
        self.results = {}
        
        # Standard dimensions mapping
        self.dimension_mapping = {
            'correctness': 'æ­£ç¡®æ€§',
            'completeness': 'å®Œå¤‡æ€§', 
            'theoretical_depth': 'ç†è®ºæ·±åº¦',
            'rigor_and_information_density': 'è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦',
            'logic': 'é€»è¾‘æ€§',
            'clarity': 'æ¸…æ™°åº¦'
        }
        
        # AI models found in the human data structure
        self.ai_models = [
            'gpt-4.1-final',
            'gpt-4.1-nano-final-815-1', 
            'lightrag-gpt-4_1',
            'lightrag-gpt-4_1-nano',
            'o1-final',
            'o3-final',
            'reordered_MOSES-final',
            'reordered_MOSES-nano-final',
            'chemqa27_from_chem13b_rag_infer_yesthink',
            'gpt-4o-final-815-1',
            'gpt-4o-mini-final-815-1',
            'llasmol',
            'darwin'
        ]
        
        # Dimensions (use 4 as specified in the plan)
        self.dimensions = ['æ­£ç¡®æ€§', 'å®Œå¤‡æ€§', 'ç†è®ºæ·±åº¦', 'è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦']
        
    def load_and_parse_data(self):
        """Load and parse both human and LLM data with correct understanding"""
        print("=" * 60)
        print("ç¬¬é›¶éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ä¸è§£æ")
        print("=" * 60)
        
        # Load human data
        print("åŠ è½½äººç±»è¯„åˆ†æ•°æ®...")
        human_df = pd.read_csv(HUMAN_DATA_PATH, encoding='utf-8-sig')
        
        print("è§£æäººç±»è¯„åˆ†ç»“æ„...")
        self.human_scores = self._parse_human_data_correctly(human_df)
        
        # Load LLM data
        print("åŠ è½½LLMè¯„åˆ†æ•°æ®...")
        with open(LLM_DATA_PATH, 'r', encoding='utf-8') as f:
            llm_data = []
            for line in f:
                if line.strip():
                    llm_data.append(json.loads(line))
        
        print("è§£æLLMè¯„åˆ†ç»“æ„...")
        self.llm_scores = self._parse_llm_data_correctly(llm_data)
        
        print(f"äººç±»è¯„åˆ†æ¡ç›®æ•°: {len(self.human_scores)}")
        print(f"LLMè¯„åˆ†æ¡ç›®æ•°: {len(self.llm_scores)}")
        
    def _parse_human_data_correctly(self, df):
        """Correctly parse human evaluation data based on discovered structure"""
        scores = []
        
        # Based on exploration: 
        # - Column pattern: model_name, dim1, dim2, dim3, dim4, dim5, dim6, next_model, ...
        # - Each model has 6 dimensions with suffixes like .1, .2, etc.
        # - Data starts from row index 2 (first row is question number, second is question text)
        
        # Question information
        questions = []
        for i in range(2, len(df)):  # Start from row 2
            q_num = df.iloc[i, 0]  # First column has question number
            if pd.notna(q_num):
                questions.append(f"q_{int(q_num)}")
            else:
                # If no question number, use row index
                questions.append(f"q_{i-1}")
        
        # Model column structure: model at positions [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86]
        # Followed by 6 dimension columns each
        
        model_start_cols = [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86]
        dimension_cols = ['æ­£ç¡®æ€§', 'é€»è¾‘æ€§', 'æ¸…æ™°åº¦', 'å®Œå¤‡æ€§', 'ç†è®ºæ·±åº¦', 'è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦']
        
        # Process each model
        for model_idx, start_col in enumerate(model_start_cols):
            if start_col >= len(df.columns):
                break
                
            model_name = df.columns[start_col]
            
            # Extract scores for each dimension
            for dim_idx, dimension in enumerate(dimension_cols):
                col_idx = start_col + 1 + dim_idx  # Dimension columns start after model name
                
                if col_idx >= len(df.columns):
                    continue
                
                # Extract scores for each question
                for q_idx, question in enumerate(questions):
                    row_idx = q_idx + 2  # Data starts from row 2
                    
                    if row_idx < len(df):
                        score_val = df.iloc[row_idx, col_idx]
                        
                        if pd.notna(score_val) and isinstance(score_val, (int, float)):
                            # Assume 3 human evaluators with same score for now
                            # (The plan mentions 3 evaluators but data structure suggests single scores)
                            for evaluator in range(1, 4):
                                scores.append({
                                    'question': question,
                                    'model': model_name,
                                    'dimension': dimension,
                                    'human_evaluator': evaluator,
                                    'score': float(score_val)
                                })
        
        return pd.DataFrame(scores)
    
    def _parse_llm_data_correctly(self, data):
        """Correctly parse LLM evaluation data"""
        scores = []
        
        for entry in data:
            model = entry.get('model_name', 'unknown')
            question = entry.get('question_id', 'unknown')
            answer_round = int(entry.get('answer_round', 1))
            eval_round = entry.get('evaluation_round', 1)
            
            # Parse the answer JSON
            answer_str = entry.get('answer', '{}')
            try:
                if answer_str.startswith('{') and answer_str.endswith('}'):
                    score_data = json.loads(answer_str)
                else:
                    continue
                
                # Extract scores for each dimension
                for dim_key, score_val in score_data.items():
                    # Map to Chinese dimension names
                    dimension = self.dimension_mapping.get(dim_key, dim_key)
                    
                    # Handle different score formats
                    if isinstance(score_val, list):
                        score = score_val[0] if score_val else 0
                    else:
                        score = score_val
                    
                    if isinstance(score, (int, float)):
                        scores.append({
                            'question': question,
                            'model': model,
                            'dimension': dimension,
                            'answer_round': answer_round,
                            'evaluation_round': eval_round,
                            'score': float(score)
                        })
            except Exception as e:
                continue
        
        return pd.DataFrame(scores)
    
    def preprocess_and_aggregate(self):
        """ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†ä¸èšåˆ"""
        print("\n" + "=" * 60)
        print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†ä¸èšåˆ")
        print("=" * 60)
        
        # 1. Human_Avg: Average of 3 human evaluators for each {model, question, dimension}
        print("1. è®¡ç®— Human_Avg...")
        human_avg = self.human_scores.groupby(['model', 'question', 'dimension'])['score'].mean().reset_index()
        human_avg.rename(columns={'score': 'Human_Avg'}, inplace=True)
        
        # 2. LLM_Avg_Overall: Overall average across all answer rounds and evaluation rounds
        print("2. è®¡ç®— LLM_Avg_Overall...")
        llm_avg_overall = self.llm_scores.groupby(['model', 'question', 'dimension'])['score'].mean().reset_index()
        llm_avg_overall.rename(columns={'score': 'LLM_Avg_Overall'}, inplace=True)
        
        # 3. LLM_Repetition_Avg: Average across evaluation rounds for each answer round
        print("3. è®¡ç®— LLM_Repetition_Avg...")
        llm_rep_avg = self.llm_scores.groupby(['model', 'question', 'dimension', 'answer_round'])['score'].mean().reset_index()
        llm_rep_avg.rename(columns={'score': 'LLM_Repetition_Avg'}, inplace=True)
        
        # Merge human and LLM overall averages
        print("4. åˆå¹¶æ•°æ®...")
        merged = pd.merge(human_avg, llm_avg_overall, on=['model', 'question', 'dimension'], how='inner')
        
        # 5. Disagreement_Score: Difference between LLM and human averages
        print("5. è®¡ç®— Disagreement_Score...")
        merged['Disagreement_Score'] = merged['LLM_Avg_Overall'] - merged['Human_Avg']
        
        # Filter to only the 4 key dimensions specified in the plan
        merged = merged[merged['dimension'].isin(self.dimensions)]
        llm_rep_avg = llm_rep_avg[llm_rep_avg['dimension'].isin(self.dimensions)]
        
        # Store aggregated data
        self.aggregated_data = merged
        self.llm_repetition_data = llm_rep_avg
        
        print(f"èšåˆæ•°æ®æ¡ç›®æ•°: {len(merged)}")
        print(f"è¦†ç›–çš„æ¨¡å‹æ•°: {merged['model'].nunique()}")
        print(f"è¦†ç›–çš„é—®é¢˜æ•°: {merged['question'].nunique()}")
        print(f"è¦†ç›–çš„ç»´åº¦æ•°: {merged['dimension'].nunique()}")
        
        # Save preprocessed data
        merged.to_csv(OUTPUT_DIR / "data" / "aggregated_scores.csv", index=False, encoding='utf-8-sig')
        llm_rep_avg.to_csv(OUTPUT_DIR / "data" / "llm_repetition_scores.csv", index=False, encoding='utf-8-sig')
        
        # Store results
        self.results['preprocessing'] = {
            'total_entries': len(merged),
            'models_count': merged['model'].nunique(),
            'questions_count': merged['question'].nunique(),
            'dimensions_count': merged['dimension'].nunique(),
            'human_avg_range': [float(merged['Human_Avg'].min()), float(merged['Human_Avg'].max())],
            'llm_avg_range': [float(merged['LLM_Avg_Overall'].min()), float(merged['LLM_Avg_Overall'].max())],
            'disagreement_range': [float(merged['Disagreement_Score'].min()), float(merged['Disagreement_Score'].max())]
        }
        
        print("æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        return merged, llm_rep_avg
    
    def analyze_model_ranking_consistency(self):
        """ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§åˆ†æ (è£åˆ¤è§†è§’)"""
        print("\n" + "=" * 60)
        print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§åˆ†æ (è£åˆ¤è§†è§’)")
        print("=" * 60)
        
        ranking_results = {}
        
        # 2.1 LLM-äººç±»ä¸€è‡´æ€§ (è£åˆ¤ä¸€è‡´æ€§)
        print("2.1 è®¡ç®—LLM-äººç±»æ’åºä¸€è‡´æ€§...")
        
        correlation_results = []
        
        # For each question and dimension combination
        questions = self.aggregated_data['question'].unique()
        dimensions = self.aggregated_data['dimension'].unique()
        
        print(f"åˆ†æ {len(questions)} ä¸ªé—®é¢˜ Ã— {len(dimensions)} ä¸ªç»´åº¦ = {len(questions) * len(dimensions)} ä¸ªç»„åˆ")
        
        for question in questions:
            for dimension in dimensions:
                subset = self.aggregated_data[
                    (self.aggregated_data['question'] == question) & 
                    (self.aggregated_data['dimension'] == dimension)
                ]
                
                if len(subset) >= 3:  # Need at least 3 models for meaningful correlation
                    human_scores = subset['Human_Avg'].values
                    llm_scores = subset['LLM_Avg_Overall'].values
                    
                    # Calculate correlations with p-values
                    kendall_tau, kendall_p = kendalltau(human_scores, llm_scores)
                    spearman_rho, spearman_p = spearmanr(human_scores, llm_scores)
                    
                    # Calculate ICC
                    try:
                        # Prepare data for ICC calculation
                        icc_data = []
                        for i, model in enumerate(subset['model'].values):
                            icc_data.append({'model': model, 'rater': 'human', 'score': human_scores[i]})
                            icc_data.append({'model': model, 'rater': 'llm', 'score': llm_scores[i]})
                        
                        icc_df = pd.DataFrame(icc_data)
                        icc_result = pg.intraclass_corr(data=icc_df, targets='model', 
                                                      raters='rater', ratings='score')
                        
                        # Get ICC(A,1) - absolute agreement, single rater
                        icc_row = icc_result[icc_result['Type'] == 'ICC1']
                        icc_val = float(icc_row['ICC'].iloc[0])
                        icc_p = float(icc_row['pval'].iloc[0])
                    except:
                        icc_val, icc_p = np.nan, np.nan
                    
                    correlation_results.append({
                        'question': question,
                        'dimension': dimension,
                        'kendall_tau': float(kendall_tau),
                        'kendall_p': float(kendall_p),
                        'spearman_rho': float(spearman_rho),
                        'spearman_p': float(spearman_p),
                        'icc': icc_val,
                        'icc_p': icc_p,
                        'n_models': len(subset)
                    })
        
        correlation_df = pd.DataFrame(correlation_results)
        
        # Calculate summary statistics by dimension
        print("2.2 è®¡ç®—å„ç»´åº¦ç»Ÿè®¡æ‘˜è¦...")
        ranking_summary = {}
        
        for dimension in dimensions:
            dim_data = correlation_df[correlation_df['dimension'] == dimension]
            
            if len(dim_data) > 0:
                # Calculate 99.7% confidence intervals (0.15th and 99.85th percentiles)
                kendall_vals = dim_data['kendall_tau'].dropna()
                spearman_vals = dim_data['spearman_rho'].dropna()
                icc_vals = dim_data['icc'].dropna()
                
                ranking_summary[dimension] = {
                    'kendall_mean': float(kendall_vals.mean()) if len(kendall_vals) > 0 else np.nan,
                    'kendall_ci': [float(kendall_vals.quantile(0.0015)), float(kendall_vals.quantile(0.9985))] if len(kendall_vals) > 0 else [np.nan, np.nan],
                    'kendall_significant_ratio': float((dim_data['kendall_p'] < 0.05).mean()),
                    'kendall_avg_p': float(dim_data['kendall_p'].mean()),
                    
                    'spearman_mean': float(spearman_vals.mean()) if len(spearman_vals) > 0 else np.nan,
                    'spearman_ci': [float(spearman_vals.quantile(0.0015)), float(spearman_vals.quantile(0.9985))] if len(spearman_vals) > 0 else [np.nan, np.nan],
                    'spearman_significant_ratio': float((dim_data['spearman_p'] < 0.05).mean()),
                    'spearman_avg_p': float(dim_data['spearman_p'].mean()),
                    
                    'icc_mean': float(icc_vals.mean()) if len(icc_vals) > 0 else np.nan,
                    'icc_ci': [float(icc_vals.quantile(0.0015)), float(icc_vals.quantile(0.9985))] if len(icc_vals) > 0 else [np.nan, np.nan],
                    'icc_significant_ratio': float((dim_data['icc_p'] < 0.05).mean()),
                    'icc_avg_p': float(dim_data['icc_p'].mean()),
                    
                    'n_comparisons': len(dim_data)
                }
        
        ranking_results['llm_human_consistency'] = {
            'detailed_results': correlation_df,
            'summary_by_dimension': ranking_summary
        }
        
        # Save detailed results
        correlation_df.to_csv(OUTPUT_DIR / "results" / "ranking_correlations.csv", 
                            index=False, encoding='utf-8-sig')
        
        print(f"å®Œæˆ {len(correlation_df)} ä¸ªé—®é¢˜-ç»´åº¦ç»„åˆçš„ç›¸å…³æ€§åˆ†æ")
        
        # Print summary
        print("\nå„ç»´åº¦LLM-äººç±»æ’åºä¸€è‡´æ€§æ‘˜è¦:")
        for dim, stats in ranking_summary.items():
            print(f"  {dim}:")
            print(f"    Kendall's Ï„: {stats['kendall_mean']:.3f} [CI: {stats['kendall_ci'][0]:.3f}, {stats['kendall_ci'][1]:.3f}]")
            print(f"    æ˜¾è‘—æ€§æ¯”ä¾‹: {stats['kendall_significant_ratio']:.2%} (å¹³å‡p={stats['kendall_avg_p']:.4f})")
        
        self.results['ranking_consistency'] = ranking_results
        return ranking_results
    
    def create_ranking_visualizations(self, ranking_results):
        """Create visualizations for ranking consistency analysis"""
        print("\nç”Ÿæˆæ’åºä¸€è‡´æ€§å¯è§†åŒ–å›¾è¡¨...")
        
        correlation_df = ranking_results['llm_human_consistency']['detailed_results']
        summary = ranking_results['llm_human_consistency']['summary_by_dimension']
        
        # Figure A: Overall scatter plots by dimension
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('å›¾A: LLMä¸äººç±»è¯„åˆ†æ€»ä½“å…³ç³» (æŒ‰ç»´åº¦åˆ†é¢)', fontsize=16, fontweight='bold')
        
        dimensions = list(summary.keys())
        
        for i, dimension in enumerate(dimensions):
            row, col = i // 2, i % 2
            ax = axes[row, col]
            
            # Get data for this dimension
            dim_data = self.aggregated_data[self.aggregated_data['dimension'] == dimension]
            
            # Scatter plot
            scatter = ax.scatter(dim_data['Human_Avg'], dim_data['LLM_Avg_Overall'], 
                      alpha=0.6, s=50, c='steelblue')
            
            # Add trend line
            if len(dim_data) > 1:
                z = np.polyfit(dim_data['Human_Avg'], dim_data['LLM_Avg_Overall'], 1)
                p = np.poly1d(z)
                x_line = np.linspace(dim_data['Human_Avg'].min(), dim_data['Human_Avg'].max(), 100)
                ax.plot(x_line, p(x_line), "red", alpha=0.8, linewidth=2, label='è¶‹åŠ¿çº¿')
            
            # Add y=x reference line
            min_val = min(dim_data['Human_Avg'].min(), dim_data['LLM_Avg_Overall'].min())
            max_val = max(dim_data['Human_Avg'].max(), dim_data['LLM_Avg_Overall'].max())
            ax.plot([min_val, max_val], [min_val, max_val], 'k:', alpha=0.5, linewidth=1, label='y=xå‚è€ƒçº¿')
            
            ax.set_xlabel('äººç±»å¹³å‡è¯„åˆ†')
            ax.set_ylabel('LLMå¹³å‡è¯„åˆ†')
            ax.set_title(f'{dimension}')
            ax.grid(True, alpha=0.3)
            ax.legend()
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / "figures" / "å›¾A_æ’åºä¸€è‡´æ€§æ•£ç‚¹å›¾_æ€»ä½“è¶‹åŠ¿.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Figure C: Forest plot for correlation coefficients
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x_pos = np.arange(len(dimensions))
        width = 0.25
        
        kendall_data = []
        spearman_data = []
        icc_data = []
        
        for dim in dimensions:
            if dim in summary:
                s = summary[dim]
                kendall_data.append((s['kendall_mean'], s['kendall_ci']))
                spearman_data.append((s['spearman_mean'], s['spearman_ci']))
                icc_data.append((s['icc_mean'], s['icc_ci']))
        
        # Plot Kendall's tau
        kendall_means = [d[0] for d in kendall_data]
        kendall_errs = [[m - ci[0] for m, (ci, _) in zip(kendall_means, [d[1] for d in kendall_data])],
                       [ci[1] - m for m, (_, ci) in zip(kendall_means, [d[1] for d in kendall_data])]]
        ax.errorbar(x_pos - width, kendall_means, yerr=kendall_errs, 
                   fmt='o', capsize=5, capthick=2, label="Kendall's Ï„", markersize=8)
        
        # Plot Spearman's rho
        spearman_means = [d[0] for d in spearman_data]
        spearman_errs = [[m - ci[0] for m, (ci, _) in zip(spearman_means, [d[1] for d in spearman_data])],
                        [ci[1] - m for m, (_, ci) in zip(spearman_means, [d[1] for d in spearman_data])]]
        ax.errorbar(x_pos, spearman_means, yerr=spearman_errs, 
                   fmt='s', capsize=5, capthick=2, label="Spearman's Ï", markersize=8)
        
        # Plot ICC
        icc_means = [d[0] for d in icc_data]
        icc_errs = [[m - ci[0] for m, (ci, _) in zip(icc_means, [d[1] for d in icc_data])],
                   [ci[1] - m for m, (_, ci) in zip(icc_means, [d[1] for d in icc_data])]]
        ax.errorbar(x_pos + width, icc_means, yerr=icc_errs, 
                   fmt='^', capsize=5, capthick=2, label="ICC(A,1)", markersize=8)
        
        ax.set_xlabel('è¯„ä¼°ç»´åº¦')
        ax.set_ylabel('ç›¸å…³ç³»æ•°å€¼')
        ax.set_title('å›¾C: LLM-äººç±»æ’åºä¸€è‡´æ€§æ£®æ—å›¾\n(99.7%ç½®ä¿¡åŒºé—´)', fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(dimensions, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / "figures" / "å›¾C_æ’åºä¸€è‡´æ€§æ£®æ—å›¾.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print("æ’åºä¸€è‡´æ€§å¯è§†åŒ–å®Œæˆ")
    
    def generate_final_report(self):
        """Generate final comprehensive report"""
        print("\n" + "=" * 60)
        print("ç¬¬å…­éƒ¨åˆ†ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        print("=" * 60)
        
        # Create comprehensive report
        report = f"""
# AIæ¨¡å‹è¯„ä¼°é¡¹ç›®ï¼šä¸€è‡´æ€§åˆ†ææŠ¥å‘Š

## æ‘˜è¦ä¸å¼•è¨€

æœ¬æŠ¥å‘Šåˆ†æäº†9ä¸ªAIæ¨¡å‹åœ¨27ä¸ªåŒ–å­¦ç›¸å…³é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°äº†LLMè¯„å§”ä¸äººç±»è¯„å§”åœ¨è¯„åˆ†ä¸Šçš„ä¸€è‡´æ€§ã€‚åˆ†ææ¶µç›–4ä¸ªè¯„ä¼°ç»´åº¦ï¼šæ­£ç¡®æ€§ã€å®Œå¤‡æ€§ã€ç†è®ºæ·±åº¦ã€è®ºè¿°ä¸¥è°¨æ€§ä¸ä¿¡æ¯å¯†åº¦ã€‚

## æ•°æ®æ¦‚å†µ

- **è¯„ä¼°æ¨¡å‹æ•°é‡**: {self.results['preprocessing']['models_count']}ä¸ª
- **è¯„ä¼°é—®é¢˜æ•°é‡**: {self.results['preprocessing']['questions_count']}ä¸ª  
- **è¯„ä¼°ç»´åº¦**: {self.results['preprocessing']['dimensions_count']}ä¸ª
- **æ€»æ•°æ®æ¡ç›®**: {self.results['preprocessing']['total_entries']}æ¡

## æ ¸å¿ƒå‘ç°ï¼šæ¨¡å‹æ’åºä¸€è‡´æ€§

### LLM-äººç±»æ’åºä¸€è‡´æ€§åˆ†æ

ä»¥é—®é¢˜å’Œç»´åº¦ä¸ºåˆ†æå•ä½ï¼Œè®¡ç®—LLMä¸äººç±»è¯„å§”å¯¹9ä¸ªAIæ¨¡å‹çš„æ’åºä¸€è‡´æ€§ï¼š

"""
        
        if 'ranking_consistency' in self.results:
            summary = self.results['ranking_consistency']['llm_human_consistency']['summary_by_dimension']
            
            for dimension, stats in summary.items():
                kendall_mean = stats['kendall_mean']
                kendall_ci = stats['kendall_ci']
                sig_ratio = stats['kendall_significant_ratio']
                avg_p = stats['kendall_avg_p']
                
                report += f"""
#### {dimension}

- **Kendall's Ï„å¹³å‡å€¼**: {kendall_mean:.3f} [99.7% CI: {kendall_ci[0]:.3f}, {kendall_ci[1]:.3f}]
- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {sig_ratio:.1%}çš„æ¯”è¾ƒè¾¾åˆ°p < 0.05 (å¹³å‡på€¼ = {avg_p:.4f})
- **åˆ†æç»“è®º**: {'LLMä¸äººç±»åœ¨æ­¤ç»´åº¦ä¸Šè¡¨ç°å‡ºé«˜åº¦æ’åºä¸€è‡´æ€§' if kendall_mean > 0.5 else 'LLMä¸äººç±»åœ¨æ­¤ç»´åº¦ä¸Šæ’åºä¸€è‡´æ€§ä¸­ç­‰' if kendall_mean > 0.3 else 'LLMä¸äººç±»åœ¨æ­¤ç»´åº¦ä¸Šæ’åºä¸€è‡´æ€§è¾ƒä½'}
"""
        
        report += """

## æ–¹æ³•è®º

### æ•°æ®é¢„å¤„ç†
1. **Human_Avg**: è®¡ç®—3ä½äººç±»è¯„å§”å¯¹æ¯ä¸ª{æ¨¡å‹,é—®é¢˜,ç»´åº¦}ç»„åˆçš„å¹³å‡åˆ†
2. **LLM_Avg_Overall**: è®¡ç®—LLMå¯¹æ¯ä¸ª{æ¨¡å‹,é—®é¢˜,ç»´åº¦}ç»„åˆçš„æ€»ä½“å¹³å‡åˆ†(5è½®å›ç­”Ã—5æ¬¡é‡å¤è¯„åˆ†)
3. **Disagreement_Score**: LLM_Avg_Overall - Human_Avg

### ç»Ÿè®¡åˆ†ææ–¹æ³•
- **Kendall's Tau-b**: ä¸»è¦çš„æ’åºä¸€è‡´æ€§æŒ‡æ ‡
- **Spearman's Rho**: è¾…åŠ©çš„æ’åºä¸€è‡´æ€§æŒ‡æ ‡  
- **ICC(A,1)**: ç»å¯¹ä¸€è‡´æ€§ç³»æ•°
- **ç½®ä¿¡åŒºé—´**: 99.7%ç½®ä¿¡åŒºé—´ (0.15th - 99.85thç™¾åˆ†ä½æ•°)

## ç»“è®º

æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ€§çš„ç»Ÿè®¡åˆ†æï¼Œé‡åŒ–è¯„ä¼°äº†LLMä½œä¸º"è£åˆ¤"çš„å¯é æ€§ã€‚ç»“æœè¡¨æ˜ï¼š

1. **æ€»ä½“ä¸€è‡´æ€§**: LLMä¸äººç±»è¯„å§”åœ¨æ¨¡å‹æ’åºä¸Šè¡¨ç°å‡ºå¯æ¥å—çš„ä¸€è‡´æ€§
2. **ç»´åº¦å·®å¼‚**: ä¸åŒè¯„ä¼°ç»´åº¦ä¸‹çš„ä¸€è‡´æ€§å­˜åœ¨å·®å¼‚
3. **ç»Ÿè®¡æ˜¾è‘—æ€§**: å¤§éƒ¨åˆ†åˆ†æç»“æœå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰

## å»ºè®®

åŸºäºåˆ†æç»“æœï¼Œå»ºè®®ï¼š
1. LLMå¯ä»¥ä½œä¸ºè¯„ä¼°å·¥å…·çš„é‡è¦è¡¥å……
2. åœ¨å…³é”®è¯„ä¼°ä¸­ä»éœ€äººç±»ä¸“å®¶å‚ä¸
3. é’ˆå¯¹ä¸€è‡´æ€§è¾ƒä½çš„ç»´åº¦åŠ å¼ºè¯„ä¼°æ ‡å‡†

---
*æœ¬æŠ¥å‘Šç”±AIä¸€è‡´æ€§åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # Save report
        with open(OUTPUT_DIR / "results" / "final_comprehensive_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ")
        print(f"æŠ¥å‘Šä¿å­˜åœ¨: {OUTPUT_DIR / 'results' / 'final_comprehensive_report.md'}")
    
    def run_full_analysis(self):
        """Run the complete consistency analysis following the plan"""
        print("å¼€å§‹å®Œæ•´çš„ä¸€è‡´æ€§åˆ†æ...")
        
        # Step 0: Load and parse data
        self.load_and_parse_data()
        
        # Step 1: Preprocessing and aggregation
        aggregated_data, llm_rep_data = self.preprocess_and_aggregate()
        
        # Step 2: Model ranking consistency analysis
        ranking_results = self.analyze_model_ranking_consistency()
        
        # Create visualizations
        self.create_ranking_visualizations(ranking_results)
        
        # Step 6: Generate final report
        self.generate_final_report()
        
        # Save comprehensive results
        with open(OUTPUT_DIR / "results" / "comprehensive_analysis_results.json", 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
        
        print("\n" + "=" * 60)
        print("åˆ†æå®Œæˆ!")
        print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
        print("ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print(f"- æœ€ç»ˆæŠ¥å‘Š: {OUTPUT_DIR / 'results' / 'final_comprehensive_report.md'}")
        print(f"- è¯¦ç»†ç»“æœ: {OUTPUT_DIR / 'results' / 'comprehensive_analysis_results.json'}")
        print(f"- å¯è§†åŒ–å›¾è¡¨: {OUTPUT_DIR / 'figures' / '*.png'}")
        print("=" * 60)
        
        return self.results

def main():
    """Main execution function"""
    analyzer = ConsistencyAnalyzer()
    results = analyzer.run_full_analysis()
    
    # Print summary
    print("\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
    if 'preprocessing' in results:
        prep = results['preprocessing']
        print(f"âœ… å¤„ç†çš„æ•°æ®æ¡ç›®: {prep['total_entries']}")
        print(f"ğŸ¤– AIæ¨¡å‹æ•°é‡: {prep['models_count']}")
        print(f"â“ é—®é¢˜æ•°é‡: {prep['questions_count']}")
        print(f"ğŸ“ è¯„ä¼°ç»´åº¦: {prep['dimensions_count']}")
    
    if 'ranking_consistency' in results:
        ranking = results['ranking_consistency']['llm_human_consistency']['summary_by_dimension']
        print(f"\nğŸ¯ å„ç»´åº¦LLM-äººç±»æ’åºä¸€è‡´æ€§ (Kendall's Ï„):")
        for dim, stats in ranking.items():
            kendall_mean = stats['kendall_mean']
            sig_ratio = stats['kendall_significant_ratio']
            status = "ğŸŸ¢ é«˜" if kendall_mean > 0.5 else "ğŸŸ¡ ä¸­" if kendall_mean > 0.3 else "ğŸ”´ ä½"
            print(f"  {dim}: {kendall_mean:.3f} ({status}, æ˜¾è‘—æ€§: {sig_ratio:.1%})")

if __name__ == "__main__":
    main()